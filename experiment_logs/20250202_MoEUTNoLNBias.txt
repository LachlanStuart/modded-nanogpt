14:34:58.456: from collections import defaultdict
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import atexit

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.profiler import profile, record_function, ProfilerActivity
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
# torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)
import triton
import triton.language as tl

try:
    from lovely_tensors import monkey_patch
    monkey_patch()
except ImportError:
    pass


# -----------------------------------------------------------------------------
#region  Custom operators : FP8 matmul by @YouJiacheng
@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        # x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        x_f8 = x.mul(x_s).to(torch.float8_e5m2)
        # w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e5m2)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    # return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)
    return x @ w.t(), x.to(torch.float8_e5m2), w.to(torch.float8_e5m2)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)
#endregion
# -----------------------------------------------------------------------------
#region Muon optimizer
@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()
#endregion
# -----------------------------------------------------------------------------
#region PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve
#endregion
# -----------------------------------------------------------------------------
#region The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

def create_block_masks(input_seq: Tensor, sliding_window_num_blocks: Tensor):
    BLOCK_SIZE = 128
    docs = (input_seq == 50256).cumsum(0)

    def document_causal(b, h, q_idx, kv_idx):
        causal_mask = q_idx >= kv_idx
        document_mask = docs[q_idx] == docs[kv_idx]
        return causal_mask & document_mask

    def dense_to_ordered(dense_mask: Tensor):
        num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
        indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
        return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

    # manual block mask creation by @YouJiacheng
    assert len(input_seq) % BLOCK_SIZE == 0
    NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
    block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
    any_causal_bm = block_idx[:, None] >= block_idx
    all_causal_bm = block_idx[:, None] > block_idx
    docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
    docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
    any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
    all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
    any_bm = any_causal_bm & any_document_bm
    all_bm = all_causal_bm & all_document_bm
    partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
    full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
    def build_bm(sw_num_blocks: Tensor) -> BlockMask:
        return BlockMask.from_kv_blocks(
            torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
            partial_kv_indices,
            torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
            full_kv_indices,
            BLOCK_SIZE=BLOCK_SIZE,
            mask_mod=document_causal,
        )
    # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
    return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        # self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977


    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i, block in enumerate(self.blocks[:self.num_encoder_layers]):
            x = block(x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i, block in enumerate(self.blocks[self.num_encoder_layers:]):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = block(x, ve_dec[i], x0, block_masks[i])

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

#endregion
# -----------------------------------------------------------------------------
#region MoEUT Triton kernels
# From https://github.com/RobertCsordas/moeut/blob/master/moeut/cvmm.py

@dataclass
class CVMMSel:
    raw_sel: torch.Tensor
    sel: torch.Tensor
    sel_index: torch.Tensor
    out_index: torch.Tensor | None = None
    reduction_weight: torch.Tensor | None = None

    def clone(self) -> 'CVMMSel':
        return CVMMSel(self.raw_sel, self.sel, self.sel_index, self.out_index, self.reduction_weight)


def cvmm_prepare_sel(sel: torch.Tensor, n_experts: int) -> CVMMSel:
    fsel = sel.flatten()
    ssel, sel_index = fsel.sort()
    return CVMMSel(sel, ssel.view_as(sel), sel_index, None)


def dtype_to_type_id(dtype: torch.dtype):
    if dtype == torch.float32:
        return 0
    elif dtype == torch.float16:
        return 1
    elif dtype == torch.bfloat16:
        return 2

    raise ValueError("Unknown dtype")


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
    ],
    key=['M', 'N', 'K', 'dtype_id', 'allow_tf32']
)
@triton.jit
def cvmm_kernel(
    # Pointers to matrices
    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,
    # Matrix dimensions
    M, N, K,
    # The stride variables represent how much to increase the ptr by when moving by 1
    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`
    # by to get the element one row down (A has M rows).
    stride_am, stride_ak,
    stride_bo, stride_bk, stride_bn,
    stride_cm, stride_cn,
    stride_index, stride_sel, stride_out_index,
    out_index_is_none: tl.constexpr,
    dtype_id: tl.constexpr, allow_tf32: tl.constexpr,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr
):
    """Kernel for computing the matmul C = A x B.
    A has shape (M, K), B has shape (K, N) and C has shape (M, N)
    """
    # -----------------------------------------------------------
    # Map program ids `pid` to the block of C it should compute.
    # This is done in a grouped ordering to promote L2 data reuse.
    # See above `L2 Cache Optimizations` section for details.
    pid = tl.program_id(axis=0)

    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_n = (pid % num_pid_in_group) // group_size_m

    pid_m = first_pid_m + (pid % group_size_m)

    sel_first = tl.load(sel_ptr + pid_m * BLOCK_SIZE_M * stride_sel)
    sel_last = tl.load(sel_ptr + (min((pid_m + 1) * BLOCK_SIZE_M, M) - 1) * stride_sel)
    sel_all = tl.load(sel_ptr + stride_sel * ((pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M))

    for matrix_id in range(sel_first, sel_last + 1):
        # ----------------------------------------------------------
        # Create pointers for the first blocks of A and B.
        # We will advance this pointer as we move in the K direction
        # and accumulate
        # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
        # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
        # See above `Pointer Arithmetics` section for details
        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N

        remap_offs_am = tl.load(index_ptr + stride_index * offs_am)

        # Create offset pointers
        offs_k = tl.arange(0, BLOCK_SIZE_K)
        a_ptrs = a_ptr + (remap_offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
        b_ptrs = b_ptr + matrix_id * stride_bo + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

        # -----------------------------------------------------------
        # Iterate to compute a block of the C matrix.
        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
        # of fp32 values for higher accuracy.
        # `accumulator` will be converted back to fp16 after the loop.
        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
            # Load the next block of A and B, generate a mask by checking the K dimension.
            # If it is out of bounds, set it to 0.
            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
            # We accumulate along the K dimension.

            # Triton was unhappy with passing dtypes as vars.
            if dtype_id == 1:
                a = a.to(tl.float16)
                b = b.to(tl.float16)
            elif dtype_id == 2:
                a = a.to(tl.bfloat16)
                b = b.to(tl.bfloat16)

            accumulator += tl.dot(a, b, allow_tf32=allow_tf32)

            # Advance the ptrs to the next K block.
            a_ptrs += BLOCK_SIZE_K * stride_ak
            b_ptrs += BLOCK_SIZE_K * stride_bk


        if dtype_id == 1:
            c = accumulator.to(tl.float16)
        elif dtype_id == 2:
            c = accumulator.to(tl.bfloat16)
        else:
            c = accumulator

        # -----------------------------------------------------------
        # Write back the block of the output matrix C with masks.
        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)

        if out_index_is_none:
            remap_offs_cm = remap_offs_am
        else:
            remap_offs_cm = tl.load(out_index_ptr + stride_out_index * offs_am)

        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
        c_ptrs = c_ptr + stride_cm * remap_offs_cm[:, None] + stride_cn * offs_cn[None, :]
        c_mask = ((offs_cm[:, None] < M) & (sel_all[:, None] == matrix_id)) & (offs_cn[None, :] < N)
        tl.store(c_ptrs, c, mask=c_mask)


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 128}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 4}, num_stages=4, num_warps=4),

        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        # triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 128}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),

        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 16}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 16}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
    ],
    key=['M', 'N', 'K', 'out_dtype_id', 'allow_tf32', 'dtype_id'], reset_to_zero = ['c_ptr']
)
@triton.jit
def cvmm_backward_kernel3(
    # Pointers to matrices
    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,
    # Matrix dimensions
    M, N, K,
    # The stride variables represent how much to increase the ptr by when moving by 1
    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`
    # by to get the element one row down (A has M rows).
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_co, stride_cm, stride_cn,
    stride_index, stride_sel, stride_out_index,
    out_index_is_none: tl.constexpr,
    out_dtype_id: tl.constexpr, allow_tf32: tl.constexpr, dtype_id: tl.constexpr,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr, K_BLOCKS: tl.constexpr
):
    """Kernel for computing the matmul C = A x B.
    A has shape (M, K), B has shape (K, N) and C has shape (M, N)
    """
    # -----------------------------------------------------------
    # Map program ids `pid` to the block of C it should compute.
    # This is done in a grouped ordering to promote L2 data reuse.
    # See above `L2 Cache Optimizations` section for details.
    pid = tl.program_id(axis=0)
    k_block_id = tl.program_id(axis=1)

    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # ----------------------------------------------------------
    # Create pointers for the first blocks of A and B.
    # We will advance this pointer as we move in the K direction
    # and accumulate
    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
    # See above `Pointer Arithmetics` section for details
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N

    # -----------------------------------------------------------
    # Iterate to compute a block of the C matrix.
    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
    # of fp32 values for higher accuracy.
    # `accumulator` will be converted back to fp16 after the loop.

    a_ptrs_this = a_ptr + offs_am[:, None] * stride_am
    b_ptrs_this = b_ptr + offs_bn[None, :] * stride_bn

    # Kactual = end_i - start_i
    # Nblocks = (Kactual + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K

    # WORK_PER_WORKER = (Nblocks + K_BLOCKS - 1) // K_BLOCKS
    # WORK_PER_WORKER = WORK_PER_WORKER if WORK_PER_WORKER > MIN_WORK_SIZE else MIN_WORK_SIZE


    # # Kloop_start = (Kactual + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K

    # first_block_k = k_block_id * WORK_PER_WORKER
    # last_block_k = min((k_block_id+1) * WORK_PER_WORKER, Nblocks)

    block_start_index = k_block_id * BLOCK_SIZE_K * K_BLOCKS
    block_end_index = min(block_start_index + BLOCK_SIZE_K * K_BLOCKS, K) - 1

    first_mat = tl.load(sel_ptr + stride_sel * block_start_index)
    last_mat = tl.load(sel_ptr + stride_sel * block_end_index)


    for matrix_index in range(first_mat, last_mat + 1):
        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

        start_i = block_start_index
        end_i = block_end_index + 1
        while start_i < end_i:
            middle = (start_i + end_i) // 2
            middle_matrix = tl.load(sel_ptr + middle * stride_sel)
            if middle_matrix < matrix_index:
                start_i = middle + 1
            else:
                end_i = middle


        # # Continue binary search: find the first matrix that is > matrix_index
        start_i2 = start_i
        end_i = block_end_index + 1
        while start_i2 < end_i:
            middle = (start_i2 + end_i) // 2
            middle_matrix = tl.load(sel_ptr + middle * stride_sel)
            if middle_matrix <= matrix_index:
                start_i2 = middle + 1
            else:
                end_i = middle

        end_i = start_i2

        count = end_i - start_i

        block_mem_indices_f_base = start_i  + tl.arange(0, BLOCK_SIZE_K)

        if count > 0:
            for k in range((count + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K):
                # block_mem_indices = (k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)) % K
                block_mem_indices_f = block_mem_indices_f_base + k * BLOCK_SIZE_K
                block_mem_indices = block_mem_indices_f % K
                a_index = tl.load(index_ptr + stride_index * block_mem_indices)
                if out_index_is_none:
                    b_index = a_index
                else:
                    b_index = tl.load(out_index_ptr + stride_out_index * block_mem_indices)
                sel_ok = block_mem_indices_f < end_i

                a_ptrs = a_ptrs_this + a_index[None, :] * stride_ak
                b_ptrs = b_ptrs_this + b_index[:, None] * stride_bk

                # Load the next block of A and B, generate a mask by checking the K dimension.
                # If it is out of bounds, set it to 0.
                a = tl.load(a_ptrs, mask=sel_ok[None, :], other=0.0)
                b = tl.load(b_ptrs, mask=sel_ok[:, None], other=0.0)

                if dtype_id == 1:
                    a = a.to(tl.float16)
                    b = b.to(tl.float16)
                elif dtype_id == 2:
                    a = a.to(tl.bfloat16)
                    b = b.to(tl.bfloat16)

                # We accumulate along the K dimension.
                accumulator += tl.dot(a, b, allow_tf32=allow_tf32)

            if out_dtype_id == 1:
                c = accumulator.to(tl.float16)
            elif out_dtype_id == 2:
                c = accumulator.to(tl.bfloat16)
            else:
                c = accumulator

            # -----------------------------------------------------------
            # Write back the block of the output matrix C with masks.
            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
            c_ptrs = c_ptr + stride_co * matrix_index + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
            # tl.store(c_ptrs, c, mask=c_mask)
            tl.atomic_add(c_ptrs, c, mask=c_mask)


torch.library.define("mylib::cvmm_triton", "(Tensor x, Tensor sel_index, Tensor sel, Tensor keys, ScalarType out_dtype, Tensor out_index) -> Tensor")
@torch.library.impl("mylib::cvmm_triton", "default")
def cvmm_triton(
    x: torch.Tensor,
    sel_index: torch.Tensor,
    sel: torch.Tensor,
    keys: torch.Tensor,
    out_dtype: torch.dtype,
    out_index: torch.Tensor
):
    x = x.flatten(end_dim=-2)
    assert x.shape[-1] == keys.shape[1]

    sel_shape = sel.shape
    sel = sel.flatten()

    M = sel.shape[0]
    O, K, N = keys.shape
    # Allocates output.
    out = torch.empty((M, N), device=x.device, dtype=out_dtype)
    # out = torch.zeros((M, N), device=x.device, dtype=out_dtype)
    # 1D launch kernel where each block gets its own program.

    # expected_m_per_matrix = int(math.ceil(M / O * 1.5))
    # expected_m_per_matrix = M

    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )

    out_index_is_none = False
    if out_index.numel() == 1 and out_index == -1:
        out_index_is_none = True

    cvmm_kernel[grid](
        x, keys, out, sel_index, sel, out_index,
        M, N, K,
        x.stride(0), x.stride(1),
        keys.stride(0), keys.stride(1), keys.stride(2),
        out.stride(0), out.stride(1),
        sel_index.stride(0), sel.stride(0), 0 if out_index_is_none else out_index.stride(0),
        out_index_is_none=out_index_is_none,
        dtype_id = dtype_to_type_id(out.dtype), allow_tf32=False, #torch.backends.cuda.matmul.allow_tf32
    )

    return out.view(*sel_shape, N)


@torch.library.register_fake("mylib::cvmm_triton", cvmm_triton)
def cvmm_triton_abstract(x, sel_idx, sel, keys, out_dtype, out_index):
    sel_shape = sel.shape
    sel = sel.flatten()
    M = sel.shape[0]
    O, K, N = keys.shape
    out = torch.empty((M, N), device=x.device, dtype=out_dtype)
    sel_shape = sel.shape
    return out.view(*sel_shape, N)


def cvmm_triton_backward(
    x: torch.Tensor,
    sel_index: torch.Tensor,
    sel: torch.Tensor,
    grads: torch.Tensor,
    n_experts: int,
    key_dtype: torch.dtype,
    op_dtype: torch.dtype,
    out_index: torch.Tensor
):
    x = x.flatten(end_dim=-2)
    x = x.transpose(0, 1)
    grads = grads.flatten(end_dim=-2)
    sel = sel.flatten()
    M, _ = x.shape
    K, N = grads.shape
    # FIX: out must be atomic_add'able, which excludes bfloat16. Cast to key_dtype after. Maybe this could be f16
    out = torch.zeros((n_experts, M, N), device=x.device, dtype=torch.float32)
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), triton.cdiv(K, META['BLOCK_SIZE_K'] * META['K_BLOCKS'])
    )
    out_index_is_none = False
    if out_index.numel() == 1 and out_index == -1:
        out_index_is_none = True

    cvmm_backward_kernel3[grid](
        x, grads, out, sel_index, sel, out_index,
        M, N, K,
        x.stride(0), x.stride(1),
        grads.stride(0), grads.stride(1),
        out.stride(0), out.stride(1), out.stride(2),
        sel_index.stride(0), sel.stride(0), 0 if out_index_is_none else out_index.stride(0),
        out_index_is_none=out_index_is_none,
        out_dtype_id=dtype_to_type_id(out.dtype),
        dtype_id=dtype_to_type_id(op_dtype),
        allow_tf32=False #torch.backends.cuda.matmul.allow_tf32
    )
    return out.to(dtype=key_dtype)


class CVMM(torch.autograd.Function):
    warned = False

    @staticmethod
    def forward(
        ctx,
        x: torch.Tensor,
        sel_index: torch.Tensor,
        sel: torch.Tensor,
        keys: torch.Tensor,
        out_index: torch.Tensor | None = None,
        reduction_weight: torch.Tensor | None = None
    ):
        ctx.save_for_backward(x, keys, sel, sel_index, out_index, reduction_weight)

        # out_type = get_dtype()
        out_type = x.dtype
        # out_type = torch.float32
        if out_index is None:
            out_index = torch.tensor(-1).cuda()
        res = torch.ops.mylib.cvmm_triton(x, sel_index, sel, keys, out_type, out_index)

        if reduction_weight is not None:
            res = res.view(*reduction_weight.shape, res.shape[-1])
            res = (reduction_weight.unsqueeze(-2).type_as(res) @ res).squeeze(-2)

        ctx.op_type = out_type
        ctx.keys_type = keys.dtype
        ctx.dtype = out_type
        return res.type_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        x, keys, sel, sel_index, out_index, reduction_weight = ctx.saved_tensors
        keys_dt = keys

        # Backward for weight
        if reduction_weight is not None:
            # Project back the grads with he reduction weight, so the grad for the weight matrix is ok
            grad_output_w = reduction_weight.unsqueeze(-1).type_as(grad_output) @ grad_output.unsqueeze(-2)
        else:
            grad_output_w  = grad_output

        out_index_is_none = False
        if out_index is None:
            out_index_is_none = True
            out_index = torch.tensor(-1).cuda()

        grad_w = cvmm_triton_backward(
            x,
            sel_index,
            sel,
            grad_output_w,
            keys_dt.shape[0],
            ctx.keys_type,
            ctx.dtype,
            out_index=out_index
        )

        # Backward for input and reduction weight
        grad_w_off = None

        bw_index = sel_index if out_index_is_none else out_index
        bw_index_out = torch.tensor(-1).cuda()
        if reduction_weight is not None:
            # Hack the output indices to emulate repeats
            bw_index_out = bw_index
            bw_index = bw_index // reduction_weight.shape[-1]

        grad_x_full = torch.ops.mylib.cvmm_triton(
            grad_output,
            bw_index,
            sel,
            keys_dt.transpose(1,2),
            ctx.op_type,
            bw_index_out
        )

        grad_x_full = grad_x_full.view(*x.shape[:-1], -1, x.shape[-1])
        if reduction_weight is not None:
            # grad_x_full is the unscaled grad. For the input, we have to scale it, for the reduction wegiht,
            # we have to compute dot products with the input.
            grad_x = (reduction_weight.view(*grad_x_full.shape[:-1]).unsqueeze(-2).type_as(grad_x_full) @ grad_x_full).squeeze(-2)
            grad_w_off = (grad_x_full.type_as(reduction_weight) @ x.unsqueeze(-1).type_as(reduction_weight)).squeeze(-1).view_as(reduction_weight)
        elif grad_x_full.shape[-2] != 1:
            grad_x = grad_x_full.sum(-2)
        else:
            grad_x = grad_x_full

        grad_x = grad_x.view_as(x)

        return grad_x, None, None, grad_w, None, grad_w_off


def cvmm(x: torch.Tensor, sel: torch.Tensor | CVMMSel, keys: torch.Tensor):
    if not isinstance(sel, CVMMSel):
        sel = cvmm_prepare_sel(sel, keys.shape[0])
    assert x.dtype == keys.dtype, f"{x.dtype=} != {keys.dtype=}"

    return CVMM.apply(x, sel.sel_index, sel.sel, keys, sel.out_index, sel.reduction_weight)


def cvmm_prepare_sel2(sel: torch.Tensor, w: torch.Tensor | None = None) -> CVMMSel:
    # Has multiple selections for each batch element
    n_per_batch = sel.shape[-1]

    # indices = torch.arange(sel.nelement() // n_per_batch, device=sel.device, dtype=torch.int32)
    # indices = indices.repeat_interleave(n_per_batch).flatten()

    fsel = sel.flatten()
    ssel, sel_index = fsel.sort()

    # in_index = indices[sel_index]
    in_index = sel_index // n_per_batch

    return CVMMSel(sel, ssel.view_as(sel), in_index, sel_index, w)

#endregion
# -----------------------------------------------------------------------------
#region MoEUT
def log_mean(x: torch.Tensor, dim: int = 0):
    return x.logsumexp(dim) - torch.log(torch.tensor(x.shape[dim]))


def entropy_l(l: torch.Tensor) -> torch.Tensor:
    return - (l * l.exp()).sum(-1)


def entropy_reg(sel: torch.Tensor, dim: int) -> torch.Tensor:
    sel = F.log_softmax(sel, dim=-1)
    sel = log_mean(sel, dim)
    return - entropy_l(sel).mean()


class SigmaMoE(torch.nn.Module):
    def __init__(self, dmodel: int, n_experts: int, expert_size: int, k: int,
                 activation=F.relu,
                 v_dim: int | None = None,
                 expert_dropout: float = 0.0):

        super().__init__()
        self.k_dim = dmodel
        self.v_dim = v_dim if v_dim is not None else dmodel
        self.n_experts = n_experts
        self.expert_size = expert_size
        self.size = self.n_experts * self.expert_size
        self.k_vec_dim = self.k_dim
        self.num_heads = k
        self.activation = activation
        self.expert_dropout = expert_dropout

        self.sel_hist = []

        self.keys = torch.nn.Parameter(torch.empty(self.n_experts, self.k_vec_dim, self.expert_size))
        self.values = torch.nn.Parameter(torch.empty(self.n_experts, self.expert_size, self.v_dim))
        self.expert_sel = torch.nn.Parameter(torch.empty(self.n_experts, self.k_vec_dim))

    @torch.no_grad
    def reset_parameters(self, std_scale: float):
        torch.nn.init.normal_(self.expert_sel, 0, std_scale / (self.k_dim) ** 0.5)
        torch.nn.init.normal_(self.keys, 0, std_scale / (self.k_dim) ** 0.5)
        torch.nn.init.normal_(self.values, 0, std_scale / (self.n_experts * self.expert_size) ** 0.5)

        # self.renorm_keep_std(self.expert_sel, dim=1)
        std = self.expert_sel.std()
        self.expert_sel.div_(self.expert_sel.norm(dim=1, keepdim=True))
        self.expert_sel.mul_(std / self.expert_sel.std())


    def forward(self, input: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        # Selection score calculation
        sel = F.linear(norm(input), self.expert_sel, None)
        sel_r = sel

        # Selection activation and topk
        sel = F.sigmoid(sel)

        if self.training and self.expert_dropout > 0:
            mask = torch.rand_like(sel) < self.expert_dropout
            sel = sel.masked_fill(mask, float("-inf"))

        sel_val, sel_index = sel.topk(self.num_heads, dim=-1, sorted=False)

        # Preprocess the selection indices. They will be needed for both layers and save some time
        sel_indices = cvmm_prepare_sel2(sel_index.int())

        # "Up-projection" layer for each head
        scores = cvmm(input, sel_indices, self.keys)
        scores = self.activation(scores)

        # Down projection layer for each head
        sel_indices = sel_indices.clone()
        sel_indices.reduction_weight = sel_val
        sel_indices.sel_index = sel_indices.out_index
        sel_indices.out_index = None

        out = cvmm(scores, sel_indices, self.values)

        res = out.view(*input.shape[:-1], self.v_dim)
        return input + res, sel_r


class SwitchHeadRoPE(torch.nn.Module):
    def __init__(self, state_size: int, num_heads: int, n_experts: int, max_seq_len: int,
                 head_dim: int | None = None, expert_dropout: float = 0.0, moe_k: int = 2
                 ):

        super().__init__()

        self.input_size = state_size
        self.output_size = state_size
        self.pe_size = self.input_size
        self.expert_dropout = expert_dropout
        self.moe_k = moe_k
        self.attention_to_visualize = []
        self.selections_to_visualize = {}
        self.n_experts = n_experts

        self.sel_hist = []

        self.num_heads = num_heads
        self.head_dim = head_dim or (state_size // num_heads)
        self.rotary = Rotary(self.head_dim, max_seq_len)

        self.q = torch.nn.Linear(self.input_size, self.head_dim * self.num_heads, bias=False)
        self.k = torch.nn.Linear(self.input_size, self.head_dim * self.num_heads, bias=False)

        if self.n_experts > 1:
            self.v = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size, self.head_dim))
            self.o = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.head_dim, self.output_size))
            self.sel_v = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size))
        else:
            self.v = torch.nn.Parameter(torch.empty(self.num_heads * self.head_dim, self.input_size))
            self.o = torch.nn.Parameter(torch.empty(self.output_size, self.num_heads * self.head_dim))

        self.sel_o = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size))

    @torch.no_grad
    def reset_parameters(self, std_scale: float):
        def renorm_rows(x: torch.Tensor):
            std_t = x.std(dim=-1, keepdim=True)
            x.div_(x.norm(dim=-1, keepdim=True))
            x.mul_(std_t / x.std())

        if self.n_experts > 1:
            torch.nn.init.normal_(self.sel_v, 0, std_scale / (self.input_size) ** 0.5)
            renorm_rows(self.sel_v)

        torch.nn.init.normal_(self.sel_o, 0, std_scale / (self.input_size) ** 0.5)
        renorm_rows(self.sel_o)

        torch.nn.init.normal_(self.k.weight, 0, std_scale / (self.input_size) ** 0.5)
        torch.nn.init.normal_(self.q.weight, 0, std_scale / (self.input_size) ** 0.5)
        torch.nn.init.normal_(self.v, 0, std_scale / (self.input_size) ** 0.5)
        torch.nn.init.normal_(self.o, 0, std_scale / (self.num_heads * self.head_dim) ** 0.5)


    def project_to_torch_order(self, x: torch.Tensor):
        return x.view(*x.shape[:-1], self.num_heads, -1).transpose(-2, -3)


    def get_sel(self, t: torch.Tensor, w: torch.Tensor) -> tuple[CVMMSel, torch.Tensor]:
        sel = F.linear(t, w).float()
        sel = sel_raw = sel.view(*sel.shape[:-1], self.num_heads, -1)
        sel = sel.sigmoid()

        with torch.no_grad():
            if self.expert_dropout > 0 and self.training:
                mask = torch.rand_like(sel) < self.expert_dropout
                sel2 = sel.masked_fill(mask, float('-inf'))
            else:
                sel2 = sel
            _, sel_index = sel2.topk(self.moe_k, dim=-1, sorted=False)
        sel_val = torch.gather(sel, -1, sel_index)

        sel_index_shifted = (torch.arange(self.num_heads, device=sel_index.device, dtype=sel_index.dtype) * self.n_experts).unsqueeze(-1) + sel_index
        return cvmm_prepare_sel2(sel_index_shifted.flatten(-2,-1), sel_val), sel_raw


    def forward(self, x: torch.Tensor, ve: torch.Tensor | None, block_mask: BlockMask
                ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        # *src: [batch_size, out_len, c]
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"

        xnorm = norm(x)
        q = self.q(xnorm)
        k = self.k(xnorm)

        if self.n_experts > 1:
            v_sel, v_sel_r = self.get_sel(xnorm, self.sel_v)
            o_sel, o_sel_r = self.get_sel(xnorm, self.sel_o)

            v = cvmm(x, v_sel, self.v).transpose(-2, -3)
        else:
            o_gate = F.sigmoid(F.linear(xnorm, self.sel_o))
            # v = self.project_to_torch_order(F.linear(v_src, self.v))
            # return x.view(*x.shape[:-1], self.num_heads, -1).transpose(-2, -3)
            v = F.linear(x, self.v).view(B, T, self.num_heads, self.head_dim).transpose(1,2)

        q = q.view(B, T, self.num_heads, self.head_dim)
        k = k.view(B, T, self.num_heads, self.head_dim)

        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q).transpose(1,2), self.rotary(k).transpose(1,2)
        # if ve is not None:
        #     v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        # else: # skip mid-layers token value embeddings by @YouJiacheng
        #     v = self.lambdas[0] * v

        res = flex_attention(q, k, v, block_mask=block_mask, scale=0.12)
        res = res.transpose(1, 2)

        if self.n_experts > 1:
            # The output selection indices are calculated from the current state and are also used for projecting "q".
            # But that projection needs to create multiple copies for the different heads. Here we already have the
            # heads, but we have to create copies for the top-k elements. We can calculate that from the reduction
            # weight. We also want to compute not only the weighted average between the top-k elements, but also
            # of the different heads. So reshape the reduction weight accordingly.
            o_sel.sel_index = o_sel.out_index // o_sel.reduction_weight.shape[-1]
            o_sel.reduction_weight = o_sel.reduction_weight.flatten(-2)
            out = cvmm(res, o_sel, self.o)
        else:
            res = res * o_gate[..., None]
            out = F.linear(res.flatten(-2), self.o)

        return x + out, o_sel_r, v_sel_r


class MoEUTLayer(torch.nn.Module):
    def __init__(self, d_model: int, num_heads: int, ff_expert_size: int, ff_n_experts: int,
                 att_n_experts: int, max_seq_len: int, head_dim: int | None = None, att_k: int = 2,
                 ff_k: int = 8, ff_expert_dropout: float = 0.0, att_expert_dropout: float = 0.0):

        super().__init__()
        self.attention = SwitchHeadRoPE(
            d_model, num_heads, att_n_experts, max_seq_len=max_seq_len, head_dim=head_dim, moe_k=att_k,
            expert_dropout=att_expert_dropout)
        self.ffn = SigmaMoE(d_model, ff_n_experts, ff_expert_size, k=ff_k, expert_dropout=ff_expert_dropout)

    def forward(self, x: torch.Tensor, ve: torch.Tensor | None, block_mask: BlockMask) -> torch.Tensor:
        x, o_sel_r, v_sel_r = self.attention(x, ve, block_mask)
        x, ffn_sel_r = self.ffn(x)
        return x, o_sel_r, v_sel_r, ffn_sel_r


class MoEUT(torch.nn.Module):
    def __init__(self, d_model: int, n_layers: int, num_heads: int, ff_expert_size: int, ff_n_experts: int,
                 att_n_experts: int, max_seq_len: int, head_dim: int | None = None, att_k: int = 2,
                 ff_k: int = 8, ff_expert_dropout: float = 0.0, att_expert_dropout: float = 0.0,
                 entropy_reg: float = 0.01, att_entropy_reg: float = 0.001,
                 group_size: int = 2):
        super().__init__()

        self.entropy_reg = entropy_reg
        self.att_entropy_reg = att_entropy_reg

        self.n_repeats = n_layers // group_size
        self.layers = torch.nn.ModuleList([
            MoEUTLayer(d_model, num_heads, ff_expert_size, ff_n_experts, att_n_experts,
                       max_seq_len, head_dim, att_k, ff_k,
                       ff_expert_dropout, att_expert_dropout)
            for _ in range(group_size)
        ])

        self.reset_parameters()

    def forward(self, x: torch.Tensor, block_mask: BlockMask) -> tuple[torch.Tensor, torch.Tensor]:
        # Run the model
        o_sels = defaultdict(list)
        v_sels = defaultdict(list)
        ffn_sels = defaultdict(list)
        for r in range(self.n_repeats):
            for li, layer in enumerate(self.layers):
                x, o_sel_r, v_sel_r, ffn_sel_r = layer(x, ve=None, block_mask=block_mask)
                o_sels[li].append(o_sel_r)
                v_sels[li].append(v_sel_r)
                ffn_sels[li].append(ffn_sel_r)

        # Collect regularizaiton losses. Must be at the end because it is across the layers.
        # For attention losses:
        # def get_reg_loss(self) -> torch.Tensor:
        #     loss = 0
        #     if self.sel_hist:
        #         for i in range(len(self.sel_hist[0])):
        #             loss = loss + entropy_reg(torch.stack([l[i] for l in self.sel_hist], dim=-3).flatten(-4,-3), -3)
        #     self.sel_hist = []
        #     return loss
        # For ffn losses:
        # loss = entropy_reg(torch.stack(self.sel_hist, dim=-2).flatten(-3, -2), -2)
        # self.sel_hist = []
        # return loss

        ffn_reg_loss = torch.stack([
            entropy_reg(torch.stack(sel_hist, dim=-2).flatten(-3, -2), -2)
            for sel_hist in ffn_sels.values()
        ]).sum()
        att_reg_loss = torch.stack([
            entropy_reg(torch.stack(sel_hist, dim=-3).flatten(-4, -3), -3)
            for sel_hist in [*o_sels.values(), *v_sels.values()]
        ]).sum()

        reg_loss = self.entropy_reg * ffn_reg_loss + self.att_entropy_reg * att_reg_loss

        return x, reg_loss

    @torch.no_grad
    def reset_parameters(self):
        scale = (2 / (self.n_repeats * len(self.layers))) ** 0.5
        for layer in self.modules():
            if isinstance(layer, (SwitchHeadRoPE, SigmaMoE)):
                layer.reset_parameters(scale)
            elif isinstance(layer, torch.nn.LayerNorm):
                torch.nn.init.ones_(layer.weight)
                torch.nn.init.zeros_(layer.bias)


class MoEUTWrapper(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.moeut = MoEUT(
            d_model=model_dim,
            n_layers=num_layers,
            num_heads=num_heads,
            ff_expert_size=128,
            ff_n_experts=64,  # FIXME: Arbitrary decision
            att_n_experts=10, # !! From MoEUT paper, but they also used really weird numbers like d_head=41 so I don't trust their judgement
            max_seq_len=max_seq_len,
            head_dim=None,
            att_k=2,
            ff_k=8,
            ff_expert_dropout=0.0,
            att_expert_dropout=0.0,
            entropy_reg=0.01,
            att_entropy_reg=0.001,
            group_size=2,
        )
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977


    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, _ = create_block_masks(input_seq, sliding_window_num_blocks)

        x = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x, reg_loss = self.moeut(x, long_bm)
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        if self.training:
            loss = loss + reg_loss
        return loss


#endregion
# -----------------------------------------------------------------------------
#region Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets
#endregion
# -----------------------------------------------------------------------------
#region utils, hyperparams
def print0(s, console=True):
    if master_process:
        timestamp = time.strftime("%H:%M:%S.") + f"{time.time() % 1:.3f}"[2:]
        s = f"{timestamp}: {s}"
        if console:
            print(s)
        if logfile:
            with open(logfile, "a") as f:
                print(s, file=f)

def log_mem():
    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )

@dataclass(frozen=True, kw_only=True)
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations: int = 1770 # number of iterations to run
    cooldown_frac: float = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len: int = 48*1024 # FlexAttention sequence length
    val_seq_len: int = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint: bool = False
    dev: bool = False

TEST_HPARAMS = Hyperparameters(
    train_files = "data/fineweb1B/fineweb_train_*.bin",
    val_files = "data/fineweb1B/fineweb_val_*.bin",
    val_tokens = 1048576,
    num_iterations = 1000, #770,
    cooldown_frac = 0.4,
    val_loss_every = 125,
    seq_len = 16*1024,
    val_seq_len = 4*16*1024,
    save_checkpoint = False,
    dev=False,
)
DEV_HPARAMS = Hyperparameters(
    train_files = "data/fineweb1B/fineweb_train_*.bin",
    val_files = "data/fineweb1B/fineweb_val_*.bin",
    val_tokens = 1024,
    num_iterations = 20,
    cooldown_frac = 0.4,
    val_loss_every = 125,
    seq_len = 512,
    val_seq_len = 512,
    save_checkpoint = False,
    dev=True,
)

#endregion
# -----------------------------------------------------------------------------
#region main()
master_process = None
logfile = None
def main(args = TEST_HPARAMS):
# def main(args = DEV_HPARAMS):
    global master_process, logfile
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    atexit.register(dist.destroy_process_group)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    if master_process and not args.dev:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)


    # begin by printing this file (the Python code)
    print0(code, console=False)
    print0("="*100, console=False)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}", console=False)
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}", console=False)
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi(), console=False)
    print0("="*100, console=False)
    atexit.register(log_mem)

    torch.random.manual_seed(0)
    torch.cuda.synchronize()
    print0("Init data")
    # load data
    train_batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

    torch.cuda.synchronize()
    print0("Init model")
    # REF: model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    model: nn.Module = MoEUTWrapper(vocab_size=50257, num_layers=12, num_heads=3, model_dim=384, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    model.bfloat16()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()

    # count parameters
    n_params_by_dtype = defaultdict(lambda: 0)
    for name, param in model.named_parameters():
        dist.broadcast(param.detach(), 0)
        n_params_by_dtype[param.dtype] += param.numel()
    for dt, n_params in n_params_by_dtype.items():
        print0(f"{dt}: {n_params/1024/1024:.3f}Mi params")
    print0(f"total: {sum(n_params_by_dtype.values())/1024/1024:.3f}Mi params")


    torch.cuda.synchronize()
    print0("Init optimizers")
    # collect the parameters to optimize
    hidden_matrix_params = [p for n, p in model.named_parameters() if p.ndim >= 2 and "embed" not in n and "lm_head" not in n]
    embed_params = [p for n, p in model.named_parameters() if "embed" in n]
    scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    lr_mod = (args.seq_len/Hyperparameters().seq_len) ** 0.5  # Correct LR based on difference in batch size vs 4
    print(f"{lr_mod=}")
    adam_params = [dict(params=head_params, lr=0.008*lr_mod), dict(params=embed_params, lr=0.6*lr_mod), dict(params=scalar_params, lr=0.04*lr_mod)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05*lr_mod, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(step: int):
        t = 1 - step / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    if not args.dev:
        model: nn.Module = torch.compile(model) #, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    print0("Starting train loop")
    train_steps = args.num_iterations
    prof = None
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # if step == 5:
        #     prof = profile(record_shapes=True, profile_memory=True, with_stack=True)
        #     prof.__enter__()
        #     prof.start()
        # if prof is not None:
        #     if step == 9:
        #         prof.__exit__(None, None, None)
        #         prof.export_chrome_trace("trace.json")
        #         prof = None
        #     else:
        #         prof.step()

        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_batch_size = world_size * args.val_seq_len
            assert args.val_tokens % val_batch_size == 0
            val_steps = args.val_tokens // val_batch_size
            val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for i in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION -----------------
        inputs, targets = next(train_loader)
        train_losses = []
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            loss = model(input_seq, target_seq, sw_num_blks(window_size))
            loss.backward()
            dist.all_reduce(loss, op=dist.ReduceOp.AVG)
            train_losses.append(loss.item())
            del loss
        train_loss = sum(train_losses or [torch.nan]) / max(len(train_losses), 1)
        for param in model.parameters():
            if param.grad is not None:
                dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        del param
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)

        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_loss:{train_loss:.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms {torch.cuda.memory_allocated()=}", console=True)

    if master_process and logfile is not None:
        new_logfile = input("Name run? ")
        if new_logfile:
            old_logfile = logfile
            logfile = f"logs/{new_logfile}.txt"
            os.rename(old_logfile, logfile)
            print0(f"Renamed {old_logfile} -> {logfile}")
    else:
        print(logfile)
#endregion
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    main()

14:34:58.456: ====================================================================================================
14:34:58.456: Running Python 3.12.7 (main, Oct 16 2024, 04:37:19) [Clang 18.1.8 ]
14:34:58.456: Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
14:34:58.539: Sun Feb  2 14:34:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090 Ti     On  |   00000000:2D:00.0 Off |                  Off |
|  0%   46C    P2             95W /  450W |     882MiB /  24564MiB |      2%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A        26      G   /Xwayland                                   N/A      |
|    0   N/A  N/A    253649      C   /python3.12                                 N/A      |
+-----------------------------------------------------------------------------------------+

14:34:58.539: ====================================================================================================
14:34:58.539: Init data
14:34:58.539: Init model
14:34:58.895: torch.bfloat16: 55.105Mi params
14:34:58.895: total: 55.105Mi params
14:34:58.895: Init optimizers
14:34:58.910: Starting train loop
14:35:41.868: step:0/1000 val_loss:10.8258 train_time:0ms step_avg:nanms
14:36:21.583: step:1/1000 train_loss:10.7336 train_time:39715ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:22.036: step:2/1000 train_loss:10.7416 train_time:40169ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:22.268: step:3/1000 train_loss:9.8320 train_time:40401ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:22.499: step:4/1000 train_loss:8.8640 train_time:40631ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:22.732: step:5/1000 train_loss:8.0452 train_time:40864ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:22.963: step:6/1000 train_loss:7.6347 train_time:41096ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:23.196: step:7/1000 train_loss:7.6638 train_time:41328ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:23.425: step:8/1000 train_loss:7.4605 train_time:41558ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:23.659: step:9/1000 train_loss:7.2946 train_time:41791ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:23.890: step:10/1000 train_loss:7.8794 train_time:42022ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:24.122: step:11/1000 train_loss:7.3114 train_time:232ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:24.352: step:12/1000 train_loss:7.1193 train_time:462ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:36:24.583: step:13/1000 train_loss:6.3232 train_time:693ms step_avg:231.12ms torch.cuda.memory_allocated()=369551872
14:36:24.815: step:14/1000 train_loss:7.2400 train_time:925ms step_avg:231.37ms torch.cuda.memory_allocated()=369551872
14:36:25.048: step:15/1000 train_loss:7.4481 train_time:1159ms step_avg:231.71ms torch.cuda.memory_allocated()=369551872
14:36:25.279: step:16/1000 train_loss:7.5174 train_time:1389ms step_avg:231.48ms torch.cuda.memory_allocated()=369551872
14:36:25.509: step:17/1000 train_loss:7.8025 train_time:1620ms step_avg:231.38ms torch.cuda.memory_allocated()=369551872
14:36:25.739: step:18/1000 train_loss:7.8864 train_time:1850ms step_avg:231.19ms torch.cuda.memory_allocated()=369551872
14:36:25.971: step:19/1000 train_loss:7.2846 train_time:2082ms step_avg:231.28ms torch.cuda.memory_allocated()=369551872
14:36:26.202: step:20/1000 train_loss:7.2909 train_time:2312ms step_avg:231.21ms torch.cuda.memory_allocated()=369551872
14:36:26.432: step:21/1000 train_loss:7.2755 train_time:2543ms step_avg:231.14ms torch.cuda.memory_allocated()=369551872
14:36:26.662: step:22/1000 train_loss:7.1142 train_time:2773ms step_avg:231.05ms torch.cuda.memory_allocated()=369551872
14:36:26.893: step:23/1000 train_loss:6.9290 train_time:3004ms step_avg:231.04ms torch.cuda.memory_allocated()=369551872
14:36:27.124: step:24/1000 train_loss:6.9957 train_time:3234ms step_avg:231.03ms torch.cuda.memory_allocated()=369551872
14:36:27.355: step:25/1000 train_loss:6.9556 train_time:3465ms step_avg:231.02ms torch.cuda.memory_allocated()=369551872
14:36:27.586: step:26/1000 train_loss:6.8552 train_time:3696ms step_avg:231.02ms torch.cuda.memory_allocated()=369551872
14:36:27.818: step:27/1000 train_loss:6.9176 train_time:3928ms step_avg:231.04ms torch.cuda.memory_allocated()=369551872
14:36:28.049: step:28/1000 train_loss:7.2660 train_time:4159ms step_avg:231.08ms torch.cuda.memory_allocated()=369551872
14:36:28.280: step:29/1000 train_loss:7.2490 train_time:4390ms step_avg:231.04ms torch.cuda.memory_allocated()=369551872
14:36:28.509: step:30/1000 train_loss:6.8663 train_time:4620ms step_avg:230.98ms torch.cuda.memory_allocated()=369551872
14:36:28.741: step:31/1000 train_loss:6.9776 train_time:4851ms step_avg:231.02ms torch.cuda.memory_allocated()=369551872
14:36:28.972: step:32/1000 train_loss:6.6619 train_time:5082ms step_avg:231.02ms torch.cuda.memory_allocated()=369551872
14:36:29.203: step:33/1000 train_loss:6.8270 train_time:5313ms step_avg:230.99ms torch.cuda.memory_allocated()=369551872
14:36:29.434: step:34/1000 train_loss:6.5440 train_time:5545ms step_avg:231.02ms torch.cuda.memory_allocated()=369551872
14:36:29.666: step:35/1000 train_loss:6.9779 train_time:5776ms step_avg:231.03ms torch.cuda.memory_allocated()=369551872
14:36:29.896: step:36/1000 train_loss:6.6906 train_time:6007ms step_avg:231.02ms torch.cuda.memory_allocated()=369551872
14:36:30.130: step:37/1000 train_loss:6.8405 train_time:6240ms step_avg:231.10ms torch.cuda.memory_allocated()=369551872
14:36:30.360: step:38/1000 train_loss:6.7102 train_time:6471ms step_avg:231.09ms torch.cuda.memory_allocated()=369551872
14:36:30.591: step:39/1000 train_loss:6.8957 train_time:6702ms step_avg:231.09ms torch.cuda.memory_allocated()=369551872
14:36:30.823: step:40/1000 train_loss:6.5466 train_time:6934ms step_avg:231.12ms torch.cuda.memory_allocated()=369551872
14:36:31.056: step:41/1000 train_loss:6.6226 train_time:7166ms step_avg:231.16ms torch.cuda.memory_allocated()=369551872
14:36:31.287: step:42/1000 train_loss:6.4836 train_time:7398ms step_avg:231.18ms torch.cuda.memory_allocated()=369551872
14:36:31.518: step:43/1000 train_loss:6.6674 train_time:7629ms step_avg:231.17ms torch.cuda.memory_allocated()=369551872
14:36:31.750: step:44/1000 train_loss:6.3268 train_time:7860ms step_avg:231.19ms torch.cuda.memory_allocated()=369551872
14:36:31.981: step:45/1000 train_loss:7.3625 train_time:8091ms step_avg:231.17ms torch.cuda.memory_allocated()=369551872
14:36:32.211: step:46/1000 train_loss:6.5716 train_time:8322ms step_avg:231.15ms torch.cuda.memory_allocated()=369551872
14:36:32.442: step:47/1000 train_loss:6.6222 train_time:8552ms step_avg:231.13ms torch.cuda.memory_allocated()=369551872
14:36:32.673: step:48/1000 train_loss:6.9326 train_time:8784ms step_avg:231.15ms torch.cuda.memory_allocated()=369551872
14:36:32.905: step:49/1000 train_loss:6.6671 train_time:9015ms step_avg:231.15ms torch.cuda.memory_allocated()=369551872
14:36:33.136: step:50/1000 train_loss:6.7026 train_time:9247ms step_avg:231.16ms torch.cuda.memory_allocated()=369551872
14:36:33.367: step:51/1000 train_loss:6.5688 train_time:9478ms step_avg:231.16ms torch.cuda.memory_allocated()=369551872
14:36:33.598: step:52/1000 train_loss:6.8288 train_time:9708ms step_avg:231.14ms torch.cuda.memory_allocated()=369551872
14:36:33.831: step:53/1000 train_loss:6.1489 train_time:9942ms step_avg:231.20ms torch.cuda.memory_allocated()=369551872
14:36:34.063: step:54/1000 train_loss:6.5323 train_time:10173ms step_avg:231.20ms torch.cuda.memory_allocated()=369551872
14:36:34.294: step:55/1000 train_loss:7.0685 train_time:10404ms step_avg:231.20ms torch.cuda.memory_allocated()=369551872
14:36:34.524: step:56/1000 train_loss:6.3885 train_time:10635ms step_avg:231.19ms torch.cuda.memory_allocated()=369551872
14:36:34.756: step:57/1000 train_loss:6.6656 train_time:10867ms step_avg:231.20ms torch.cuda.memory_allocated()=369551872
14:36:34.987: step:58/1000 train_loss:6.5554 train_time:11097ms step_avg:231.18ms torch.cuda.memory_allocated()=369551872
14:36:35.218: step:59/1000 train_loss:6.3489 train_time:11329ms step_avg:231.20ms torch.cuda.memory_allocated()=369551872
14:36:35.450: step:60/1000 train_loss:6.7515 train_time:11560ms step_avg:231.19ms torch.cuda.memory_allocated()=369551872
14:36:35.680: step:61/1000 train_loss:6.8953 train_time:11791ms step_avg:231.19ms torch.cuda.memory_allocated()=369551872
14:36:35.912: step:62/1000 train_loss:6.8067 train_time:12023ms step_avg:231.20ms torch.cuda.memory_allocated()=369551872
14:36:36.145: step:63/1000 train_loss:6.7196 train_time:12255ms step_avg:231.22ms torch.cuda.memory_allocated()=369551872
14:36:36.376: step:64/1000 train_loss:6.6551 train_time:12486ms step_avg:231.22ms torch.cuda.memory_allocated()=369551872
14:36:36.606: step:65/1000 train_loss:6.8394 train_time:12717ms step_avg:231.21ms torch.cuda.memory_allocated()=369551872
14:36:36.838: step:66/1000 train_loss:6.4214 train_time:12948ms step_avg:231.21ms torch.cuda.memory_allocated()=369551872
14:36:37.070: step:67/1000 train_loss:6.8641 train_time:13180ms step_avg:231.23ms torch.cuda.memory_allocated()=369551872
14:36:37.301: step:68/1000 train_loss:6.4412 train_time:13412ms step_avg:231.23ms torch.cuda.memory_allocated()=369551872
14:36:37.533: step:69/1000 train_loss:6.3184 train_time:13643ms step_avg:231.23ms torch.cuda.memory_allocated()=369551872
14:36:37.764: step:70/1000 train_loss:6.4826 train_time:13874ms step_avg:231.23ms torch.cuda.memory_allocated()=369551872
14:36:37.995: step:71/1000 train_loss:6.7276 train_time:14105ms step_avg:231.23ms torch.cuda.memory_allocated()=369551872
14:36:38.225: step:72/1000 train_loss:6.5648 train_time:14336ms step_avg:231.22ms torch.cuda.memory_allocated()=369551872
14:36:38.457: step:73/1000 train_loss:6.5098 train_time:14567ms step_avg:231.22ms torch.cuda.memory_allocated()=369551872
14:36:38.691: step:74/1000 train_loss:6.6418 train_time:14801ms step_avg:231.26ms torch.cuda.memory_allocated()=369551872
14:36:38.925: step:75/1000 train_loss:6.2217 train_time:15035ms step_avg:231.30ms torch.cuda.memory_allocated()=369551872
14:36:39.161: step:76/1000 train_loss:6.4961 train_time:15271ms step_avg:231.38ms torch.cuda.memory_allocated()=369551872
14:36:39.399: step:77/1000 train_loss:6.3944 train_time:15509ms step_avg:231.48ms torch.cuda.memory_allocated()=369551872
14:36:39.637: step:78/1000 train_loss:6.1986 train_time:15747ms step_avg:231.58ms torch.cuda.memory_allocated()=369551872
14:36:39.875: step:79/1000 train_loss:6.4966 train_time:15986ms step_avg:231.67ms torch.cuda.memory_allocated()=369551872
14:36:40.113: step:80/1000 train_loss:6.2513 train_time:16223ms step_avg:231.76ms torch.cuda.memory_allocated()=369551872
14:36:40.351: step:81/1000 train_loss:6.2353 train_time:16461ms step_avg:231.84ms torch.cuda.memory_allocated()=369551872
14:36:40.589: step:82/1000 train_loss:6.3232 train_time:16699ms step_avg:231.93ms torch.cuda.memory_allocated()=369551872
14:36:40.826: step:83/1000 train_loss:6.6134 train_time:16936ms step_avg:232.00ms torch.cuda.memory_allocated()=369551872
14:36:41.065: step:84/1000 train_loss:6.9292 train_time:17175ms step_avg:232.10ms torch.cuda.memory_allocated()=369551872
14:36:41.304: step:85/1000 train_loss:6.3069 train_time:17414ms step_avg:232.18ms torch.cuda.memory_allocated()=369551872
14:36:41.541: step:86/1000 train_loss:6.5841 train_time:17651ms step_avg:232.26ms torch.cuda.memory_allocated()=369551872
14:36:41.780: step:87/1000 train_loss:6.5460 train_time:17890ms step_avg:232.33ms torch.cuda.memory_allocated()=369551872
14:36:42.019: step:88/1000 train_loss:6.4011 train_time:18129ms step_avg:232.43ms torch.cuda.memory_allocated()=369551872
14:36:42.258: step:89/1000 train_loss:6.3407 train_time:18368ms step_avg:232.51ms torch.cuda.memory_allocated()=369551872
14:36:42.495: step:90/1000 train_loss:6.4708 train_time:18605ms step_avg:232.57ms torch.cuda.memory_allocated()=369551872
14:36:42.735: step:91/1000 train_loss:6.3312 train_time:18845ms step_avg:232.66ms torch.cuda.memory_allocated()=369551872
14:36:43.286: step:92/1000 train_loss:6.6279 train_time:19396ms step_avg:236.54ms torch.cuda.memory_allocated()=369551872
14:36:43.523: step:93/1000 train_loss:7.1377 train_time:19633ms step_avg:236.55ms torch.cuda.memory_allocated()=369551872
14:36:43.762: step:94/1000 train_loss:6.2772 train_time:19872ms step_avg:236.57ms torch.cuda.memory_allocated()=369551872
14:36:43.001: step:95/1000 train_loss:6.4390 train_time:20112ms step_avg:236.61ms torch.cuda.memory_allocated()=369551872
14:36:44.240: step:96/1000 train_loss:6.1008 train_time:20350ms step_avg:236.63ms torch.cuda.memory_allocated()=369551872
14:36:44.479: step:97/1000 train_loss:6.2031 train_time:20589ms step_avg:236.66ms torch.cuda.memory_allocated()=369551872
14:36:44.717: step:98/1000 train_loss:6.4536 train_time:20828ms step_avg:236.68ms torch.cuda.memory_allocated()=369551872
14:36:44.957: step:99/1000 train_loss:5.8659 train_time:21067ms step_avg:236.70ms torch.cuda.memory_allocated()=369551872
14:36:45.195: step:100/1000 train_loss:6.3644 train_time:21305ms step_avg:236.72ms torch.cuda.memory_allocated()=369551872
14:36:45.434: step:101/1000 train_loss:6.2889 train_time:21544ms step_avg:236.75ms torch.cuda.memory_allocated()=369551872
14:36:45.673: step:102/1000 train_loss:6.0443 train_time:21783ms step_avg:236.77ms torch.cuda.memory_allocated()=369551872
14:36:45.913: step:103/1000 train_loss:6.1813 train_time:22024ms step_avg:236.81ms torch.cuda.memory_allocated()=369551872
14:36:46.152: step:104/1000 train_loss:6.1345 train_time:22262ms step_avg:236.83ms torch.cuda.memory_allocated()=369551872
14:36:46.390: step:105/1000 train_loss:6.2195 train_time:22500ms step_avg:236.84ms torch.cuda.memory_allocated()=369551872
14:36:46.628: step:106/1000 train_loss:6.5243 train_time:22738ms step_avg:236.86ms torch.cuda.memory_allocated()=369551872
14:36:46.867: step:107/1000 train_loss:6.2606 train_time:22977ms step_avg:236.88ms torch.cuda.memory_allocated()=369551872
14:36:47.107: step:108/1000 train_loss:6.3537 train_time:23218ms step_avg:236.91ms torch.cuda.memory_allocated()=369551872
14:36:47.348: step:109/1000 train_loss:6.3239 train_time:23458ms step_avg:236.95ms torch.cuda.memory_allocated()=369551872
14:36:47.586: step:110/1000 train_loss:5.8382 train_time:23696ms step_avg:236.96ms torch.cuda.memory_allocated()=369551872
14:36:47.827: step:111/1000 train_loss:6.1450 train_time:23937ms step_avg:237.00ms torch.cuda.memory_allocated()=369551872
14:36:48.067: step:112/1000 train_loss:6.3321 train_time:24177ms step_avg:237.03ms torch.cuda.memory_allocated()=369551872
14:36:48.306: step:113/1000 train_loss:6.0547 train_time:24416ms step_avg:237.05ms torch.cuda.memory_allocated()=369551872
14:36:48.544: step:114/1000 train_loss:6.0475 train_time:24654ms step_avg:237.06ms torch.cuda.memory_allocated()=369551872
14:36:48.785: step:115/1000 train_loss:6.0155 train_time:24895ms step_avg:237.10ms torch.cuda.memory_allocated()=369551872
14:36:49.027: step:116/1000 train_loss:6.2120 train_time:25137ms step_avg:237.14ms torch.cuda.memory_allocated()=369551872
14:36:49.270: step:117/1000 train_loss:6.2960 train_time:25380ms step_avg:237.20ms torch.cuda.memory_allocated()=369551872
14:36:49.510: step:118/1000 train_loss:6.0859 train_time:25620ms step_avg:237.22ms torch.cuda.memory_allocated()=369551872
14:36:49.749: step:119/1000 train_loss:5.9664 train_time:25859ms step_avg:237.24ms torch.cuda.memory_allocated()=369551872
14:36:49.990: step:120/1000 train_loss:5.8118 train_time:26100ms step_avg:237.28ms torch.cuda.memory_allocated()=369551872
14:36:50.229: step:121/1000 train_loss:6.6353 train_time:26339ms step_avg:237.29ms torch.cuda.memory_allocated()=369551872
14:36:50.469: step:122/1000 train_loss:6.4764 train_time:26579ms step_avg:237.32ms torch.cuda.memory_allocated()=369551872
14:36:50.708: step:123/1000 train_loss:6.2163 train_time:26818ms step_avg:237.33ms torch.cuda.memory_allocated()=369551872
14:36:50.947: step:124/1000 train_loss:6.1670 train_time:27057ms step_avg:237.34ms torch.cuda.memory_allocated()=369551872
14:36:51.187: step:125/1000 train_loss:6.2012 train_time:27297ms step_avg:237.37ms torch.cuda.memory_allocated()=369551872
14:36:54.110: step:125/1000 val_loss:6.3523 train_time:27298ms step_avg:237.37ms
14:36:54.349: step:126/1000 train_loss:6.6348 train_time:27536ms step_avg:237.38ms torch.cuda.memory_allocated()=369551872
14:36:54.587: step:127/1000 train_loss:6.4472 train_time:27774ms step_avg:237.39ms torch.cuda.memory_allocated()=369551872
14:36:54.825: step:128/1000 train_loss:6.1800 train_time:28012ms step_avg:237.39ms torch.cuda.memory_allocated()=369551872
14:36:55.064: step:129/1000 train_loss:5.9377 train_time:28252ms step_avg:237.41ms torch.cuda.memory_allocated()=369551872
14:36:55.309: step:130/1000 train_loss:6.4222 train_time:28496ms step_avg:237.47ms torch.cuda.memory_allocated()=369551872
14:36:55.547: step:131/1000 train_loss:6.0917 train_time:28734ms step_avg:237.47ms torch.cuda.memory_allocated()=369551872
14:36:55.788: step:132/1000 train_loss:6.2353 train_time:28976ms step_avg:237.50ms torch.cuda.memory_allocated()=369551872
14:36:56.032: step:133/1000 train_loss:6.1994 train_time:29219ms step_avg:237.55ms torch.cuda.memory_allocated()=369551872
14:36:56.273: step:134/1000 train_loss:6.0974 train_time:29460ms step_avg:237.58ms torch.cuda.memory_allocated()=369551872
14:36:56.511: step:135/1000 train_loss:6.6395 train_time:29698ms step_avg:237.59ms torch.cuda.memory_allocated()=369551872
14:36:56.749: step:136/1000 train_loss:6.2433 train_time:29936ms step_avg:237.59ms torch.cuda.memory_allocated()=369551872
14:36:56.987: step:137/1000 train_loss:6.0781 train_time:30174ms step_avg:237.59ms torch.cuda.memory_allocated()=369551872
14:36:57.227: step:138/1000 train_loss:6.0355 train_time:30414ms step_avg:237.61ms torch.cuda.memory_allocated()=369551872
14:36:57.465: step:139/1000 train_loss:6.3802 train_time:30652ms step_avg:237.61ms torch.cuda.memory_allocated()=369551872
14:36:57.705: step:140/1000 train_loss:5.8144 train_time:30893ms step_avg:237.63ms torch.cuda.memory_allocated()=369551872
14:36:57.945: step:141/1000 train_loss:6.1458 train_time:31132ms step_avg:237.65ms torch.cuda.memory_allocated()=369551872
14:36:58.186: step:142/1000 train_loss:6.0619 train_time:31373ms step_avg:237.68ms torch.cuda.memory_allocated()=369551872
14:36:58.424: step:143/1000 train_loss:6.1245 train_time:31611ms step_avg:237.68ms torch.cuda.memory_allocated()=369551872
14:36:58.664: step:144/1000 train_loss:6.7425 train_time:31851ms step_avg:237.69ms torch.cuda.memory_allocated()=369551872
14:36:58.902: step:145/1000 train_loss:6.0816 train_time:32089ms step_avg:237.70ms torch.cuda.memory_allocated()=369551872
14:36:59.141: step:146/1000 train_loss:6.2348 train_time:32328ms step_avg:237.71ms torch.cuda.memory_allocated()=369551872
14:36:59.379: step:147/1000 train_loss:6.0712 train_time:32566ms step_avg:237.71ms torch.cuda.memory_allocated()=369551872
14:36:59.616: step:148/1000 train_loss:5.7593 train_time:32803ms step_avg:237.71ms torch.cuda.memory_allocated()=369551872
14:36:59.855: step:149/1000 train_loss:5.7700 train_time:33042ms step_avg:237.71ms torch.cuda.memory_allocated()=369551872
14:37:00.100: step:150/1000 train_loss:5.8169 train_time:33287ms step_avg:237.76ms torch.cuda.memory_allocated()=369551872
14:37:00.343: step:151/1000 train_loss:5.9262 train_time:33530ms step_avg:237.80ms torch.cuda.memory_allocated()=369551872
14:37:00.586: step:152/1000 train_loss:6.1731 train_time:33773ms step_avg:237.84ms torch.cuda.memory_allocated()=369551872
14:37:00.828: step:153/1000 train_loss:6.0186 train_time:34015ms step_avg:237.87ms torch.cuda.memory_allocated()=369551872
14:37:01.073: step:154/1000 train_loss:6.0729 train_time:34260ms step_avg:237.92ms torch.cuda.memory_allocated()=369551872
14:37:01.318: step:155/1000 train_loss:5.8608 train_time:34505ms step_avg:237.97ms torch.cuda.memory_allocated()=369551872
14:37:01.561: step:156/1000 train_loss:6.1651 train_time:34748ms step_avg:238.00ms torch.cuda.memory_allocated()=369551872
14:37:01.804: step:157/1000 train_loss:6.0840 train_time:34992ms step_avg:238.04ms torch.cuda.memory_allocated()=369551872
14:37:02.048: step:158/1000 train_loss:6.1521 train_time:35235ms step_avg:238.07ms torch.cuda.memory_allocated()=369551872
14:37:02.291: step:159/1000 train_loss:5.9020 train_time:35478ms step_avg:238.11ms torch.cuda.memory_allocated()=369551872
14:37:02.535: step:160/1000 train_loss:5.9295 train_time:35722ms step_avg:238.15ms torch.cuda.memory_allocated()=369551872
14:37:02.777: step:161/1000 train_loss:5.7894 train_time:35965ms step_avg:238.18ms torch.cuda.memory_allocated()=369551872
14:37:03.020: step:162/1000 train_loss:5.9876 train_time:36207ms step_avg:238.20ms torch.cuda.memory_allocated()=369551872
14:37:03.262: step:163/1000 train_loss:5.8697 train_time:36450ms step_avg:238.23ms torch.cuda.memory_allocated()=369551872
14:37:03.507: step:164/1000 train_loss:5.7667 train_time:36695ms step_avg:238.28ms torch.cuda.memory_allocated()=369551872
14:37:03.752: step:165/1000 train_loss:5.8945 train_time:36940ms step_avg:238.32ms torch.cuda.memory_allocated()=369551872
14:37:03.997: step:166/1000 train_loss:5.7738 train_time:37185ms step_avg:238.36ms torch.cuda.memory_allocated()=369551872
14:37:04.241: step:167/1000 train_loss:6.1067 train_time:37429ms step_avg:238.40ms torch.cuda.memory_allocated()=369551872
14:37:04.485: step:168/1000 train_loss:6.1914 train_time:37673ms step_avg:238.43ms torch.cuda.memory_allocated()=369551872
14:37:04.729: step:169/1000 train_loss:5.7629 train_time:37917ms step_avg:238.47ms torch.cuda.memory_allocated()=369551872
14:37:04.973: step:170/1000 train_loss:5.8108 train_time:38160ms step_avg:238.50ms torch.cuda.memory_allocated()=369551872
14:37:05.220: step:171/1000 train_loss:5.9252 train_time:38407ms step_avg:238.55ms torch.cuda.memory_allocated()=369551872
14:37:05.462: step:172/1000 train_loss:5.9118 train_time:38650ms step_avg:238.58ms torch.cuda.memory_allocated()=369551872
14:37:05.703: step:173/1000 train_loss:5.7365 train_time:38891ms step_avg:238.59ms torch.cuda.memory_allocated()=369551872
14:37:05.948: step:174/1000 train_loss:6.0864 train_time:39135ms step_avg:238.63ms torch.cuda.memory_allocated()=369551872
14:37:06.193: step:175/1000 train_loss:5.9835 train_time:39380ms step_avg:238.67ms torch.cuda.memory_allocated()=369551872
14:37:06.437: step:176/1000 train_loss:5.9726 train_time:39624ms step_avg:238.70ms torch.cuda.memory_allocated()=369551872
14:37:06.679: step:177/1000 train_loss:5.8105 train_time:39866ms step_avg:238.72ms torch.cuda.memory_allocated()=369551872
14:37:06.923: step:178/1000 train_loss:6.0102 train_time:40110ms step_avg:238.75ms torch.cuda.memory_allocated()=369551872
14:37:07.165: step:179/1000 train_loss:5.8304 train_time:40353ms step_avg:238.77ms torch.cuda.memory_allocated()=369551872
14:37:07.408: step:180/1000 train_loss:5.7253 train_time:40596ms step_avg:238.80ms torch.cuda.memory_allocated()=369551872
14:37:07.654: step:181/1000 train_loss:5.9096 train_time:40841ms step_avg:238.84ms torch.cuda.memory_allocated()=369551872
14:37:07.897: step:182/1000 train_loss:5.6726 train_time:41084ms step_avg:238.86ms torch.cuda.memory_allocated()=369551872
14:37:08.142: step:183/1000 train_loss:5.8234 train_time:41330ms step_avg:238.90ms torch.cuda.memory_allocated()=369551872
14:37:08.385: step:184/1000 train_loss:5.9059 train_time:41572ms step_avg:238.92ms torch.cuda.memory_allocated()=369551872
14:37:08.628: step:185/1000 train_loss:5.7451 train_time:41815ms step_avg:238.94ms torch.cuda.memory_allocated()=369551872
14:37:08.874: step:186/1000 train_loss:5.9461 train_time:42062ms step_avg:238.99ms torch.cuda.memory_allocated()=369551872
14:37:09.118: step:187/1000 train_loss:5.9586 train_time:42305ms step_avg:239.01ms torch.cuda.memory_allocated()=369551872
14:37:09.362: step:188/1000 train_loss:6.1317 train_time:42549ms step_avg:239.04ms torch.cuda.memory_allocated()=369551872
14:37:09.607: step:189/1000 train_loss:5.9646 train_time:42794ms step_avg:239.08ms torch.cuda.memory_allocated()=369551872
14:37:09.851: step:190/1000 train_loss:5.9916 train_time:43039ms step_avg:239.10ms torch.cuda.memory_allocated()=369551872
14:37:10.096: step:191/1000 train_loss:5.9770 train_time:43284ms step_avg:239.14ms torch.cuda.memory_allocated()=369551872
14:37:10.343: step:192/1000 train_loss:6.1435 train_time:43531ms step_avg:239.18ms torch.cuda.memory_allocated()=369551872
14:37:10.588: step:193/1000 train_loss:5.9415 train_time:43776ms step_avg:239.21ms torch.cuda.memory_allocated()=369551872
14:37:10.831: step:194/1000 train_loss:5.8562 train_time:44018ms step_avg:239.23ms torch.cuda.memory_allocated()=369551872
14:37:11.074: step:195/1000 train_loss:6.5089 train_time:44261ms step_avg:239.25ms torch.cuda.memory_allocated()=369551872
14:37:11.317: step:196/1000 train_loss:6.2407 train_time:44504ms step_avg:239.27ms torch.cuda.memory_allocated()=369551872
14:37:11.561: step:197/1000 train_loss:5.7921 train_time:44749ms step_avg:239.30ms torch.cuda.memory_allocated()=369551872
14:37:11.804: step:198/1000 train_loss:5.8595 train_time:44992ms step_avg:239.32ms torch.cuda.memory_allocated()=369551872
14:37:12.047: step:199/1000 train_loss:5.8730 train_time:45235ms step_avg:239.34ms torch.cuda.memory_allocated()=369551872
14:37:12.290: step:200/1000 train_loss:5.8754 train_time:45477ms step_avg:239.35ms torch.cuda.memory_allocated()=369551872
14:37:12.532: step:201/1000 train_loss:5.7730 train_time:45720ms step_avg:239.37ms torch.cuda.memory_allocated()=369551872
14:37:12.776: step:202/1000 train_loss:6.0198 train_time:45964ms step_avg:239.39ms torch.cuda.memory_allocated()=369551872
14:37:13.022: step:203/1000 train_loss:6.0551 train_time:46210ms step_avg:239.43ms torch.cuda.memory_allocated()=369551872
14:37:13.266: step:204/1000 train_loss:5.5668 train_time:46453ms step_avg:239.45ms torch.cuda.memory_allocated()=369551872
14:37:13.511: step:205/1000 train_loss:6.3360 train_time:46699ms step_avg:239.48ms torch.cuda.memory_allocated()=369551872
14:37:13.757: step:206/1000 train_loss:6.2690 train_time:46944ms step_avg:239.51ms torch.cuda.memory_allocated()=369551872
14:37:13.999: step:207/1000 train_loss:6.0776 train_time:47187ms step_avg:239.53ms torch.cuda.memory_allocated()=369551872
14:37:14.244: step:208/1000 train_loss:6.0668 train_time:47431ms step_avg:239.55ms torch.cuda.memory_allocated()=369551872
14:37:14.487: step:209/1000 train_loss:5.8081 train_time:47674ms step_avg:239.57ms torch.cuda.memory_allocated()=369551872
14:37:14.731: step:210/1000 train_loss:6.0849 train_time:47918ms step_avg:239.59ms torch.cuda.memory_allocated()=369551872
14:37:14.973: step:211/1000 train_loss:6.1152 train_time:48161ms step_avg:239.61ms torch.cuda.memory_allocated()=369551872
14:37:15.217: step:212/1000 train_loss:6.0005 train_time:48404ms step_avg:239.63ms torch.cuda.memory_allocated()=369551872
14:37:15.461: step:213/1000 train_loss:5.8960 train_time:48649ms step_avg:239.65ms torch.cuda.memory_allocated()=369551872
14:37:15.705: step:214/1000 train_loss:5.7673 train_time:48893ms step_avg:239.67ms torch.cuda.memory_allocated()=369551872
14:37:15.952: step:215/1000 train_loss:5.9718 train_time:49139ms step_avg:239.70ms torch.cuda.memory_allocated()=369551872
14:37:16.194: step:216/1000 train_loss:6.0809 train_time:49382ms step_avg:239.72ms torch.cuda.memory_allocated()=369551872
14:37:16.437: step:217/1000 train_loss:5.7433 train_time:49625ms step_avg:239.73ms torch.cuda.memory_allocated()=369551872
14:37:16.680: step:218/1000 train_loss:5.8400 train_time:49867ms step_avg:239.75ms torch.cuda.memory_allocated()=369551872
14:37:16.924: step:219/1000 train_loss:5.8942 train_time:50111ms step_avg:239.76ms torch.cuda.memory_allocated()=369551872
14:37:17.167: step:220/1000 train_loss:5.8910 train_time:50355ms step_avg:239.78ms torch.cuda.memory_allocated()=369551872
14:37:17.410: step:221/1000 train_loss:6.0735 train_time:50597ms step_avg:239.80ms torch.cuda.memory_allocated()=369551872
14:37:17.652: step:222/1000 train_loss:5.9326 train_time:50840ms step_avg:239.81ms torch.cuda.memory_allocated()=369551872
14:37:17.895: step:223/1000 train_loss:5.9043 train_time:51083ms step_avg:239.82ms torch.cuda.memory_allocated()=369551872
14:37:18.143: step:224/1000 train_loss:5.7288 train_time:51330ms step_avg:239.86ms torch.cuda.memory_allocated()=369551872
14:37:18.390: step:225/1000 train_loss:5.6632 train_time:51577ms step_avg:239.89ms torch.cuda.memory_allocated()=369551872
14:37:18.638: step:226/1000 train_loss:6.4929 train_time:51826ms step_avg:239.93ms torch.cuda.memory_allocated()=369551872
14:37:18.886: step:227/1000 train_loss:5.7708 train_time:52073ms step_avg:239.97ms torch.cuda.memory_allocated()=369551872
14:37:19.133: step:228/1000 train_loss:5.7416 train_time:52321ms step_avg:240.00ms torch.cuda.memory_allocated()=369551872
14:37:19.380: step:229/1000 train_loss:5.7544 train_time:52567ms step_avg:240.03ms torch.cuda.memory_allocated()=369551872
14:37:19.627: step:230/1000 train_loss:5.8052 train_time:52814ms step_avg:240.06ms torch.cuda.memory_allocated()=369551872
14:37:19.873: step:231/1000 train_loss:5.9767 train_time:53060ms step_avg:240.09ms torch.cuda.memory_allocated()=369551872
14:37:20.120: step:232/1000 train_loss:5.6232 train_time:53307ms step_avg:240.12ms torch.cuda.memory_allocated()=369551872
14:37:20.369: step:233/1000 train_loss:5.9915 train_time:53556ms step_avg:240.16ms torch.cuda.memory_allocated()=369551872
14:37:20.617: step:234/1000 train_loss:5.6044 train_time:53804ms step_avg:240.20ms torch.cuda.memory_allocated()=369551872
14:37:20.864: step:235/1000 train_loss:5.8793 train_time:54051ms step_avg:240.23ms torch.cuda.memory_allocated()=369551872
14:37:21.112: step:236/1000 train_loss:5.6962 train_time:54299ms step_avg:240.26ms torch.cuda.memory_allocated()=369551872
14:37:21.361: step:237/1000 train_loss:5.5395 train_time:54548ms step_avg:240.30ms torch.cuda.memory_allocated()=369551872
14:37:21.609: step:238/1000 train_loss:5.6877 train_time:54796ms step_avg:240.33ms torch.cuda.memory_allocated()=369551872
14:37:21.877: step:239/1000 train_loss:5.8176 train_time:55064ms step_avg:240.45ms torch.cuda.memory_allocated()=369551872
14:37:22.152: step:240/1000 train_loss:5.6471 train_time:55339ms step_avg:240.60ms torch.cuda.memory_allocated()=369551872
14:37:22.435: step:241/1000 train_loss:4.7882 train_time:55623ms step_avg:240.79ms torch.cuda.memory_allocated()=369551872
14:37:22.693: step:242/1000 train_loss:5.8107 train_time:55880ms step_avg:240.86ms torch.cuda.memory_allocated()=369551872
14:37:22.940: step:243/1000 train_loss:5.9593 train_time:56127ms step_avg:240.89ms torch.cuda.memory_allocated()=369551872
14:37:23.188: step:244/1000 train_loss:5.6700 train_time:56375ms step_avg:240.92ms torch.cuda.memory_allocated()=369551872
14:37:23.434: step:245/1000 train_loss:5.6731 train_time:56621ms step_avg:240.94ms torch.cuda.memory_allocated()=369551872
14:37:23.682: step:246/1000 train_loss:5.6753 train_time:56869ms step_avg:240.97ms torch.cuda.memory_allocated()=369551872
14:37:23.932: step:247/1000 train_loss:5.8521 train_time:57120ms step_avg:241.01ms torch.cuda.memory_allocated()=369551872
14:37:24.179: step:248/1000 train_loss:5.6918 train_time:57366ms step_avg:241.03ms torch.cuda.memory_allocated()=369551872
14:37:24.426: step:249/1000 train_loss:6.2050 train_time:57613ms step_avg:241.06ms torch.cuda.memory_allocated()=369551872
14:37:24.675: step:250/1000 train_loss:5.7923 train_time:57862ms step_avg:241.09ms torch.cuda.memory_allocated()=369551872
14:37:27.344: step:250/1000 val_loss:5.9551 train_time:57862ms step_avg:241.09ms
14:37:27.592: step:251/1000 train_loss:5.5890 train_time:58110ms step_avg:241.12ms torch.cuda.memory_allocated()=369551872
14:37:27.841: step:252/1000 train_loss:5.9327 train_time:58359ms step_avg:241.15ms torch.cuda.memory_allocated()=369551872
14:37:28.091: step:253/1000 train_loss:5.5232 train_time:58610ms step_avg:241.19ms torch.cuda.memory_allocated()=369551872
14:37:28.340: step:254/1000 train_loss:5.3532 train_time:58859ms step_avg:241.22ms torch.cuda.memory_allocated()=369551872
14:37:28.589: step:255/1000 train_loss:5.5672 train_time:59107ms step_avg:241.25ms torch.cuda.memory_allocated()=369551872
14:37:28.837: step:256/1000 train_loss:5.8281 train_time:59356ms step_avg:241.28ms torch.cuda.memory_allocated()=369551872
14:37:29.085: step:257/1000 train_loss:5.9947 train_time:59603ms step_avg:241.31ms torch.cuda.memory_allocated()=369551872
14:37:29.333: step:258/1000 train_loss:5.7684 train_time:59852ms step_avg:241.34ms torch.cuda.memory_allocated()=369551872
14:37:29.579: step:259/1000 train_loss:5.8469 train_time:60098ms step_avg:241.36ms torch.cuda.memory_allocated()=369551872
14:37:29.827: step:260/1000 train_loss:5.8508 train_time:60345ms step_avg:241.38ms torch.cuda.memory_allocated()=369551872
14:37:30.075: step:261/1000 train_loss:6.0191 train_time:60593ms step_avg:241.41ms torch.cuda.memory_allocated()=369551872
14:37:30.324: step:262/1000 train_loss:5.7355 train_time:60842ms step_avg:241.44ms torch.cuda.memory_allocated()=369551872
14:37:30.570: step:263/1000 train_loss:5.8221 train_time:61089ms step_avg:241.46ms torch.cuda.memory_allocated()=369551872
14:37:30.817: step:264/1000 train_loss:5.7527 train_time:61335ms step_avg:241.48ms torch.cuda.memory_allocated()=369551872
14:37:31.064: step:265/1000 train_loss:5.9286 train_time:61583ms step_avg:241.50ms torch.cuda.memory_allocated()=369551872
14:37:31.311: step:266/1000 train_loss:5.6571 train_time:61829ms step_avg:241.52ms torch.cuda.memory_allocated()=369551872
14:37:31.559: step:267/1000 train_loss:5.8843 train_time:62077ms step_avg:241.55ms torch.cuda.memory_allocated()=369551872
14:37:31.806: step:268/1000 train_loss:5.8117 train_time:62324ms step_avg:241.57ms torch.cuda.memory_allocated()=369551872
14:37:32.055: step:269/1000 train_loss:5.8771 train_time:62574ms step_avg:241.60ms torch.cuda.memory_allocated()=369551872
14:37:32.312: step:270/1000 train_loss:5.9107 train_time:62830ms step_avg:241.65ms torch.cuda.memory_allocated()=369551872
14:37:32.593: step:271/1000 train_loss:5.8234 train_time:63111ms step_avg:241.81ms torch.cuda.memory_allocated()=369551872
14:37:32.877: step:272/1000 train_loss:6.1417 train_time:63395ms step_avg:241.97ms torch.cuda.memory_allocated()=369551872
14:37:33.158: step:273/1000 train_loss:5.9048 train_time:63676ms step_avg:242.11ms torch.cuda.memory_allocated()=369551872
14:37:33.429: step:274/1000 train_loss:5.8360 train_time:63947ms step_avg:242.23ms torch.cuda.memory_allocated()=369551872
14:37:33.698: step:275/1000 train_loss:5.8855 train_time:64216ms step_avg:242.33ms torch.cuda.memory_allocated()=369551872
14:37:33.952: step:276/1000 train_loss:6.0873 train_time:64471ms step_avg:242.37ms torch.cuda.memory_allocated()=369551872
14:37:34.200: step:277/1000 train_loss:5.6724 train_time:64718ms step_avg:242.39ms torch.cuda.memory_allocated()=369551872
14:37:34.447: step:278/1000 train_loss:5.6417 train_time:64966ms step_avg:242.41ms torch.cuda.memory_allocated()=369551872
14:37:34.694: step:279/1000 train_loss:5.7398 train_time:65213ms step_avg:242.43ms torch.cuda.memory_allocated()=369551872
14:37:34.941: step:280/1000 train_loss:5.8211 train_time:65460ms step_avg:242.44ms torch.cuda.memory_allocated()=369551872
14:37:35.186: step:281/1000 train_loss:5.5990 train_time:65705ms step_avg:242.45ms torch.cuda.memory_allocated()=369551872
14:37:35.434: step:282/1000 train_loss:5.6616 train_time:65953ms step_avg:242.47ms torch.cuda.memory_allocated()=369551872
14:37:35.682: step:283/1000 train_loss:5.5084 train_time:66201ms step_avg:242.49ms torch.cuda.memory_allocated()=369551872
14:37:35.931: step:284/1000 train_loss:5.7350 train_time:66450ms step_avg:242.52ms torch.cuda.memory_allocated()=369551872
14:37:36.180: step:285/1000 train_loss:6.0030 train_time:66698ms step_avg:242.54ms torch.cuda.memory_allocated()=369551872
14:37:36.427: step:286/1000 train_loss:6.8400 train_time:66946ms step_avg:242.56ms torch.cuda.memory_allocated()=369551872
14:37:36.674: step:287/1000 train_loss:6.3006 train_time:67192ms step_avg:242.57ms torch.cuda.memory_allocated()=369551872
14:37:36.921: step:288/1000 train_loss:5.9929 train_time:67440ms step_avg:242.59ms torch.cuda.memory_allocated()=369551872
14:37:37.169: step:289/1000 train_loss:6.0183 train_time:67687ms step_avg:242.61ms torch.cuda.memory_allocated()=369551872
14:37:37.416: step:290/1000 train_loss:5.7581 train_time:67934ms step_avg:242.62ms torch.cuda.memory_allocated()=369551872
14:37:37.662: step:291/1000 train_loss:5.2978 train_time:68181ms step_avg:242.64ms torch.cuda.memory_allocated()=369551872
14:37:37.910: step:292/1000 train_loss:6.0058 train_time:68428ms step_avg:242.65ms torch.cuda.memory_allocated()=369551872
14:37:38.158: step:293/1000 train_loss:5.4510 train_time:68676ms step_avg:242.67ms torch.cuda.memory_allocated()=369551872
14:37:38.407: step:294/1000 train_loss:5.3969 train_time:68925ms step_avg:242.69ms torch.cuda.memory_allocated()=369551872
14:37:38.656: step:295/1000 train_loss:5.7162 train_time:69174ms step_avg:242.72ms torch.cuda.memory_allocated()=369551872
14:37:38.903: step:296/1000 train_loss:5.7431 train_time:69421ms step_avg:242.73ms torch.cuda.memory_allocated()=369551872
14:37:39.148: step:297/1000 train_loss:5.6247 train_time:69667ms step_avg:242.74ms torch.cuda.memory_allocated()=369551872
14:37:39.399: step:298/1000 train_loss:5.4567 train_time:69918ms step_avg:242.77ms torch.cuda.memory_allocated()=369551872
14:37:39.652: step:299/1000 train_loss:5.9482 train_time:70170ms step_avg:242.80ms torch.cuda.memory_allocated()=369551872
14:37:39.902: step:300/1000 train_loss:5.7877 train_time:70421ms step_avg:242.83ms torch.cuda.memory_allocated()=369551872
14:37:40.154: step:301/1000 train_loss:5.6697 train_time:70673ms step_avg:242.86ms torch.cuda.memory_allocated()=369551872
14:37:40.405: step:302/1000 train_loss:5.2848 train_time:70923ms step_avg:242.89ms torch.cuda.memory_allocated()=369551872
14:37:40.657: step:303/1000 train_loss:5.5475 train_time:71175ms step_avg:242.92ms torch.cuda.memory_allocated()=369551872
14:37:40.906: step:304/1000 train_loss:5.7837 train_time:71424ms step_avg:242.94ms torch.cuda.memory_allocated()=369551872
14:37:41.158: step:305/1000 train_loss:5.5924 train_time:71676ms step_avg:242.97ms torch.cuda.memory_allocated()=369551872
14:37:41.407: step:306/1000 train_loss:5.8669 train_time:71925ms step_avg:242.99ms torch.cuda.memory_allocated()=369551872
14:37:41.657: step:307/1000 train_loss:5.6783 train_time:72175ms step_avg:243.01ms torch.cuda.memory_allocated()=369551872
14:37:41.907: step:308/1000 train_loss:5.8035 train_time:72425ms step_avg:243.04ms torch.cuda.memory_allocated()=369551872
14:37:42.157: step:309/1000 train_loss:5.5275 train_time:72676ms step_avg:243.06ms torch.cuda.memory_allocated()=369551872
14:37:42.407: step:310/1000 train_loss:5.6959 train_time:72925ms step_avg:243.08ms torch.cuda.memory_allocated()=369551872
14:37:42.657: step:311/1000 train_loss:5.5895 train_time:73175ms step_avg:243.11ms torch.cuda.memory_allocated()=369551872
14:37:42.917: step:312/1000 train_loss:5.6326 train_time:73436ms step_avg:243.16ms torch.cuda.memory_allocated()=369551872
14:37:43.203: step:313/1000 train_loss:5.4495 train_time:73721ms step_avg:243.30ms torch.cuda.memory_allocated()=369551872
14:37:43.475: step:314/1000 train_loss:5.6307 train_time:73994ms step_avg:243.40ms torch.cuda.memory_allocated()=369551872
14:37:43.743: step:315/1000 train_loss:5.8184 train_time:74262ms step_avg:243.48ms torch.cuda.memory_allocated()=369551872
14:37:44.013: step:316/1000 train_loss:5.7597 train_time:74531ms step_avg:243.57ms torch.cuda.memory_allocated()=369551872
14:37:44.290: step:317/1000 train_loss:5.6219 train_time:74808ms step_avg:243.67ms torch.cuda.memory_allocated()=369551872
14:37:44.553: step:318/1000 train_loss:5.6355 train_time:75072ms step_avg:243.74ms torch.cuda.memory_allocated()=369551872
14:37:44.804: step:319/1000 train_loss:5.7126 train_time:75323ms step_avg:243.76ms torch.cuda.memory_allocated()=369551872
14:37:45.056: step:320/1000 train_loss:5.5562 train_time:75575ms step_avg:243.79ms torch.cuda.memory_allocated()=369551872
14:37:45.308: step:321/1000 train_loss:5.5080 train_time:75826ms step_avg:243.82ms torch.cuda.memory_allocated()=369551872
14:37:45.558: step:322/1000 train_loss:5.6967 train_time:76076ms step_avg:243.83ms torch.cuda.memory_allocated()=369551872
14:37:45.808: step:323/1000 train_loss:5.8508 train_time:76327ms step_avg:243.86ms torch.cuda.memory_allocated()=369551872
14:37:46.058: step:324/1000 train_loss:5.7829 train_time:76576ms step_avg:243.87ms torch.cuda.memory_allocated()=369551872
14:37:46.315: step:325/1000 train_loss:5.6293 train_time:76833ms step_avg:243.92ms torch.cuda.memory_allocated()=369551872
14:37:46.565: step:326/1000 train_loss:5.6911 train_time:77083ms step_avg:243.94ms torch.cuda.memory_allocated()=369551872
14:37:46.815: step:327/1000 train_loss:5.6788 train_time:77333ms step_avg:243.95ms torch.cuda.memory_allocated()=369551872
14:37:47.066: step:328/1000 train_loss:5.6260 train_time:77584ms step_avg:243.98ms torch.cuda.memory_allocated()=369551872
14:37:47.317: step:329/1000 train_loss:5.6639 train_time:77835ms step_avg:244.00ms torch.cuda.memory_allocated()=369551872
14:37:47.568: step:330/1000 train_loss:5.5527 train_time:78087ms step_avg:244.02ms torch.cuda.memory_allocated()=369551872
14:37:47.820: step:331/1000 train_loss:5.5554 train_time:78339ms step_avg:244.05ms torch.cuda.memory_allocated()=369551872
14:37:48.071: step:332/1000 train_loss:5.9474 train_time:78590ms step_avg:244.07ms torch.cuda.memory_allocated()=369551872
14:37:48.323: step:333/1000 train_loss:5.5271 train_time:78841ms step_avg:244.09ms torch.cuda.memory_allocated()=369551872
14:37:48.572: step:334/1000 train_loss:5.4315 train_time:79091ms step_avg:244.11ms torch.cuda.memory_allocated()=369551872
14:37:48.823: step:335/1000 train_loss:5.7881 train_time:79341ms step_avg:244.13ms torch.cuda.memory_allocated()=369551872
14:37:49.075: step:336/1000 train_loss:5.5150 train_time:79593ms step_avg:244.15ms torch.cuda.memory_allocated()=369551872
14:37:49.328: step:337/1000 train_loss:5.6832 train_time:79847ms step_avg:244.18ms torch.cuda.memory_allocated()=369551872
14:37:49.577: step:338/1000 train_loss:5.6065 train_time:80096ms step_avg:244.19ms torch.cuda.memory_allocated()=369551872
14:37:49.829: step:339/1000 train_loss:5.7209 train_time:80348ms step_avg:244.22ms torch.cuda.memory_allocated()=369551872
14:37:50.079: step:340/1000 train_loss:5.6026 train_time:80597ms step_avg:244.23ms torch.cuda.memory_allocated()=369551872
14:37:50.329: step:341/1000 train_loss:5.4463 train_time:80847ms step_avg:244.25ms torch.cuda.memory_allocated()=369551872
14:37:50.580: step:342/1000 train_loss:5.9059 train_time:81099ms step_avg:244.27ms torch.cuda.memory_allocated()=369551872
14:37:50.828: step:343/1000 train_loss:5.5411 train_time:81346ms step_avg:244.28ms torch.cuda.memory_allocated()=369551872
14:37:51.076: step:344/1000 train_loss:5.7566 train_time:81595ms step_avg:244.30ms torch.cuda.memory_allocated()=369551872
14:37:51.325: step:345/1000 train_loss:5.6747 train_time:81844ms step_avg:244.31ms torch.cuda.memory_allocated()=369551872
14:37:51.575: step:346/1000 train_loss:5.4215 train_time:82093ms step_avg:244.33ms torch.cuda.memory_allocated()=369551872
14:37:51.825: step:347/1000 train_loss:5.5927 train_time:82343ms step_avg:244.34ms torch.cuda.memory_allocated()=369551872
14:37:52.077: step:348/1000 train_loss:5.5577 train_time:82595ms step_avg:244.36ms torch.cuda.memory_allocated()=369551872
14:37:52.326: step:349/1000 train_loss:6.3665 train_time:82845ms step_avg:244.38ms torch.cuda.memory_allocated()=369551872
14:37:52.576: step:350/1000 train_loss:5.5724 train_time:83094ms step_avg:244.39ms torch.cuda.memory_allocated()=369551872
14:37:52.828: step:351/1000 train_loss:5.7102 train_time:83346ms step_avg:244.42ms torch.cuda.memory_allocated()=369551872
14:37:53.078: step:352/1000 train_loss:5.4994 train_time:83596ms step_avg:244.43ms torch.cuda.memory_allocated()=369551872
14:37:53.327: step:353/1000 train_loss:5.6061 train_time:83845ms step_avg:244.45ms torch.cuda.memory_allocated()=369551872
14:37:53.577: step:354/1000 train_loss:5.7816 train_time:84096ms step_avg:244.46ms torch.cuda.memory_allocated()=369551872
14:37:53.828: step:355/1000 train_loss:5.5316 train_time:84347ms step_avg:244.48ms torch.cuda.memory_allocated()=369551872
14:37:54.080: step:356/1000 train_loss:5.5413 train_time:84599ms step_avg:244.50ms torch.cuda.memory_allocated()=369551872
14:37:54.331: step:357/1000 train_loss:5.6878 train_time:84850ms step_avg:244.52ms torch.cuda.memory_allocated()=369551872
14:37:54.581: step:358/1000 train_loss:5.3821 train_time:85100ms step_avg:244.54ms torch.cuda.memory_allocated()=369551872
14:37:54.831: step:359/1000 train_loss:5.8176 train_time:85349ms step_avg:244.55ms torch.cuda.memory_allocated()=369551872
14:37:55.080: step:360/1000 train_loss:5.6559 train_time:85598ms step_avg:244.57ms torch.cuda.memory_allocated()=369551872
14:37:55.333: step:361/1000 train_loss:5.7063 train_time:85851ms step_avg:244.59ms torch.cuda.memory_allocated()=369551872
14:37:55.586: step:362/1000 train_loss:5.7903 train_time:86104ms step_avg:244.61ms torch.cuda.memory_allocated()=369551872
14:37:55.836: step:363/1000 train_loss:5.6179 train_time:86354ms step_avg:244.63ms torch.cuda.memory_allocated()=369551872
14:37:56.085: step:364/1000 train_loss:5.4718 train_time:86604ms step_avg:244.64ms torch.cuda.memory_allocated()=369551872
14:37:56.340: step:365/1000 train_loss:5.5294 train_time:86859ms step_avg:244.67ms torch.cuda.memory_allocated()=369551872
14:37:56.588: step:366/1000 train_loss:5.6513 train_time:87107ms step_avg:244.68ms torch.cuda.memory_allocated()=369551872
14:37:56.837: step:367/1000 train_loss:5.3121 train_time:87356ms step_avg:244.69ms torch.cuda.memory_allocated()=369551872
14:37:57.088: step:368/1000 train_loss:5.9959 train_time:87607ms step_avg:244.71ms torch.cuda.memory_allocated()=369551872
14:37:57.339: step:369/1000 train_loss:5.5519 train_time:87858ms step_avg:244.73ms torch.cuda.memory_allocated()=369551872
14:37:57.590: step:370/1000 train_loss:5.4616 train_time:88108ms step_avg:244.74ms torch.cuda.memory_allocated()=369551872
14:37:57.849: step:371/1000 train_loss:5.7286 train_time:88367ms step_avg:244.78ms torch.cuda.memory_allocated()=369551872
14:37:58.105: step:372/1000 train_loss:5.4206 train_time:88624ms step_avg:244.82ms torch.cuda.memory_allocated()=369551872
14:37:58.358: step:373/1000 train_loss:5.7289 train_time:88876ms step_avg:244.84ms torch.cuda.memory_allocated()=369551872
14:37:58.611: step:374/1000 train_loss:5.7187 train_time:89129ms step_avg:244.86ms torch.cuda.memory_allocated()=369551872
14:37:58.867: step:375/1000 train_loss:5.9219 train_time:89385ms step_avg:244.89ms torch.cuda.memory_allocated()=369551872
14:38:01.575: step:375/1000 val_loss:5.7434 train_time:89385ms step_avg:244.89ms
14:38:01.830: step:376/1000 train_loss:5.9067 train_time:89640ms step_avg:244.92ms torch.cuda.memory_allocated()=369551872
14:38:02.086: step:377/1000 train_loss:5.5427 train_time:89895ms step_avg:244.95ms torch.cuda.memory_allocated()=369551872
14:38:02.336: step:378/1000 train_loss:5.8060 train_time:90146ms step_avg:244.96ms torch.cuda.memory_allocated()=369551872
14:38:02.590: step:379/1000 train_loss:5.7139 train_time:90400ms step_avg:244.99ms torch.cuda.memory_allocated()=369551872
14:38:02.846: step:380/1000 train_loss:5.4591 train_time:90655ms step_avg:245.01ms torch.cuda.memory_allocated()=369551872
14:38:03.101: step:381/1000 train_loss:5.3819 train_time:90910ms step_avg:245.04ms torch.cuda.memory_allocated()=369551872
14:38:03.355: step:382/1000 train_loss:5.7391 train_time:91164ms step_avg:245.07ms torch.cuda.memory_allocated()=369551872
14:38:03.609: step:383/1000 train_loss:5.6851 train_time:91418ms step_avg:245.09ms torch.cuda.memory_allocated()=369551872
14:38:03.867: step:384/1000 train_loss:5.5231 train_time:91676ms step_avg:245.12ms torch.cuda.memory_allocated()=369551872
14:38:04.121: step:385/1000 train_loss:5.5346 train_time:91931ms step_avg:245.15ms torch.cuda.memory_allocated()=369551872
14:38:04.374: step:386/1000 train_loss:5.4996 train_time:92184ms step_avg:245.17ms torch.cuda.memory_allocated()=369551872
14:38:04.627: step:387/1000 train_loss:5.6026 train_time:92436ms step_avg:245.19ms torch.cuda.memory_allocated()=369551872
14:38:04.879: step:388/1000 train_loss:5.5819 train_time:92689ms step_avg:245.21ms torch.cuda.memory_allocated()=369551872
14:38:05.131: step:389/1000 train_loss:5.4891 train_time:92940ms step_avg:245.22ms torch.cuda.memory_allocated()=369551872
14:38:05.384: step:390/1000 train_loss:5.7012 train_time:93193ms step_avg:245.25ms torch.cuda.memory_allocated()=369551872
14:38:05.636: step:391/1000 train_loss:5.5512 train_time:93445ms step_avg:245.26ms torch.cuda.memory_allocated()=369551872
14:38:05.903: step:392/1000 train_loss:5.6094 train_time:93712ms step_avg:245.32ms torch.cuda.memory_allocated()=369551872
14:38:06.173: step:393/1000 train_loss:5.7009 train_time:93983ms step_avg:245.39ms torch.cuda.memory_allocated()=369551872
14:38:06.464: step:394/1000 train_loss:5.9617 train_time:94273ms step_avg:245.50ms torch.cuda.memory_allocated()=369551872
14:38:06.760: step:395/1000 train_loss:5.7364 train_time:94569ms step_avg:245.63ms torch.cuda.memory_allocated()=369551872
14:38:07.047: step:396/1000 train_loss:5.5975 train_time:94857ms step_avg:245.74ms torch.cuda.memory_allocated()=369551872
14:38:07.306: step:397/1000 train_loss:5.7139 train_time:95115ms step_avg:245.78ms torch.cuda.memory_allocated()=369551872
14:38:07.564: step:398/1000 train_loss:5.4041 train_time:95373ms step_avg:245.81ms torch.cuda.memory_allocated()=369551872
14:38:07.825: step:399/1000 train_loss:5.8563 train_time:95635ms step_avg:245.85ms torch.cuda.memory_allocated()=369551872
14:38:08.077: step:400/1000 train_loss:5.5362 train_time:95886ms step_avg:245.86ms torch.cuda.memory_allocated()=369551872
14:38:08.328: step:401/1000 train_loss:5.5861 train_time:96138ms step_avg:245.88ms torch.cuda.memory_allocated()=369551872
14:38:08.581: step:402/1000 train_loss:5.6185 train_time:96390ms step_avg:245.89ms torch.cuda.memory_allocated()=369551872
14:38:08.832: step:403/1000 train_loss:5.6809 train_time:96642ms step_avg:245.91ms torch.cuda.memory_allocated()=369551872
14:38:09.087: step:404/1000 train_loss:5.5016 train_time:96896ms step_avg:245.93ms torch.cuda.memory_allocated()=369551872
14:38:09.343: step:405/1000 train_loss:5.4804 train_time:97153ms step_avg:245.96ms torch.cuda.memory_allocated()=369551872
14:38:09.596: step:406/1000 train_loss:5.3917 train_time:97406ms step_avg:245.97ms torch.cuda.memory_allocated()=369551872
14:38:09.864: step:407/1000 train_loss:5.6436 train_time:97673ms step_avg:246.03ms torch.cuda.memory_allocated()=369551872
14:38:10.143: step:408/1000 train_loss:5.3676 train_time:97952ms step_avg:246.11ms torch.cuda.memory_allocated()=369551872
14:38:10.407: step:409/1000 train_loss:5.8558 train_time:98216ms step_avg:246.16ms torch.cuda.memory_allocated()=369551872
14:38:10.662: step:410/1000 train_loss:5.5644 train_time:98471ms step_avg:246.18ms torch.cuda.memory_allocated()=369551872
14:38:10.915: step:411/1000 train_loss:5.5622 train_time:98725ms step_avg:246.20ms torch.cuda.memory_allocated()=369551872
14:38:11.169: step:412/1000 train_loss:5.7659 train_time:98979ms step_avg:246.22ms torch.cuda.memory_allocated()=369551872
14:38:11.422: step:413/1000 train_loss:5.5922 train_time:99232ms step_avg:246.23ms torch.cuda.memory_allocated()=369551872
14:38:11.676: step:414/1000 train_loss:5.3515 train_time:99486ms step_avg:246.25ms torch.cuda.memory_allocated()=369551872
14:38:11.933: step:415/1000 train_loss:5.5190 train_time:99742ms step_avg:246.28ms torch.cuda.memory_allocated()=369551872
14:38:12.185: step:416/1000 train_loss:5.4989 train_time:99994ms step_avg:246.29ms torch.cuda.memory_allocated()=369551872
14:38:12.439: step:417/1000 train_loss:5.3823 train_time:100249ms step_avg:246.31ms torch.cuda.memory_allocated()=369551872
14:38:12.694: step:418/1000 train_loss:5.4958 train_time:100504ms step_avg:246.33ms torch.cuda.memory_allocated()=369551872
14:38:12.949: step:419/1000 train_loss:5.6999 train_time:100758ms step_avg:246.35ms torch.cuda.memory_allocated()=369551872
14:38:13.203: step:420/1000 train_loss:5.5236 train_time:101013ms step_avg:246.37ms torch.cuda.memory_allocated()=369551872
14:38:13.458: step:421/1000 train_loss:5.6228 train_time:101268ms step_avg:246.39ms torch.cuda.memory_allocated()=369551872
14:38:13.713: step:422/1000 train_loss:5.6334 train_time:101523ms step_avg:246.41ms torch.cuda.memory_allocated()=369551872
14:38:13.969: step:423/1000 train_loss:5.5018 train_time:101778ms step_avg:246.44ms torch.cuda.memory_allocated()=369551872
14:38:14.219: step:424/1000 train_loss:5.4922 train_time:102029ms step_avg:246.45ms torch.cuda.memory_allocated()=369551872
14:38:14.477: step:425/1000 train_loss:5.1072 train_time:102286ms step_avg:246.47ms torch.cuda.memory_allocated()=369551872
14:38:14.733: step:426/1000 train_loss:5.3140 train_time:102543ms step_avg:246.50ms torch.cuda.memory_allocated()=369551872
14:38:15.015: step:427/1000 train_loss:5.3830 train_time:102824ms step_avg:246.58ms torch.cuda.memory_allocated()=369551872
14:38:15.286: step:428/1000 train_loss:5.3240 train_time:103096ms step_avg:246.64ms torch.cuda.memory_allocated()=369551872
14:38:15.543: step:429/1000 train_loss:5.7810 train_time:103352ms step_avg:246.66ms torch.cuda.memory_allocated()=369551872
14:38:15.811: step:430/1000 train_loss:6.0916 train_time:103621ms step_avg:246.72ms torch.cuda.memory_allocated()=369551872
14:38:16.093: step:431/1000 train_loss:5.5591 train_time:103903ms step_avg:246.80ms torch.cuda.memory_allocated()=369551872
14:38:16.368: step:432/1000 train_loss:5.4317 train_time:104177ms step_avg:246.86ms torch.cuda.memory_allocated()=369551872
14:38:16.655: step:433/1000 train_loss:5.5257 train_time:104465ms step_avg:246.96ms torch.cuda.memory_allocated()=369551872
14:38:16.922: step:434/1000 train_loss:5.7308 train_time:104732ms step_avg:247.01ms torch.cuda.memory_allocated()=369551872
14:38:17.177: step:435/1000 train_loss:5.3561 train_time:104987ms step_avg:247.03ms torch.cuda.memory_allocated()=369551872
14:38:17.429: step:436/1000 train_loss:5.3211 train_time:105239ms step_avg:247.04ms torch.cuda.memory_allocated()=369551872
14:38:17.684: step:437/1000 train_loss:5.3370 train_time:105494ms step_avg:247.06ms torch.cuda.memory_allocated()=369551872
14:38:17.944: step:438/1000 train_loss:5.3844 train_time:105754ms step_avg:247.09ms torch.cuda.memory_allocated()=369551872
14:38:18.199: step:439/1000 train_loss:5.1014 train_time:106009ms step_avg:247.11ms torch.cuda.memory_allocated()=369551872
14:38:18.455: step:440/1000 train_loss:5.1923 train_time:106265ms step_avg:247.13ms torch.cuda.memory_allocated()=369551872
14:38:18.709: step:441/1000 train_loss:5.4657 train_time:106519ms step_avg:247.14ms torch.cuda.memory_allocated()=369551872
14:38:18.963: step:442/1000 train_loss:5.3932 train_time:106773ms step_avg:247.16ms torch.cuda.memory_allocated()=369551872
14:38:19.217: step:443/1000 train_loss:5.5252 train_time:107027ms step_avg:247.18ms torch.cuda.memory_allocated()=369551872
14:38:19.480: step:444/1000 train_loss:5.2935 train_time:107289ms step_avg:247.21ms torch.cuda.memory_allocated()=369551872
14:38:19.757: step:445/1000 train_loss:5.5696 train_time:107566ms step_avg:247.28ms torch.cuda.memory_allocated()=369551872
14:38:20.027: step:446/1000 train_loss:5.5233 train_time:107836ms step_avg:247.33ms torch.cuda.memory_allocated()=369551872
14:38:20.321: step:447/1000 train_loss:5.5388 train_time:108131ms step_avg:247.44ms torch.cuda.memory_allocated()=369551872
14:38:20.604: step:448/1000 train_loss:5.5603 train_time:108414ms step_avg:247.52ms torch.cuda.memory_allocated()=369551872
14:38:20.860: step:449/1000 train_loss:5.3915 train_time:108670ms step_avg:247.54ms torch.cuda.memory_allocated()=369551872
14:38:21.118: step:450/1000 train_loss:5.4931 train_time:108927ms step_avg:247.56ms torch.cuda.memory_allocated()=369551872
14:38:21.378: step:451/1000 train_loss:5.5017 train_time:109187ms step_avg:247.59ms torch.cuda.memory_allocated()=369551872
14:38:21.636: step:452/1000 train_loss:5.5149 train_time:109446ms step_avg:247.61ms torch.cuda.memory_allocated()=369551872
14:38:21.891: step:453/1000 train_loss:5.3256 train_time:109700ms step_avg:247.63ms torch.cuda.memory_allocated()=369551872
14:38:22.145: step:454/1000 train_loss:5.5302 train_time:109954ms step_avg:247.65ms torch.cuda.memory_allocated()=369551872
14:38:22.404: step:455/1000 train_loss:5.5345 train_time:110214ms step_avg:247.67ms torch.cuda.memory_allocated()=369551872
14:38:22.665: step:456/1000 train_loss:5.6663 train_time:110475ms step_avg:247.70ms torch.cuda.memory_allocated()=369551872
14:38:22.941: step:457/1000 train_loss:5.5052 train_time:110751ms step_avg:247.76ms torch.cuda.memory_allocated()=369551872
14:38:23.222: step:458/1000 train_loss:5.4703 train_time:111032ms step_avg:247.84ms torch.cuda.memory_allocated()=369551872
14:38:23.500: step:459/1000 train_loss:5.4231 train_time:111310ms step_avg:247.91ms torch.cuda.memory_allocated()=369551872
14:38:23.785: step:460/1000 train_loss:5.4545 train_time:111594ms step_avg:247.99ms torch.cuda.memory_allocated()=369551872
14:38:24.058: step:461/1000 train_loss:5.3191 train_time:111867ms step_avg:248.04ms torch.cuda.memory_allocated()=369551872
14:38:24.318: step:462/1000 train_loss:5.4103 train_time:112127ms step_avg:248.07ms torch.cuda.memory_allocated()=369551872
14:38:24.580: step:463/1000 train_loss:5.7454 train_time:112390ms step_avg:248.10ms torch.cuda.memory_allocated()=369551872
14:38:24.835: step:464/1000 train_loss:5.6608 train_time:112645ms step_avg:248.12ms torch.cuda.memory_allocated()=369551872
14:38:25.090: step:465/1000 train_loss:5.5168 train_time:112899ms step_avg:248.13ms torch.cuda.memory_allocated()=369551872
14:38:25.347: step:466/1000 train_loss:5.6808 train_time:113156ms step_avg:248.15ms torch.cuda.memory_allocated()=369551872
14:38:25.601: step:467/1000 train_loss:5.4501 train_time:113411ms step_avg:248.16ms torch.cuda.memory_allocated()=369551872
14:38:25.855: step:468/1000 train_loss:5.3266 train_time:113665ms step_avg:248.18ms torch.cuda.memory_allocated()=369551872
14:38:26.110: step:469/1000 train_loss:5.5438 train_time:113919ms step_avg:248.19ms torch.cuda.memory_allocated()=369551872
14:38:26.365: step:470/1000 train_loss:5.2688 train_time:114174ms step_avg:248.21ms torch.cuda.memory_allocated()=369551872
14:38:26.618: step:471/1000 train_loss:5.4507 train_time:114427ms step_avg:248.22ms torch.cuda.memory_allocated()=369551872
14:38:26.872: step:472/1000 train_loss:5.5723 train_time:114682ms step_avg:248.23ms torch.cuda.memory_allocated()=369551872
14:38:27.126: step:473/1000 train_loss:5.3855 train_time:114936ms step_avg:248.24ms torch.cuda.memory_allocated()=369551872
14:38:27.383: step:474/1000 train_loss:5.6821 train_time:115192ms step_avg:248.26ms torch.cuda.memory_allocated()=369551872
14:38:27.642: step:475/1000 train_loss:5.7086 train_time:115452ms step_avg:248.28ms torch.cuda.memory_allocated()=369551872
14:38:27.896: step:476/1000 train_loss:5.6656 train_time:115706ms step_avg:248.30ms torch.cuda.memory_allocated()=369551872
14:38:28.150: step:477/1000 train_loss:5.2592 train_time:115959ms step_avg:248.31ms torch.cuda.memory_allocated()=369551872
14:38:28.407: step:478/1000 train_loss:5.4443 train_time:116217ms step_avg:248.33ms torch.cuda.memory_allocated()=369551872
14:38:28.660: step:479/1000 train_loss:5.8781 train_time:116470ms step_avg:248.34ms torch.cuda.memory_allocated()=369551872
14:38:28.914: step:480/1000 train_loss:5.3125 train_time:116723ms step_avg:248.35ms torch.cuda.memory_allocated()=369551872
14:38:29.168: step:481/1000 train_loss:5.7093 train_time:116977ms step_avg:248.36ms torch.cuda.memory_allocated()=369551872
14:38:29.426: step:482/1000 train_loss:5.4498 train_time:117236ms step_avg:248.38ms torch.cuda.memory_allocated()=369551872
14:38:29.679: step:483/1000 train_loss:5.3109 train_time:117488ms step_avg:248.39ms torch.cuda.memory_allocated()=369551872
14:38:29.935: step:484/1000 train_loss:5.4890 train_time:117744ms step_avg:248.41ms torch.cuda.memory_allocated()=369551872
14:38:30.188: step:485/1000 train_loss:5.5524 train_time:117997ms step_avg:248.42ms torch.cuda.memory_allocated()=369551872
14:38:30.445: step:486/1000 train_loss:5.5051 train_time:118254ms step_avg:248.43ms torch.cuda.memory_allocated()=369551872
14:38:30.701: step:487/1000 train_loss:5.5355 train_time:118511ms step_avg:248.45ms torch.cuda.memory_allocated()=369551872
14:38:30.958: step:488/1000 train_loss:5.3773 train_time:118767ms step_avg:248.47ms torch.cuda.memory_allocated()=369551872
14:38:31.211: step:489/1000 train_loss:5.3321 train_time:119020ms step_avg:248.48ms torch.cuda.memory_allocated()=369551872
14:38:31.469: step:490/1000 train_loss:5.5109 train_time:119278ms step_avg:248.50ms torch.cuda.memory_allocated()=369551872
14:38:31.723: step:491/1000 train_loss:5.2398 train_time:119532ms step_avg:248.51ms torch.cuda.memory_allocated()=369551872
14:38:31.981: step:492/1000 train_loss:5.2785 train_time:119791ms step_avg:248.53ms torch.cuda.memory_allocated()=369551872
14:38:32.236: step:493/1000 train_loss:5.2065 train_time:120046ms step_avg:248.54ms torch.cuda.memory_allocated()=369551872
14:38:32.491: step:494/1000 train_loss:5.8601 train_time:120300ms step_avg:248.55ms torch.cuda.memory_allocated()=369551872
14:38:32.752: step:495/1000 train_loss:5.0273 train_time:120562ms step_avg:248.58ms torch.cuda.memory_allocated()=369551872
14:38:33.006: step:496/1000 train_loss:5.2431 train_time:120816ms step_avg:248.59ms torch.cuda.memory_allocated()=369551872
14:38:33.261: step:497/1000 train_loss:5.2376 train_time:121070ms step_avg:248.60ms torch.cuda.memory_allocated()=369551872
14:38:33.513: step:498/1000 train_loss:5.3084 train_time:121323ms step_avg:248.61ms torch.cuda.memory_allocated()=369551872
14:38:33.773: step:499/1000 train_loss:5.4776 train_time:121583ms step_avg:248.64ms torch.cuda.memory_allocated()=369551872
14:38:34.027: step:500/1000 train_loss:5.1786 train_time:121837ms step_avg:248.65ms torch.cuda.memory_allocated()=369551872
14:38:36.746: step:500/1000 val_loss:5.5726 train_time:121837ms step_avg:248.65ms
14:38:36.002: step:501/1000 train_loss:4.9801 train_time:122093ms step_avg:248.66ms torch.cuda.memory_allocated()=369551872
14:38:37.257: step:502/1000 train_loss:5.4422 train_time:122348ms step_avg:248.67ms torch.cuda.memory_allocated()=369551872
14:38:37.514: step:503/1000 train_loss:5.3911 train_time:122604ms step_avg:248.69ms torch.cuda.memory_allocated()=369551872
14:38:37.769: step:504/1000 train_loss:5.4281 train_time:122859ms step_avg:248.70ms torch.cuda.memory_allocated()=369551872
14:38:38.025: step:505/1000 train_loss:5.6141 train_time:123116ms step_avg:248.72ms torch.cuda.memory_allocated()=369551872
14:38:38.283: step:506/1000 train_loss:5.5315 train_time:123374ms step_avg:248.74ms torch.cuda.memory_allocated()=369551872
14:38:38.544: step:507/1000 train_loss:5.6119 train_time:123634ms step_avg:248.76ms torch.cuda.memory_allocated()=369551872
14:38:38.799: step:508/1000 train_loss:5.4163 train_time:123890ms step_avg:248.77ms torch.cuda.memory_allocated()=369551872
14:38:39.054: step:509/1000 train_loss:5.3385 train_time:124145ms step_avg:248.79ms torch.cuda.memory_allocated()=369551872
14:38:39.314: step:510/1000 train_loss:5.1282 train_time:124405ms step_avg:248.81ms torch.cuda.memory_allocated()=369551872
14:38:39.571: step:511/1000 train_loss:5.6976 train_time:124662ms step_avg:248.83ms torch.cuda.memory_allocated()=369551872
14:38:39.828: step:512/1000 train_loss:5.4482 train_time:124919ms step_avg:248.84ms torch.cuda.memory_allocated()=369551872
14:38:40.088: step:513/1000 train_loss:5.4115 train_time:125179ms step_avg:248.86ms torch.cuda.memory_allocated()=369551872
14:38:40.342: step:514/1000 train_loss:5.7654 train_time:125433ms step_avg:248.87ms torch.cuda.memory_allocated()=369551872
14:38:40.598: step:515/1000 train_loss:5.4657 train_time:125689ms step_avg:248.89ms torch.cuda.memory_allocated()=369551872
14:38:40.855: step:516/1000 train_loss:5.5586 train_time:125946ms step_avg:248.91ms torch.cuda.memory_allocated()=369551872
14:38:41.110: step:517/1000 train_loss:5.3161 train_time:126201ms step_avg:248.92ms torch.cuda.memory_allocated()=369551872
14:38:41.364: step:518/1000 train_loss:5.2696 train_time:126455ms step_avg:248.93ms torch.cuda.memory_allocated()=369551872
14:38:41.618: step:519/1000 train_loss:5.5604 train_time:126709ms step_avg:248.94ms torch.cuda.memory_allocated()=369551872
14:38:41.880: step:520/1000 train_loss:5.3860 train_time:126971ms step_avg:248.96ms torch.cuda.memory_allocated()=369551872
14:38:42.146: step:521/1000 train_loss:4.8103 train_time:127236ms step_avg:248.99ms torch.cuda.memory_allocated()=369551872
14:38:42.407: step:522/1000 train_loss:5.2799 train_time:127497ms step_avg:249.02ms torch.cuda.memory_allocated()=369551872
14:38:42.670: step:523/1000 train_loss:5.3506 train_time:127761ms step_avg:249.05ms torch.cuda.memory_allocated()=369551872
14:38:42.933: step:524/1000 train_loss:5.7758 train_time:128024ms step_avg:249.07ms torch.cuda.memory_allocated()=369551872
14:38:43.189: step:525/1000 train_loss:5.1940 train_time:128280ms step_avg:249.09ms torch.cuda.memory_allocated()=369551872
14:38:43.442: step:526/1000 train_loss:5.3127 train_time:128533ms step_avg:249.09ms torch.cuda.memory_allocated()=369551872
14:38:43.700: step:527/1000 train_loss:5.6215 train_time:128791ms step_avg:249.11ms torch.cuda.memory_allocated()=369551872
14:38:43.965: step:528/1000 train_loss:5.4934 train_time:129056ms step_avg:249.14ms torch.cuda.memory_allocated()=369551872
14:38:44.222: step:529/1000 train_loss:5.1665 train_time:129313ms step_avg:249.16ms torch.cuda.memory_allocated()=369551872
14:38:44.477: step:530/1000 train_loss:5.3596 train_time:129567ms step_avg:249.17ms torch.cuda.memory_allocated()=369551872
14:38:44.736: step:531/1000 train_loss:5.2091 train_time:129826ms step_avg:249.19ms torch.cuda.memory_allocated()=369551872
14:38:44.995: step:532/1000 train_loss:5.4583 train_time:130086ms step_avg:249.21ms torch.cuda.memory_allocated()=369551872
14:38:45.254: step:533/1000 train_loss:5.7148 train_time:130344ms step_avg:249.22ms torch.cuda.memory_allocated()=369551872
14:38:45.506: step:534/1000 train_loss:5.4868 train_time:130597ms step_avg:249.23ms torch.cuda.memory_allocated()=369551872
14:38:45.765: step:535/1000 train_loss:5.8453 train_time:130856ms step_avg:249.25ms torch.cuda.memory_allocated()=369551872
14:38:46.021: step:536/1000 train_loss:5.6491 train_time:131112ms step_avg:249.26ms torch.cuda.memory_allocated()=369551872
14:38:46.284: step:537/1000 train_loss:5.2245 train_time:131375ms step_avg:249.29ms torch.cuda.memory_allocated()=369551872
14:38:46.543: step:538/1000 train_loss:5.1939 train_time:131634ms step_avg:249.31ms torch.cuda.memory_allocated()=369551872
14:38:46.803: step:539/1000 train_loss:5.9860 train_time:131893ms step_avg:249.33ms torch.cuda.memory_allocated()=369551872
14:38:47.080: step:540/1000 train_loss:5.4552 train_time:132170ms step_avg:249.38ms torch.cuda.memory_allocated()=369551872
14:38:47.336: step:541/1000 train_loss:5.2768 train_time:132427ms step_avg:249.39ms torch.cuda.memory_allocated()=369551872
14:38:47.606: step:542/1000 train_loss:5.3988 train_time:132696ms step_avg:249.43ms torch.cuda.memory_allocated()=369551872
14:38:47.909: step:543/1000 train_loss:5.9963 train_time:132999ms step_avg:249.53ms torch.cuda.memory_allocated()=369551872
14:38:48.169: step:544/1000 train_loss:5.2128 train_time:133259ms step_avg:249.55ms torch.cuda.memory_allocated()=369551872
14:38:48.443: step:545/1000 train_loss:5.4455 train_time:133533ms step_avg:249.60ms torch.cuda.memory_allocated()=369551872
14:38:48.700: step:546/1000 train_loss:5.5337 train_time:133791ms step_avg:249.61ms torch.cuda.memory_allocated()=369551872
14:38:48.963: step:547/1000 train_loss:5.3639 train_time:134053ms step_avg:249.63ms torch.cuda.memory_allocated()=369551872
14:38:49.221: step:548/1000 train_loss:5.1524 train_time:134311ms step_avg:249.65ms torch.cuda.memory_allocated()=369551872
14:38:49.483: step:549/1000 train_loss:5.4797 train_time:134574ms step_avg:249.67ms torch.cuda.memory_allocated()=369551872
14:38:49.740: step:550/1000 train_loss:5.1107 train_time:134831ms step_avg:249.69ms torch.cuda.memory_allocated()=369551872
14:38:49.999: step:551/1000 train_loss:5.6903 train_time:135089ms step_avg:249.70ms torch.cuda.memory_allocated()=369551872
14:38:50.258: step:552/1000 train_loss:5.3850 train_time:135349ms step_avg:249.72ms torch.cuda.memory_allocated()=369551872
14:38:50.527: step:553/1000 train_loss:5.6175 train_time:135617ms step_avg:249.76ms torch.cuda.memory_allocated()=369551872
14:38:50.795: step:554/1000 train_loss:5.1373 train_time:135886ms step_avg:249.79ms torch.cuda.memory_allocated()=369551872
14:38:51.054: step:555/1000 train_loss:5.5936 train_time:136144ms step_avg:249.81ms torch.cuda.memory_allocated()=369551872
14:38:51.312: step:556/1000 train_loss:5.4369 train_time:136402ms step_avg:249.82ms torch.cuda.memory_allocated()=369551872
14:38:51.569: step:557/1000 train_loss:5.4960 train_time:136659ms step_avg:249.83ms torch.cuda.memory_allocated()=369551872
14:38:51.831: step:558/1000 train_loss:5.6503 train_time:136922ms step_avg:249.86ms torch.cuda.memory_allocated()=369551872
14:38:52.094: step:559/1000 train_loss:5.9117 train_time:137184ms step_avg:249.88ms torch.cuda.memory_allocated()=369551872
14:38:52.358: step:560/1000 train_loss:5.3966 train_time:137448ms step_avg:249.91ms torch.cuda.memory_allocated()=369551872
14:38:52.620: step:561/1000 train_loss:5.5616 train_time:137711ms step_avg:249.93ms torch.cuda.memory_allocated()=369551872
14:38:52.880: step:562/1000 train_loss:5.6652 train_time:137970ms step_avg:249.95ms torch.cuda.memory_allocated()=369551872
14:38:53.148: step:563/1000 train_loss:6.0759 train_time:138239ms step_avg:249.98ms torch.cuda.memory_allocated()=369551872
14:38:53.415: step:564/1000 train_loss:6.0321 train_time:138506ms step_avg:250.01ms torch.cuda.memory_allocated()=369551872
14:38:53.673: step:565/1000 train_loss:5.3731 train_time:138763ms step_avg:250.02ms torch.cuda.memory_allocated()=369551872
14:38:53.940: step:566/1000 train_loss:5.6222 train_time:139031ms step_avg:250.06ms torch.cuda.memory_allocated()=369551872
14:38:54.198: step:567/1000 train_loss:5.2626 train_time:139289ms step_avg:250.07ms torch.cuda.memory_allocated()=369551872
14:38:54.458: step:568/1000 train_loss:5.1869 train_time:139549ms step_avg:250.09ms torch.cuda.memory_allocated()=369551872
14:38:54.719: step:569/1000 train_loss:5.8174 train_time:139809ms step_avg:250.11ms torch.cuda.memory_allocated()=369551872
14:38:54.975: step:570/1000 train_loss:5.2701 train_time:140066ms step_avg:250.12ms torch.cuda.memory_allocated()=369551872
14:38:55.233: step:571/1000 train_loss:5.3845 train_time:140324ms step_avg:250.13ms torch.cuda.memory_allocated()=369551872
14:38:55.495: step:572/1000 train_loss:5.4237 train_time:140585ms step_avg:250.15ms torch.cuda.memory_allocated()=369551872
14:38:55.753: step:573/1000 train_loss:5.4046 train_time:140844ms step_avg:250.17ms torch.cuda.memory_allocated()=369551872
14:38:56.009: step:574/1000 train_loss:5.3741 train_time:141099ms step_avg:250.18ms torch.cuda.memory_allocated()=369551872
14:38:56.266: step:575/1000 train_loss:5.4941 train_time:141356ms step_avg:250.19ms torch.cuda.memory_allocated()=369551872
14:38:56.523: step:576/1000 train_loss:5.5782 train_time:141614ms step_avg:250.20ms torch.cuda.memory_allocated()=369551872
14:38:56.781: step:577/1000 train_loss:5.3558 train_time:141872ms step_avg:250.22ms torch.cuda.memory_allocated()=369551872
14:38:57.040: step:578/1000 train_loss:5.7520 train_time:142130ms step_avg:250.23ms torch.cuda.memory_allocated()=369551872
14:38:57.292: step:579/1000 train_loss:5.4447 train_time:142382ms step_avg:250.23ms torch.cuda.memory_allocated()=369551872
14:38:57.553: step:580/1000 train_loss:5.1537 train_time:142643ms step_avg:250.25ms torch.cuda.memory_allocated()=369551872
14:38:57.816: step:581/1000 train_loss:5.4659 train_time:142906ms step_avg:250.27ms torch.cuda.memory_allocated()=369551872
14:38:58.075: step:582/1000 train_loss:5.6984 train_time:143165ms step_avg:250.29ms torch.cuda.memory_allocated()=369551872
14:38:58.339: step:583/1000 train_loss:5.2835 train_time:143429ms step_avg:250.31ms torch.cuda.memory_allocated()=369551872
14:38:58.595: step:584/1000 train_loss:5.4983 train_time:143686ms step_avg:250.32ms torch.cuda.memory_allocated()=369551872
14:38:58.851: step:585/1000 train_loss:5.5146 train_time:143941ms step_avg:250.33ms torch.cuda.memory_allocated()=369551872
14:38:59.118: step:586/1000 train_loss:5.7323 train_time:144208ms step_avg:250.36ms torch.cuda.memory_allocated()=369551872
14:38:59.376: step:587/1000 train_loss:5.5143 train_time:144466ms step_avg:250.38ms torch.cuda.memory_allocated()=369551872
14:38:59.633: step:588/1000 train_loss:5.4150 train_time:144724ms step_avg:250.39ms torch.cuda.memory_allocated()=369551872
14:38:59.891: step:589/1000 train_loss:5.7966 train_time:144981ms step_avg:250.40ms torch.cuda.memory_allocated()=369551872
14:39:00.159: step:590/1000 train_loss:6.6610 train_time:145249ms step_avg:250.43ms torch.cuda.memory_allocated()=369551872
14:39:00.414: step:591/1000 train_loss:5.6271 train_time:145505ms step_avg:250.44ms torch.cuda.memory_allocated()=369551872
14:39:00.664: step:592/1000 train_loss:5.4158 train_time:145755ms step_avg:250.44ms torch.cuda.memory_allocated()=369551872
14:39:00.922: step:593/1000 train_loss:5.0450 train_time:146013ms step_avg:250.45ms torch.cuda.memory_allocated()=369551872
14:39:01.183: step:594/1000 train_loss:5.4282 train_time:146273ms step_avg:250.47ms torch.cuda.memory_allocated()=369551872
14:39:01.442: step:595/1000 train_loss:5.6452 train_time:146533ms step_avg:250.48ms torch.cuda.memory_allocated()=369551872
14:39:01.707: step:596/1000 train_loss:5.2027 train_time:146797ms step_avg:250.51ms torch.cuda.memory_allocated()=369551872
14:39:01.974: step:597/1000 train_loss:5.5739 train_time:147065ms step_avg:250.54ms torch.cuda.memory_allocated()=369551872
14:39:02.242: step:598/1000 train_loss:5.3868 train_time:147332ms step_avg:250.56ms torch.cuda.memory_allocated()=369551872
14:39:02.498: step:599/1000 train_loss:5.3176 train_time:147588ms step_avg:250.57ms torch.cuda.memory_allocated()=369551872
14:39:02.766: step:600/1000 train_loss:6.2313 train_time:147857ms step_avg:250.61ms torch.cuda.memory_allocated()=369551872
14:39:03.022: step:601/1000 train_loss:4.9880 train_time:148113ms step_avg:250.61ms torch.cuda.memory_allocated()=369551872
14:39:03.283: step:602/1000 train_loss:5.2725 train_time:148373ms step_avg:250.63ms torch.cuda.memory_allocated()=369551872
14:39:03.539: step:603/1000 train_loss:5.4440 train_time:148629ms step_avg:250.64ms torch.cuda.memory_allocated()=369551872
14:39:03.796: step:604/1000 train_loss:5.5353 train_time:148887ms step_avg:250.65ms torch.cuda.memory_allocated()=369551872
14:39:04.055: step:605/1000 train_loss:5.3680 train_time:149145ms step_avg:250.66ms torch.cuda.memory_allocated()=369551872
14:39:04.316: step:606/1000 train_loss:5.5290 train_time:149407ms step_avg:250.68ms torch.cuda.memory_allocated()=369551872
14:39:04.574: step:607/1000 train_loss:5.5204 train_time:149664ms step_avg:250.69ms torch.cuda.memory_allocated()=369551872
14:39:04.842: step:608/1000 train_loss:5.6515 train_time:149932ms step_avg:250.72ms torch.cuda.memory_allocated()=369551872
14:39:05.100: step:609/1000 train_loss:5.2897 train_time:150190ms step_avg:250.73ms torch.cuda.memory_allocated()=369551872
14:39:05.361: step:610/1000 train_loss:5.2471 train_time:150452ms step_avg:250.75ms torch.cuda.memory_allocated()=369551872
14:39:05.629: step:611/1000 train_loss:4.8447 train_time:150719ms step_avg:250.78ms torch.cuda.memory_allocated()=369551872
14:39:05.894: step:612/1000 train_loss:5.0735 train_time:150984ms step_avg:250.80ms torch.cuda.memory_allocated()=369551872
14:39:06.156: step:613/1000 train_loss:5.1885 train_time:151246ms step_avg:250.82ms torch.cuda.memory_allocated()=369551872
14:39:06.420: step:614/1000 train_loss:5.2176 train_time:151511ms step_avg:250.85ms torch.cuda.memory_allocated()=369551872
14:39:06.688: step:615/1000 train_loss:5.1440 train_time:151778ms step_avg:250.87ms torch.cuda.memory_allocated()=369551872
14:39:06.942: step:616/1000 train_loss:5.2929 train_time:152033ms step_avg:250.88ms torch.cuda.memory_allocated()=369551872
14:39:07.203: step:617/1000 train_loss:5.2605 train_time:152294ms step_avg:250.90ms torch.cuda.memory_allocated()=369551872
14:39:07.458: step:618/1000 train_loss:5.5049 train_time:152548ms step_avg:250.90ms torch.cuda.memory_allocated()=369551872
14:39:07.720: step:619/1000 train_loss:5.4183 train_time:152810ms step_avg:250.92ms torch.cuda.memory_allocated()=369551872
14:39:07.978: step:620/1000 train_loss:5.2637 train_time:153068ms step_avg:250.93ms torch.cuda.memory_allocated()=369551872
14:39:08.237: step:621/1000 train_loss:5.3669 train_time:153327ms step_avg:250.94ms torch.cuda.memory_allocated()=369551872
14:39:08.498: step:622/1000 train_loss:5.2777 train_time:153588ms step_avg:250.96ms torch.cuda.memory_allocated()=369551872
14:39:08.758: step:623/1000 train_loss:5.3027 train_time:153848ms step_avg:250.98ms torch.cuda.memory_allocated()=369551872
14:39:09.013: step:624/1000 train_loss:5.4463 train_time:154103ms step_avg:250.98ms torch.cuda.memory_allocated()=369551872
14:39:09.267: step:625/1000 train_loss:5.4260 train_time:154358ms step_avg:250.99ms torch.cuda.memory_allocated()=369551872
14:39:12.011: step:625/1000 val_loss:5.4728 train_time:154358ms step_avg:250.99ms
14:39:12.267: step:626/1000 train_loss:5.5179 train_time:154614ms step_avg:251.00ms torch.cuda.memory_allocated()=369551872
14:39:12.524: step:627/1000 train_loss:5.4161 train_time:154871ms step_avg:251.01ms torch.cuda.memory_allocated()=369551872
14:39:12.786: step:628/1000 train_loss:5.7085 train_time:155133ms step_avg:251.02ms torch.cuda.memory_allocated()=369551872
14:39:13.050: step:629/1000 train_loss:5.2022 train_time:155397ms step_avg:251.04ms torch.cuda.memory_allocated()=369551872
14:39:13.311: step:630/1000 train_loss:5.4294 train_time:155657ms step_avg:251.06ms torch.cuda.memory_allocated()=369551872
14:39:13.572: step:631/1000 train_loss:5.4570 train_time:155919ms step_avg:251.08ms torch.cuda.memory_allocated()=369551872
14:39:13.838: step:632/1000 train_loss:5.1939 train_time:156185ms step_avg:251.10ms torch.cuda.memory_allocated()=369551872
14:39:14.103: step:633/1000 train_loss:5.3699 train_time:156450ms step_avg:251.12ms torch.cuda.memory_allocated()=369551872
14:39:14.370: step:634/1000 train_loss:5.1264 train_time:156716ms step_avg:251.15ms torch.cuda.memory_allocated()=369551872
14:39:14.627: step:635/1000 train_loss:5.3188 train_time:156973ms step_avg:251.16ms torch.cuda.memory_allocated()=369551872
14:39:14.891: step:636/1000 train_loss:5.4576 train_time:157238ms step_avg:251.18ms torch.cuda.memory_allocated()=369551872
14:39:15.146: step:637/1000 train_loss:5.5066 train_time:157492ms step_avg:251.18ms torch.cuda.memory_allocated()=369551872
14:39:15.402: step:638/1000 train_loss:5.4629 train_time:157749ms step_avg:251.19ms torch.cuda.memory_allocated()=369551872
14:39:15.655: step:639/1000 train_loss:5.3529 train_time:158002ms step_avg:251.20ms torch.cuda.memory_allocated()=369551872
14:39:15.907: step:640/1000 train_loss:5.2256 train_time:158254ms step_avg:251.20ms torch.cuda.memory_allocated()=369551872
14:39:16.167: step:641/1000 train_loss:5.1544 train_time:158513ms step_avg:251.21ms torch.cuda.memory_allocated()=369551872
14:39:16.426: step:642/1000 train_loss:5.4813 train_time:158773ms step_avg:251.22ms torch.cuda.memory_allocated()=369551872
14:39:16.689: step:643/1000 train_loss:5.3571 train_time:159036ms step_avg:251.24ms torch.cuda.memory_allocated()=369551872
14:39:16.952: step:644/1000 train_loss:5.4650 train_time:159299ms step_avg:251.26ms torch.cuda.memory_allocated()=369551872
14:39:17.210: step:645/1000 train_loss:5.5059 train_time:159557ms step_avg:251.27ms torch.cuda.memory_allocated()=369551872
14:39:17.472: step:646/1000 train_loss:5.2997 train_time:159819ms step_avg:251.29ms torch.cuda.memory_allocated()=369551872
14:39:17.738: step:647/1000 train_loss:5.3315 train_time:160085ms step_avg:251.31ms torch.cuda.memory_allocated()=369551872
14:39:17.999: step:648/1000 train_loss:5.3103 train_time:160346ms step_avg:251.33ms torch.cuda.memory_allocated()=369551872
14:39:18.255: step:649/1000 train_loss:5.4412 train_time:160602ms step_avg:251.33ms torch.cuda.memory_allocated()=369551872
14:39:18.516: step:650/1000 train_loss:5.2523 train_time:160862ms step_avg:251.35ms torch.cuda.memory_allocated()=369551872
14:39:18.779: step:651/1000 train_loss:5.1746 train_time:161125ms step_avg:251.37ms torch.cuda.memory_allocated()=369551872
14:39:19.042: step:652/1000 train_loss:5.2660 train_time:161389ms step_avg:251.38ms torch.cuda.memory_allocated()=369551872
14:39:19.301: step:653/1000 train_loss:5.0283 train_time:161648ms step_avg:251.40ms torch.cuda.memory_allocated()=369551872
14:39:19.561: step:654/1000 train_loss:5.1778 train_time:161908ms step_avg:251.41ms torch.cuda.memory_allocated()=369551872
14:39:19.821: step:655/1000 train_loss:5.1451 train_time:162168ms step_avg:251.42ms torch.cuda.memory_allocated()=369551872
14:39:20.078: step:656/1000 train_loss:5.3872 train_time:162425ms step_avg:251.43ms torch.cuda.memory_allocated()=369551872
14:39:20.332: step:657/1000 train_loss:5.3001 train_time:162679ms step_avg:251.44ms torch.cuda.memory_allocated()=369551872
14:39:20.592: step:658/1000 train_loss:5.2840 train_time:162939ms step_avg:251.45ms torch.cuda.memory_allocated()=369551872
14:39:20.854: step:659/1000 train_loss:5.6257 train_time:163201ms step_avg:251.46ms torch.cuda.memory_allocated()=369551872
14:39:21.115: step:660/1000 train_loss:5.5514 train_time:163462ms step_avg:251.48ms torch.cuda.memory_allocated()=369551872
14:39:21.369: step:661/1000 train_loss:5.2964 train_time:163716ms step_avg:251.48ms torch.cuda.memory_allocated()=369551872
14:39:21.628: step:662/1000 train_loss:5.1747 train_time:163975ms step_avg:251.50ms torch.cuda.memory_allocated()=369551872
14:39:21.886: step:663/1000 train_loss:5.2927 train_time:164233ms step_avg:251.51ms torch.cuda.memory_allocated()=369551872
14:39:22.144: step:664/1000 train_loss:5.2615 train_time:164491ms step_avg:251.51ms torch.cuda.memory_allocated()=369551872
14:39:22.399: step:665/1000 train_loss:5.4862 train_time:164745ms step_avg:251.52ms torch.cuda.memory_allocated()=369551872
14:39:22.660: step:666/1000 train_loss:5.7957 train_time:165007ms step_avg:251.53ms torch.cuda.memory_allocated()=369551872
14:39:22.918: step:667/1000 train_loss:5.1745 train_time:165265ms step_avg:251.54ms torch.cuda.memory_allocated()=369551872
14:39:23.176: step:668/1000 train_loss:5.4429 train_time:165523ms step_avg:251.55ms torch.cuda.memory_allocated()=369551872
14:39:23.441: step:669/1000 train_loss:5.4181 train_time:165788ms step_avg:251.57ms torch.cuda.memory_allocated()=369551872
14:39:23.698: step:670/1000 train_loss:5.2914 train_time:166044ms step_avg:251.58ms torch.cuda.memory_allocated()=369551872
14:39:23.958: step:671/1000 train_loss:5.4850 train_time:166304ms step_avg:251.60ms torch.cuda.memory_allocated()=369551872
14:39:24.212: step:672/1000 train_loss:5.2480 train_time:166559ms step_avg:251.60ms torch.cuda.memory_allocated()=369551872
14:39:24.473: step:673/1000 train_loss:5.6726 train_time:166820ms step_avg:251.61ms torch.cuda.memory_allocated()=369551872
14:39:24.749: step:674/1000 train_loss:6.0510 train_time:167096ms step_avg:251.65ms torch.cuda.memory_allocated()=369551872
14:39:25.008: step:675/1000 train_loss:5.3190 train_time:167355ms step_avg:251.66ms torch.cuda.memory_allocated()=369551872
14:39:25.273: step:676/1000 train_loss:5.3128 train_time:167620ms step_avg:251.68ms torch.cuda.memory_allocated()=369551872
14:39:25.543: step:677/1000 train_loss:5.4439 train_time:167890ms step_avg:251.71ms torch.cuda.memory_allocated()=369551872
14:39:25.822: step:678/1000 train_loss:5.5870 train_time:168169ms step_avg:251.75ms torch.cuda.memory_allocated()=369551872
14:39:26.078: step:679/1000 train_loss:5.4514 train_time:168425ms step_avg:251.76ms torch.cuda.memory_allocated()=369551872
14:39:26.355: step:680/1000 train_loss:5.3992 train_time:168702ms step_avg:251.79ms torch.cuda.memory_allocated()=369551872
14:39:26.634: step:681/1000 train_loss:5.2207 train_time:168981ms step_avg:251.83ms torch.cuda.memory_allocated()=369551872
14:39:26.907: step:682/1000 train_loss:5.2236 train_time:169254ms step_avg:251.87ms torch.cuda.memory_allocated()=369551872
14:39:27.168: step:683/1000 train_loss:5.3704 train_time:169514ms step_avg:251.88ms torch.cuda.memory_allocated()=369551872
14:39:27.432: step:684/1000 train_loss:5.4263 train_time:169779ms step_avg:251.90ms torch.cuda.memory_allocated()=369551872
14:39:27.697: step:685/1000 train_loss:5.8012 train_time:170044ms step_avg:251.92ms torch.cuda.memory_allocated()=369551872
14:39:27.970: step:686/1000 train_loss:5.3016 train_time:170316ms step_avg:251.95ms torch.cuda.memory_allocated()=369551872
14:39:28.229: step:687/1000 train_loss:5.2390 train_time:170576ms step_avg:251.96ms torch.cuda.memory_allocated()=369551872
14:39:28.497: step:688/1000 train_loss:6.0062 train_time:170844ms step_avg:251.98ms torch.cuda.memory_allocated()=369551872
14:39:28.754: step:689/1000 train_loss:5.1221 train_time:171101ms step_avg:251.99ms torch.cuda.memory_allocated()=369551872
14:39:29.014: step:690/1000 train_loss:5.1579 train_time:171361ms step_avg:252.00ms torch.cuda.memory_allocated()=369551872
14:39:29.278: step:691/1000 train_loss:5.4207 train_time:171625ms step_avg:252.02ms torch.cuda.memory_allocated()=369551872
14:39:29.541: step:692/1000 train_loss:5.0733 train_time:171888ms step_avg:252.03ms torch.cuda.memory_allocated()=369551872
14:39:29.808: step:693/1000 train_loss:5.0897 train_time:172155ms step_avg:252.06ms torch.cuda.memory_allocated()=369551872
14:39:30.082: step:694/1000 train_loss:5.5090 train_time:172429ms step_avg:252.09ms torch.cuda.memory_allocated()=369551872
14:39:30.336: step:695/1000 train_loss:5.2702 train_time:172683ms step_avg:252.09ms torch.cuda.memory_allocated()=369551872
14:39:30.600: step:696/1000 train_loss:5.4113 train_time:172947ms step_avg:252.11ms torch.cuda.memory_allocated()=369551872
14:39:30.861: step:697/1000 train_loss:5.5020 train_time:173208ms step_avg:252.12ms torch.cuda.memory_allocated()=369551872
14:39:31.119: step:698/1000 train_loss:5.1275 train_time:173466ms step_avg:252.13ms torch.cuda.memory_allocated()=369551872
14:39:31.378: step:699/1000 train_loss:5.4169 train_time:173725ms step_avg:252.14ms torch.cuda.memory_allocated()=369551872
14:39:31.651: step:700/1000 train_loss:6.2694 train_time:173998ms step_avg:252.17ms torch.cuda.memory_allocated()=369551872
14:39:31.916: step:701/1000 train_loss:5.1131 train_time:174263ms step_avg:252.19ms torch.cuda.memory_allocated()=369551872
14:39:32.182: step:702/1000 train_loss:5.0203 train_time:174529ms step_avg:252.21ms torch.cuda.memory_allocated()=369551872
14:39:32.440: step:703/1000 train_loss:5.3628 train_time:174787ms step_avg:252.22ms torch.cuda.memory_allocated()=369551872
14:39:32.700: step:704/1000 train_loss:5.3490 train_time:175047ms step_avg:252.23ms torch.cuda.memory_allocated()=369551872
14:39:32.960: step:705/1000 train_loss:5.4367 train_time:175306ms step_avg:252.24ms torch.cuda.memory_allocated()=369551872
14:39:33.213: step:706/1000 train_loss:5.1935 train_time:175559ms step_avg:252.24ms torch.cuda.memory_allocated()=369551872
14:39:33.470: step:707/1000 train_loss:5.5398 train_time:175817ms step_avg:252.25ms torch.cuda.memory_allocated()=369551872
14:39:33.732: step:708/1000 train_loss:5.2130 train_time:176079ms step_avg:252.26ms torch.cuda.memory_allocated()=369551872
14:39:33.995: step:709/1000 train_loss:5.2264 train_time:176341ms step_avg:252.28ms torch.cuda.memory_allocated()=369551872
14:39:34.255: step:710/1000 train_loss:5.3405 train_time:176602ms step_avg:252.29ms torch.cuda.memory_allocated()=369551872
14:39:34.529: step:711/1000 train_loss:5.3921 train_time:176876ms step_avg:252.32ms torch.cuda.memory_allocated()=369551872
14:39:34.788: step:712/1000 train_loss:5.3188 train_time:177135ms step_avg:252.33ms torch.cuda.memory_allocated()=369551872
14:39:35.053: step:713/1000 train_loss:5.6800 train_time:177399ms step_avg:252.35ms torch.cuda.memory_allocated()=369551872
14:39:35.315: step:714/1000 train_loss:5.2357 train_time:177662ms step_avg:252.36ms torch.cuda.memory_allocated()=369551872
14:39:35.586: step:715/1000 train_loss:5.0056 train_time:177933ms step_avg:252.39ms torch.cuda.memory_allocated()=369551872
14:39:35.846: step:716/1000 train_loss:5.2366 train_time:178193ms step_avg:252.40ms torch.cuda.memory_allocated()=369551872
14:39:36.109: step:717/1000 train_loss:5.0145 train_time:178455ms step_avg:252.41ms torch.cuda.memory_allocated()=369551872
14:39:36.370: step:718/1000 train_loss:5.1937 train_time:178717ms step_avg:252.42ms torch.cuda.memory_allocated()=369551872
14:39:36.636: step:719/1000 train_loss:5.2581 train_time:178983ms step_avg:252.44ms torch.cuda.memory_allocated()=369551872
14:39:36.898: step:720/1000 train_loss:5.1347 train_time:179245ms step_avg:252.46ms torch.cuda.memory_allocated()=369551872
14:39:37.156: step:721/1000 train_loss:5.3031 train_time:179502ms step_avg:252.46ms torch.cuda.memory_allocated()=369551872
14:39:37.414: step:722/1000 train_loss:5.4249 train_time:179761ms step_avg:252.47ms torch.cuda.memory_allocated()=369551872
14:39:37.680: step:723/1000 train_loss:4.9557 train_time:180027ms step_avg:252.49ms torch.cuda.memory_allocated()=369551872
14:39:37.940: step:724/1000 train_loss:5.0934 train_time:180287ms step_avg:252.50ms torch.cuda.memory_allocated()=369551872
14:39:38.211: step:725/1000 train_loss:5.4046 train_time:180558ms step_avg:252.53ms torch.cuda.memory_allocated()=369551872
14:39:38.470: step:726/1000 train_loss:5.2989 train_time:180817ms step_avg:252.54ms torch.cuda.memory_allocated()=369551872
14:39:38.729: step:727/1000 train_loss:5.2405 train_time:181076ms step_avg:252.55ms torch.cuda.memory_allocated()=369551872
14:39:38.988: step:728/1000 train_loss:5.2993 train_time:181335ms step_avg:252.56ms torch.cuda.memory_allocated()=369551872
14:39:39.246: step:729/1000 train_loss:4.9779 train_time:181593ms step_avg:252.56ms torch.cuda.memory_allocated()=369551872
14:39:39.503: step:730/1000 train_loss:5.2035 train_time:181850ms step_avg:252.57ms torch.cuda.memory_allocated()=369551872
14:39:39.764: step:731/1000 train_loss:5.0285 train_time:182111ms step_avg:252.58ms torch.cuda.memory_allocated()=369551872
14:39:40.022: step:732/1000 train_loss:5.0338 train_time:182369ms step_avg:252.59ms torch.cuda.memory_allocated()=369551872
14:39:40.290: step:733/1000 train_loss:4.6585 train_time:182636ms step_avg:252.61ms torch.cuda.memory_allocated()=369551872
14:39:40.550: step:734/1000 train_loss:4.9021 train_time:182897ms step_avg:252.62ms torch.cuda.memory_allocated()=369551872
14:39:40.808: step:735/1000 train_loss:5.2222 train_time:183155ms step_avg:252.63ms torch.cuda.memory_allocated()=369551872
14:39:41.084: step:736/1000 train_loss:5.1805 train_time:183431ms step_avg:252.66ms torch.cuda.memory_allocated()=369551872
14:39:41.352: step:737/1000 train_loss:6.1269 train_time:183699ms step_avg:252.68ms torch.cuda.memory_allocated()=369551872
14:39:41.615: step:738/1000 train_loss:5.4625 train_time:183962ms step_avg:252.69ms torch.cuda.memory_allocated()=369551872
14:39:41.874: step:739/1000 train_loss:5.1825 train_time:184221ms step_avg:252.70ms torch.cuda.memory_allocated()=369551872
14:39:42.135: step:740/1000 train_loss:5.1322 train_time:184482ms step_avg:252.72ms torch.cuda.memory_allocated()=369551872
14:39:42.396: step:741/1000 train_loss:5.1404 train_time:184743ms step_avg:252.73ms torch.cuda.memory_allocated()=369551872
14:39:42.659: step:742/1000 train_loss:5.2623 train_time:185006ms step_avg:252.74ms torch.cuda.memory_allocated()=369551872
14:39:42.924: step:743/1000 train_loss:5.2165 train_time:185271ms step_avg:252.76ms torch.cuda.memory_allocated()=369551872
14:39:43.186: step:744/1000 train_loss:5.0769 train_time:185533ms step_avg:252.77ms torch.cuda.memory_allocated()=369551872
14:39:43.451: step:745/1000 train_loss:5.1183 train_time:185798ms step_avg:252.79ms torch.cuda.memory_allocated()=369551872
14:39:43.711: step:746/1000 train_loss:5.2539 train_time:186058ms step_avg:252.80ms torch.cuda.memory_allocated()=369551872
14:39:43.974: step:747/1000 train_loss:5.2117 train_time:186321ms step_avg:252.81ms torch.cuda.memory_allocated()=369551872
14:39:44.247: step:748/1000 train_loss:5.0774 train_time:186593ms step_avg:252.84ms torch.cuda.memory_allocated()=369551872
14:39:44.514: step:749/1000 train_loss:5.2171 train_time:186861ms step_avg:252.86ms torch.cuda.memory_allocated()=369551872
14:39:44.784: step:750/1000 train_loss:5.1586 train_time:187131ms step_avg:252.88ms torch.cuda.memory_allocated()=369551872
14:39:47.549: step:750/1000 val_loss:5.3080 train_time:187131ms step_avg:252.88ms
14:39:47.808: step:751/1000 train_loss:5.1503 train_time:187390ms step_avg:252.89ms torch.cuda.memory_allocated()=369551872
14:39:48.080: step:752/1000 train_loss:5.2148 train_time:187661ms step_avg:252.91ms torch.cuda.memory_allocated()=369551872
14:39:48.350: step:753/1000 train_loss:5.0166 train_time:187931ms step_avg:252.94ms torch.cuda.memory_allocated()=369551872
14:39:48.609: step:754/1000 train_loss:5.4685 train_time:188190ms step_avg:252.94ms torch.cuda.memory_allocated()=369551872
14:39:48.867: step:755/1000 train_loss:5.1640 train_time:188448ms step_avg:252.95ms torch.cuda.memory_allocated()=369551872
14:39:49.127: step:756/1000 train_loss:5.2948 train_time:188709ms step_avg:252.96ms torch.cuda.memory_allocated()=369551872
14:39:49.388: step:757/1000 train_loss:5.3205 train_time:188970ms step_avg:252.97ms torch.cuda.memory_allocated()=369551872
14:39:49.647: step:758/1000 train_loss:5.4400 train_time:189228ms step_avg:252.98ms torch.cuda.memory_allocated()=369551872
14:39:49.910: step:759/1000 train_loss:4.8559 train_time:189492ms step_avg:252.99ms torch.cuda.memory_allocated()=369551872
14:39:50.172: step:760/1000 train_loss:5.2023 train_time:189753ms step_avg:253.00ms torch.cuda.memory_allocated()=369551872
14:39:50.432: step:761/1000 train_loss:5.0281 train_time:190014ms step_avg:253.01ms torch.cuda.memory_allocated()=369551872
14:39:50.692: step:762/1000 train_loss:4.9855 train_time:190273ms step_avg:253.02ms torch.cuda.memory_allocated()=369551872
14:39:50.958: step:763/1000 train_loss:5.0119 train_time:190539ms step_avg:253.04ms torch.cuda.memory_allocated()=369551872
14:39:51.219: step:764/1000 train_loss:5.0858 train_time:190801ms step_avg:253.05ms torch.cuda.memory_allocated()=369551872
14:39:51.482: step:765/1000 train_loss:5.0439 train_time:191064ms step_avg:253.06ms torch.cuda.memory_allocated()=369551872
14:39:51.739: step:766/1000 train_loss:5.3553 train_time:191320ms step_avg:253.07ms torch.cuda.memory_allocated()=369551872
14:39:51.002: step:767/1000 train_loss:5.1698 train_time:191584ms step_avg:253.08ms torch.cuda.memory_allocated()=369551872
14:39:52.268: step:768/1000 train_loss:5.4638 train_time:191849ms step_avg:253.10ms torch.cuda.memory_allocated()=369551872
14:39:52.527: step:769/1000 train_loss:4.9684 train_time:192108ms step_avg:253.11ms torch.cuda.memory_allocated()=369551872
14:39:52.784: step:770/1000 train_loss:5.2602 train_time:192366ms step_avg:253.11ms torch.cuda.memory_allocated()=369551872
14:39:53.045: step:771/1000 train_loss:5.1124 train_time:192627ms step_avg:253.12ms torch.cuda.memory_allocated()=369551872
14:39:53.318: step:772/1000 train_loss:5.0733 train_time:192899ms step_avg:253.15ms torch.cuda.memory_allocated()=369551872
14:39:53.571: step:773/1000 train_loss:5.2911 train_time:193153ms step_avg:253.15ms torch.cuda.memory_allocated()=369551872
14:39:53.840: step:774/1000 train_loss:5.1992 train_time:193422ms step_avg:253.17ms torch.cuda.memory_allocated()=369551872
14:39:54.100: step:775/1000 train_loss:5.0477 train_time:193681ms step_avg:253.18ms torch.cuda.memory_allocated()=369551872
14:39:54.360: step:776/1000 train_loss:5.0970 train_time:193941ms step_avg:253.19ms torch.cuda.memory_allocated()=369551872
14:39:54.619: step:777/1000 train_loss:4.9881 train_time:194200ms step_avg:253.19ms torch.cuda.memory_allocated()=369551872
14:39:54.878: step:778/1000 train_loss:5.0511 train_time:194460ms step_avg:253.20ms torch.cuda.memory_allocated()=369551872
14:39:55.155: step:779/1000 train_loss:4.5835 train_time:194736ms step_avg:253.23ms torch.cuda.memory_allocated()=369551872
14:39:55.418: step:780/1000 train_loss:5.0904 train_time:194999ms step_avg:253.25ms torch.cuda.memory_allocated()=369551872
14:39:55.672: step:781/1000 train_loss:5.1551 train_time:195254ms step_avg:253.25ms torch.cuda.memory_allocated()=369551872
14:39:55.933: step:782/1000 train_loss:5.0087 train_time:195514ms step_avg:253.26ms torch.cuda.memory_allocated()=369551872
14:39:56.200: step:783/1000 train_loss:5.4577 train_time:195781ms step_avg:253.27ms torch.cuda.memory_allocated()=369551872
14:39:56.469: step:784/1000 train_loss:5.0546 train_time:196051ms step_avg:253.30ms torch.cuda.memory_allocated()=369551872
14:39:56.742: step:785/1000 train_loss:5.1830 train_time:196323ms step_avg:253.32ms torch.cuda.memory_allocated()=369551872
14:39:56.002: step:786/1000 train_loss:4.9875 train_time:196584ms step_avg:253.33ms torch.cuda.memory_allocated()=369551872
14:39:57.277: step:787/1000 train_loss:4.8267 train_time:196859ms step_avg:253.36ms torch.cuda.memory_allocated()=369551872
14:39:57.545: step:788/1000 train_loss:5.0524 train_time:197127ms step_avg:253.38ms torch.cuda.memory_allocated()=369551872
14:39:57.807: step:789/1000 train_loss:4.9786 train_time:197389ms step_avg:253.39ms torch.cuda.memory_allocated()=369551872
14:39:58.072: step:790/1000 train_loss:4.8611 train_time:197653ms step_avg:253.40ms torch.cuda.memory_allocated()=369551872
14:39:58.333: step:791/1000 train_loss:5.3093 train_time:197915ms step_avg:253.41ms torch.cuda.memory_allocated()=369551872
14:39:58.590: step:792/1000 train_loss:5.0430 train_time:198171ms step_avg:253.42ms torch.cuda.memory_allocated()=369551872
14:39:58.853: step:793/1000 train_loss:5.1143 train_time:198435ms step_avg:253.43ms torch.cuda.memory_allocated()=369551872
14:39:59.113: step:794/1000 train_loss:5.1310 train_time:198695ms step_avg:253.44ms torch.cuda.memory_allocated()=369551872
14:39:59.372: step:795/1000 train_loss:5.1900 train_time:198954ms step_avg:253.44ms torch.cuda.memory_allocated()=369551872
14:39:59.635: step:796/1000 train_loss:5.0566 train_time:199217ms step_avg:253.46ms torch.cuda.memory_allocated()=369551872
14:39:59.903: step:797/1000 train_loss:5.5362 train_time:199485ms step_avg:253.47ms torch.cuda.memory_allocated()=369551872
14:40:00.165: step:798/1000 train_loss:5.0444 train_time:199746ms step_avg:253.48ms torch.cuda.memory_allocated()=369551872
14:40:00.426: step:799/1000 train_loss:5.2685 train_time:200008ms step_avg:253.50ms torch.cuda.memory_allocated()=369551872
14:40:00.685: step:800/1000 train_loss:5.3072 train_time:200267ms step_avg:253.50ms torch.cuda.memory_allocated()=369551872
14:40:00.944: step:801/1000 train_loss:5.3096 train_time:200526ms step_avg:253.51ms torch.cuda.memory_allocated()=369551872
14:40:01.213: step:802/1000 train_loss:4.7015 train_time:200795ms step_avg:253.53ms torch.cuda.memory_allocated()=369551872
14:40:01.479: step:803/1000 train_loss:4.9006 train_time:201061ms step_avg:253.54ms torch.cuda.memory_allocated()=369551872
14:40:01.762: step:804/1000 train_loss:5.2045 train_time:201344ms step_avg:253.58ms torch.cuda.memory_allocated()=369551872
14:40:02.038: step:805/1000 train_loss:5.6727 train_time:201620ms step_avg:253.61ms torch.cuda.memory_allocated()=369551872
14:40:02.302: step:806/1000 train_loss:5.2990 train_time:201883ms step_avg:253.62ms torch.cuda.memory_allocated()=369551872
14:40:02.564: step:807/1000 train_loss:5.3199 train_time:202145ms step_avg:253.63ms torch.cuda.memory_allocated()=369551872
14:40:02.828: step:808/1000 train_loss:5.0640 train_time:202410ms step_avg:253.65ms torch.cuda.memory_allocated()=369551872
14:40:03.086: step:809/1000 train_loss:4.8831 train_time:202668ms step_avg:253.65ms torch.cuda.memory_allocated()=369551872
14:40:03.356: step:810/1000 train_loss:4.7088 train_time:202937ms step_avg:253.67ms torch.cuda.memory_allocated()=369551872
14:40:03.612: step:811/1000 train_loss:5.2906 train_time:203194ms step_avg:253.68ms torch.cuda.memory_allocated()=369551872
14:40:03.877: step:812/1000 train_loss:5.2616 train_time:203458ms step_avg:253.69ms torch.cuda.memory_allocated()=369551872
14:40:04.134: step:813/1000 train_loss:5.0530 train_time:203715ms step_avg:253.69ms torch.cuda.memory_allocated()=369551872
14:40:04.387: step:814/1000 train_loss:5.2788 train_time:203968ms step_avg:253.69ms torch.cuda.memory_allocated()=369551872
14:40:04.647: step:815/1000 train_loss:5.2028 train_time:204229ms step_avg:253.70ms torch.cuda.memory_allocated()=369551872
14:40:04.906: step:816/1000 train_loss:4.9484 train_time:204488ms step_avg:253.71ms torch.cuda.memory_allocated()=369551872
14:40:05.175: step:817/1000 train_loss:5.0160 train_time:204757ms step_avg:253.73ms torch.cuda.memory_allocated()=369551872
14:40:05.438: step:818/1000 train_loss:4.9390 train_time:205020ms step_avg:253.74ms torch.cuda.memory_allocated()=369551872
14:40:05.693: step:819/1000 train_loss:5.1889 train_time:205275ms step_avg:253.74ms torch.cuda.memory_allocated()=369551872
14:40:05.963: step:820/1000 train_loss:5.2571 train_time:205545ms step_avg:253.76ms torch.cuda.memory_allocated()=369551872
14:40:06.219: step:821/1000 train_loss:4.9870 train_time:205801ms step_avg:253.76ms torch.cuda.memory_allocated()=369551872
14:40:06.482: step:822/1000 train_loss:4.9981 train_time:206064ms step_avg:253.77ms torch.cuda.memory_allocated()=369551872
14:40:06.749: step:823/1000 train_loss:4.9213 train_time:206330ms step_avg:253.79ms torch.cuda.memory_allocated()=369551872
14:40:07.020: step:824/1000 train_loss:4.9091 train_time:206602ms step_avg:253.81ms torch.cuda.memory_allocated()=369551872
14:40:07.287: step:825/1000 train_loss:5.1482 train_time:206869ms step_avg:253.83ms torch.cuda.memory_allocated()=369551872
14:40:07.547: step:826/1000 train_loss:5.0505 train_time:207129ms step_avg:253.83ms torch.cuda.memory_allocated()=369551872
14:40:07.810: step:827/1000 train_loss:4.9360 train_time:207392ms step_avg:253.85ms torch.cuda.memory_allocated()=369551872
14:40:08.082: step:828/1000 train_loss:4.9153 train_time:207663ms step_avg:253.87ms torch.cuda.memory_allocated()=369551872
14:40:08.361: step:829/1000 train_loss:4.8523 train_time:207943ms step_avg:253.90ms torch.cuda.memory_allocated()=369551872
14:40:08.627: step:830/1000 train_loss:5.1296 train_time:208209ms step_avg:253.91ms torch.cuda.memory_allocated()=369551872
14:40:08.895: step:831/1000 train_loss:5.3230 train_time:208476ms step_avg:253.93ms torch.cuda.memory_allocated()=369551872
14:40:09.176: step:832/1000 train_loss:5.9167 train_time:208758ms step_avg:253.96ms torch.cuda.memory_allocated()=369551872
14:40:09.458: step:833/1000 train_loss:5.5471 train_time:209039ms step_avg:254.00ms torch.cuda.memory_allocated()=369551872
14:40:09.721: step:834/1000 train_loss:4.9756 train_time:209303ms step_avg:254.01ms torch.cuda.memory_allocated()=369551872
14:40:09.987: step:835/1000 train_loss:4.9847 train_time:209568ms step_avg:254.02ms torch.cuda.memory_allocated()=369551872
14:40:10.261: step:836/1000 train_loss:5.1221 train_time:209842ms step_avg:254.05ms torch.cuda.memory_allocated()=369551872
14:40:10.528: step:837/1000 train_loss:5.0752 train_time:210109ms step_avg:254.06ms torch.cuda.memory_allocated()=369551872
14:40:10.791: step:838/1000 train_loss:4.9020 train_time:210373ms step_avg:254.07ms torch.cuda.memory_allocated()=369551872
14:40:11.051: step:839/1000 train_loss:5.0130 train_time:210633ms step_avg:254.08ms torch.cuda.memory_allocated()=369551872
14:40:11.315: step:840/1000 train_loss:4.8550 train_time:210896ms step_avg:254.09ms torch.cuda.memory_allocated()=369551872
14:40:11.577: step:841/1000 train_loss:5.0679 train_time:211158ms step_avg:254.10ms torch.cuda.memory_allocated()=369551872
14:40:11.844: step:842/1000 train_loss:5.1717 train_time:211425ms step_avg:254.12ms torch.cuda.memory_allocated()=369551872
14:40:12.112: step:843/1000 train_loss:4.8691 train_time:211694ms step_avg:254.13ms torch.cuda.memory_allocated()=369551872
14:40:12.373: step:844/1000 train_loss:5.0833 train_time:211954ms step_avg:254.14ms torch.cuda.memory_allocated()=369551872
14:40:12.636: step:845/1000 train_loss:5.2105 train_time:212217ms step_avg:254.15ms torch.cuda.memory_allocated()=369551872
14:40:12.894: step:846/1000 train_loss:5.2322 train_time:212475ms step_avg:254.16ms torch.cuda.memory_allocated()=369551872
14:40:13.156: step:847/1000 train_loss:5.1946 train_time:212738ms step_avg:254.17ms torch.cuda.memory_allocated()=369551872
14:40:13.413: step:848/1000 train_loss:5.2173 train_time:212994ms step_avg:254.17ms torch.cuda.memory_allocated()=369551872
14:40:13.668: step:849/1000 train_loss:5.0539 train_time:213249ms step_avg:254.17ms torch.cuda.memory_allocated()=369551872
14:40:13.929: step:850/1000 train_loss:5.2038 train_time:213511ms step_avg:254.18ms torch.cuda.memory_allocated()=369551872
14:40:14.201: step:851/1000 train_loss:4.9939 train_time:213782ms step_avg:254.20ms torch.cuda.memory_allocated()=369551872
14:40:14.463: step:852/1000 train_loss:5.0138 train_time:214044ms step_avg:254.21ms torch.cuda.memory_allocated()=369551872
14:40:14.725: step:853/1000 train_loss:4.7490 train_time:214307ms step_avg:254.22ms torch.cuda.memory_allocated()=369551872
14:40:14.985: step:854/1000 train_loss:5.3347 train_time:214567ms step_avg:254.23ms torch.cuda.memory_allocated()=369551872
14:40:15.239: step:855/1000 train_loss:4.9989 train_time:214821ms step_avg:254.23ms torch.cuda.memory_allocated()=369551872
14:40:15.498: step:856/1000 train_loss:5.0335 train_time:215079ms step_avg:254.23ms torch.cuda.memory_allocated()=369551872
14:40:15.755: step:857/1000 train_loss:5.2083 train_time:215337ms step_avg:254.23ms torch.cuda.memory_allocated()=369551872
14:40:16.010: step:858/1000 train_loss:4.9774 train_time:215591ms step_avg:254.24ms torch.cuda.memory_allocated()=369551872
14:40:16.269: step:859/1000 train_loss:4.9798 train_time:215851ms step_avg:254.24ms torch.cuda.memory_allocated()=369551872
14:40:16.529: step:860/1000 train_loss:5.4374 train_time:216110ms step_avg:254.25ms torch.cuda.memory_allocated()=369551872
14:40:16.785: step:861/1000 train_loss:5.1942 train_time:216366ms step_avg:254.25ms torch.cuda.memory_allocated()=369551872
14:40:17.045: step:862/1000 train_loss:4.8049 train_time:216627ms step_avg:254.26ms torch.cuda.memory_allocated()=369551872
14:40:17.311: step:863/1000 train_loss:4.9716 train_time:216892ms step_avg:254.27ms torch.cuda.memory_allocated()=369551872
14:40:17.570: step:864/1000 train_loss:5.0092 train_time:217152ms step_avg:254.28ms torch.cuda.memory_allocated()=369551872
14:40:17.837: step:865/1000 train_loss:4.8772 train_time:217418ms step_avg:254.29ms torch.cuda.memory_allocated()=369551872
14:40:18.110: step:866/1000 train_loss:4.5503 train_time:217692ms step_avg:254.31ms torch.cuda.memory_allocated()=369551872
14:40:18.368: step:867/1000 train_loss:5.0534 train_time:217949ms step_avg:254.32ms torch.cuda.memory_allocated()=369551872
14:40:18.626: step:868/1000 train_loss:4.7834 train_time:218207ms step_avg:254.32ms torch.cuda.memory_allocated()=369551872
14:40:18.892: step:869/1000 train_loss:4.9902 train_time:218473ms step_avg:254.33ms torch.cuda.memory_allocated()=369551872
14:40:19.148: step:870/1000 train_loss:5.0402 train_time:218730ms step_avg:254.34ms torch.cuda.memory_allocated()=369551872
14:40:19.409: step:871/1000 train_loss:5.0258 train_time:218991ms step_avg:254.34ms torch.cuda.memory_allocated()=369551872
14:40:19.679: step:872/1000 train_loss:4.8968 train_time:219261ms step_avg:254.36ms torch.cuda.memory_allocated()=369551872
14:40:19.941: step:873/1000 train_loss:4.8738 train_time:219523ms step_avg:254.37ms torch.cuda.memory_allocated()=369551872
14:40:20.209: step:874/1000 train_loss:4.8970 train_time:219791ms step_avg:254.39ms torch.cuda.memory_allocated()=369551872
14:40:20.477: step:875/1000 train_loss:4.7317 train_time:220059ms step_avg:254.40ms torch.cuda.memory_allocated()=369551872
14:40:23.260: step:875/1000 val_loss:5.1288 train_time:220059ms step_avg:254.40ms
14:40:23.517: step:876/1000 train_loss:4.9827 train_time:220316ms step_avg:254.41ms torch.cuda.memory_allocated()=369551872
14:40:23.777: step:877/1000 train_loss:4.9678 train_time:220576ms step_avg:254.41ms torch.cuda.memory_allocated()=369551872
14:40:24.060: step:878/1000 train_loss:4.5522 train_time:220859ms step_avg:254.45ms torch.cuda.memory_allocated()=369551872
14:40:24.324: step:879/1000 train_loss:4.8750 train_time:221122ms step_avg:254.46ms torch.cuda.memory_allocated()=369551872
14:40:24.589: step:880/1000 train_loss:4.8617 train_time:221388ms step_avg:254.47ms torch.cuda.memory_allocated()=369551872
14:40:24.848: step:881/1000 train_loss:4.9632 train_time:221647ms step_avg:254.47ms torch.cuda.memory_allocated()=369551872
14:40:25.117: step:882/1000 train_loss:5.2114 train_time:221915ms step_avg:254.49ms torch.cuda.memory_allocated()=369551872
14:40:25.373: step:883/1000 train_loss:4.9399 train_time:222172ms step_avg:254.49ms torch.cuda.memory_allocated()=369551872
14:40:25.633: step:884/1000 train_loss:4.9020 train_time:222432ms step_avg:254.50ms torch.cuda.memory_allocated()=369551872
14:40:25.899: step:885/1000 train_loss:4.9139 train_time:222697ms step_avg:254.51ms torch.cuda.memory_allocated()=369551872
14:40:26.157: step:886/1000 train_loss:4.9014 train_time:222955ms step_avg:254.52ms torch.cuda.memory_allocated()=369551872
14:40:26.419: step:887/1000 train_loss:5.2109 train_time:223217ms step_avg:254.52ms torch.cuda.memory_allocated()=369551872
14:40:26.679: step:888/1000 train_loss:5.1398 train_time:223478ms step_avg:254.53ms torch.cuda.memory_allocated()=369551872
14:40:26.944: step:889/1000 train_loss:5.0476 train_time:223743ms step_avg:254.54ms torch.cuda.memory_allocated()=369551872
14:40:27.211: step:890/1000 train_loss:4.9918 train_time:224010ms step_avg:254.56ms torch.cuda.memory_allocated()=369551872
14:40:27.478: step:891/1000 train_loss:4.8924 train_time:224277ms step_avg:254.57ms torch.cuda.memory_allocated()=369551872
14:40:27.738: step:892/1000 train_loss:4.9591 train_time:224537ms step_avg:254.58ms torch.cuda.memory_allocated()=369551872
14:40:28.022: step:893/1000 train_loss:5.0512 train_time:224820ms step_avg:254.61ms torch.cuda.memory_allocated()=369551872
14:40:28.278: step:894/1000 train_loss:5.0456 train_time:225076ms step_avg:254.61ms torch.cuda.memory_allocated()=369551872
14:40:28.537: step:895/1000 train_loss:4.9898 train_time:225336ms step_avg:254.62ms torch.cuda.memory_allocated()=369551872
14:40:28.805: step:896/1000 train_loss:4.7288 train_time:225603ms step_avg:254.63ms torch.cuda.memory_allocated()=369551872
14:40:29.072: step:897/1000 train_loss:5.0545 train_time:225870ms step_avg:254.65ms torch.cuda.memory_allocated()=369551872
14:40:29.328: step:898/1000 train_loss:5.0542 train_time:226127ms step_avg:254.65ms torch.cuda.memory_allocated()=369551872
14:40:29.582: step:899/1000 train_loss:5.0564 train_time:226380ms step_avg:254.65ms torch.cuda.memory_allocated()=369551872
14:40:29.851: step:900/1000 train_loss:5.0611 train_time:226650ms step_avg:254.66ms torch.cuda.memory_allocated()=369551872
14:40:30.123: step:901/1000 train_loss:5.1057 train_time:226922ms step_avg:254.68ms torch.cuda.memory_allocated()=369551872
14:40:30.389: step:902/1000 train_loss:4.7078 train_time:227187ms step_avg:254.69ms torch.cuda.memory_allocated()=369551872
14:40:30.651: step:903/1000 train_loss:4.8499 train_time:227449ms step_avg:254.70ms torch.cuda.memory_allocated()=369551872
14:40:30.906: step:904/1000 train_loss:5.0571 train_time:227705ms step_avg:254.70ms torch.cuda.memory_allocated()=369551872
14:40:31.163: step:905/1000 train_loss:5.0254 train_time:227962ms step_avg:254.71ms torch.cuda.memory_allocated()=369551872
14:40:31.420: step:906/1000 train_loss:4.8664 train_time:228219ms step_avg:254.71ms torch.cuda.memory_allocated()=369551872
14:40:31.684: step:907/1000 train_loss:5.0371 train_time:228483ms step_avg:254.72ms torch.cuda.memory_allocated()=369551872
14:40:31.953: step:908/1000 train_loss:5.1154 train_time:228751ms step_avg:254.73ms torch.cuda.memory_allocated()=369551872
14:40:32.217: step:909/1000 train_loss:4.7317 train_time:229015ms step_avg:254.74ms torch.cuda.memory_allocated()=369551872
14:40:32.477: step:910/1000 train_loss:5.1352 train_time:229275ms step_avg:254.75ms torch.cuda.memory_allocated()=369551872
14:40:32.739: step:911/1000 train_loss:5.3544 train_time:229538ms step_avg:254.76ms torch.cuda.memory_allocated()=369551872
14:40:33.025: step:912/1000 train_loss:5.1442 train_time:229824ms step_avg:254.79ms torch.cuda.memory_allocated()=369551872
14:40:33.305: step:913/1000 train_loss:5.0319 train_time:230104ms step_avg:254.82ms torch.cuda.memory_allocated()=369551872
14:40:33.562: step:914/1000 train_loss:4.9644 train_time:230361ms step_avg:254.82ms torch.cuda.memory_allocated()=369551872
14:40:33.822: step:915/1000 train_loss:4.7392 train_time:230621ms step_avg:254.83ms torch.cuda.memory_allocated()=369551872
14:40:34.091: step:916/1000 train_loss:5.6093 train_time:230889ms step_avg:254.84ms torch.cuda.memory_allocated()=369551872
14:40:34.355: step:917/1000 train_loss:5.4087 train_time:231154ms step_avg:254.86ms torch.cuda.memory_allocated()=369551872
14:40:34.617: step:918/1000 train_loss:5.1508 train_time:231416ms step_avg:254.86ms torch.cuda.memory_allocated()=369551872
14:40:34.882: step:919/1000 train_loss:5.2837 train_time:231681ms step_avg:254.87ms torch.cuda.memory_allocated()=369551872
14:40:35.152: step:920/1000 train_loss:4.7947 train_time:231951ms step_avg:254.89ms torch.cuda.memory_allocated()=369551872
14:40:35.415: step:921/1000 train_loss:5.0055 train_time:232214ms step_avg:254.90ms torch.cuda.memory_allocated()=369551872
14:40:35.676: step:922/1000 train_loss:4.9023 train_time:232475ms step_avg:254.91ms torch.cuda.memory_allocated()=369551872
14:40:35.943: step:923/1000 train_loss:5.0429 train_time:232742ms step_avg:254.92ms torch.cuda.memory_allocated()=369551872
14:40:36.200: step:924/1000 train_loss:5.0225 train_time:232998ms step_avg:254.92ms torch.cuda.memory_allocated()=369551872
14:40:36.467: step:925/1000 train_loss:4.6807 train_time:233266ms step_avg:254.94ms torch.cuda.memory_allocated()=369551872
14:40:36.732: step:926/1000 train_loss:5.0845 train_time:233531ms step_avg:254.95ms torch.cuda.memory_allocated()=369551872
14:40:36.999: step:927/1000 train_loss:4.9888 train_time:233797ms step_avg:254.96ms torch.cuda.memory_allocated()=369551872
14:40:37.260: step:928/1000 train_loss:4.7643 train_time:234059ms step_avg:254.97ms torch.cuda.memory_allocated()=369551872
14:40:37.521: step:929/1000 train_loss:4.6736 train_time:234320ms step_avg:254.97ms torch.cuda.memory_allocated()=369551872
14:40:37.784: step:930/1000 train_loss:5.3043 train_time:234583ms step_avg:254.98ms torch.cuda.memory_allocated()=369551872
14:40:38.047: step:931/1000 train_loss:4.9461 train_time:234846ms step_avg:254.99ms torch.cuda.memory_allocated()=369551872
14:40:38.321: step:932/1000 train_loss:4.5685 train_time:235120ms step_avg:255.01ms torch.cuda.memory_allocated()=369551872
14:40:38.585: step:933/1000 train_loss:4.6826 train_time:235384ms step_avg:255.02ms torch.cuda.memory_allocated()=369551872
14:40:38.858: step:934/1000 train_loss:4.7572 train_time:235657ms step_avg:255.04ms torch.cuda.memory_allocated()=369551872
14:40:39.123: step:935/1000 train_loss:4.8740 train_time:235922ms step_avg:255.05ms torch.cuda.memory_allocated()=369551872
14:40:39.383: step:936/1000 train_loss:4.8686 train_time:236181ms step_avg:255.06ms torch.cuda.memory_allocated()=369551872
14:40:39.641: step:937/1000 train_loss:4.9124 train_time:236439ms step_avg:255.06ms torch.cuda.memory_allocated()=369551872
14:40:39.910: step:938/1000 train_loss:4.8032 train_time:236709ms step_avg:255.07ms torch.cuda.memory_allocated()=369551872
14:40:40.176: step:939/1000 train_loss:5.2322 train_time:236975ms step_avg:255.09ms torch.cuda.memory_allocated()=369551872
14:40:40.452: step:940/1000 train_loss:4.8907 train_time:237251ms step_avg:255.11ms torch.cuda.memory_allocated()=369551872
14:40:40.717: step:941/1000 train_loss:5.0948 train_time:237516ms step_avg:255.12ms torch.cuda.memory_allocated()=369551872
14:40:40.988: step:942/1000 train_loss:4.8951 train_time:237787ms step_avg:255.14ms torch.cuda.memory_allocated()=369551872
14:40:41.251: step:943/1000 train_loss:4.7137 train_time:238050ms step_avg:255.14ms torch.cuda.memory_allocated()=369551872
14:40:41.520: step:944/1000 train_loss:4.6266 train_time:238319ms step_avg:255.16ms torch.cuda.memory_allocated()=369551872
14:40:41.784: step:945/1000 train_loss:5.2648 train_time:238583ms step_avg:255.17ms torch.cuda.memory_allocated()=369551872
14:40:42.046: step:946/1000 train_loss:4.9713 train_time:238845ms step_avg:255.18ms torch.cuda.memory_allocated()=369551872
14:40:42.302: step:947/1000 train_loss:5.0903 train_time:239101ms step_avg:255.18ms torch.cuda.memory_allocated()=369551872
14:40:42.558: step:948/1000 train_loss:4.9816 train_time:239357ms step_avg:255.18ms torch.cuda.memory_allocated()=369551872
14:40:42.836: step:949/1000 train_loss:4.8936 train_time:239634ms step_avg:255.20ms torch.cuda.memory_allocated()=369551872
14:40:43.129: step:950/1000 train_loss:4.9096 train_time:239928ms step_avg:255.24ms torch.cuda.memory_allocated()=369551872
14:40:43.396: step:951/1000 train_loss:4.8402 train_time:240194ms step_avg:255.25ms torch.cuda.memory_allocated()=369551872
14:40:43.662: step:952/1000 train_loss:5.0343 train_time:240461ms step_avg:255.27ms torch.cuda.memory_allocated()=369551872
14:40:43.933: step:953/1000 train_loss:5.1565 train_time:240732ms step_avg:255.28ms torch.cuda.memory_allocated()=369551872
14:40:44.197: step:954/1000 train_loss:4.8797 train_time:240995ms step_avg:255.29ms torch.cuda.memory_allocated()=369551872
14:40:44.484: step:955/1000 train_loss:4.7293 train_time:241283ms step_avg:255.33ms torch.cuda.memory_allocated()=369551872
14:40:44.746: step:956/1000 train_loss:4.6826 train_time:241545ms step_avg:255.33ms torch.cuda.memory_allocated()=369551872
14:40:45.012: step:957/1000 train_loss:4.8599 train_time:241811ms step_avg:255.34ms torch.cuda.memory_allocated()=369551872
14:40:45.303: step:958/1000 train_loss:5.0826 train_time:242101ms step_avg:255.38ms torch.cuda.memory_allocated()=369551872
14:40:45.566: step:959/1000 train_loss:4.8964 train_time:242365ms step_avg:255.39ms torch.cuda.memory_allocated()=369551872
14:40:45.830: step:960/1000 train_loss:4.9904 train_time:242628ms step_avg:255.40ms torch.cuda.memory_allocated()=369551872
14:40:46.097: step:961/1000 train_loss:4.7313 train_time:242895ms step_avg:255.41ms torch.cuda.memory_allocated()=369551872
14:40:46.366: step:962/1000 train_loss:4.6590 train_time:243165ms step_avg:255.43ms torch.cuda.memory_allocated()=369551872
14:40:46.631: step:963/1000 train_loss:5.0121 train_time:243429ms step_avg:255.43ms torch.cuda.memory_allocated()=369551872
14:40:46.892: step:964/1000 train_loss:4.9541 train_time:243691ms step_avg:255.44ms torch.cuda.memory_allocated()=369551872
14:40:47.161: step:965/1000 train_loss:4.9374 train_time:243960ms step_avg:255.46ms torch.cuda.memory_allocated()=369551872
14:40:47.420: step:966/1000 train_loss:4.9716 train_time:244219ms step_avg:255.46ms torch.cuda.memory_allocated()=369551872
14:40:47.682: step:967/1000 train_loss:4.9612 train_time:244481ms step_avg:255.47ms torch.cuda.memory_allocated()=369551872
14:40:47.942: step:968/1000 train_loss:4.9286 train_time:244741ms step_avg:255.47ms torch.cuda.memory_allocated()=369551872
14:40:48.208: step:969/1000 train_loss:4.7833 train_time:245007ms step_avg:255.48ms torch.cuda.memory_allocated()=369551872
14:40:48.466: step:970/1000 train_loss:4.8069 train_time:245265ms step_avg:255.48ms torch.cuda.memory_allocated()=369551872
14:40:48.728: step:971/1000 train_loss:5.0108 train_time:245527ms step_avg:255.49ms torch.cuda.memory_allocated()=369551872
14:40:48.991: step:972/1000 train_loss:4.8556 train_time:245790ms step_avg:255.50ms torch.cuda.memory_allocated()=369551872
14:40:49.253: step:973/1000 train_loss:4.9632 train_time:246052ms step_avg:255.51ms torch.cuda.memory_allocated()=369551872
14:40:49.512: step:974/1000 train_loss:4.8061 train_time:246311ms step_avg:255.51ms torch.cuda.memory_allocated()=369551872
14:40:49.767: step:975/1000 train_loss:4.9578 train_time:246566ms step_avg:255.51ms torch.cuda.memory_allocated()=369551872
14:40:50.023: step:976/1000 train_loss:5.0003 train_time:246822ms step_avg:255.51ms torch.cuda.memory_allocated()=369551872
14:40:50.296: step:977/1000 train_loss:5.0326 train_time:247094ms step_avg:255.53ms torch.cuda.memory_allocated()=369551872
14:40:50.561: step:978/1000 train_loss:5.2738 train_time:247359ms step_avg:255.54ms torch.cuda.memory_allocated()=369551872
14:40:50.850: step:979/1000 train_loss:5.5188 train_time:247649ms step_avg:255.57ms torch.cuda.memory_allocated()=369551872
14:40:51.148: step:980/1000 train_loss:5.7175 train_time:247947ms step_avg:255.62ms torch.cuda.memory_allocated()=369551872
14:40:51.421: step:981/1000 train_loss:5.1661 train_time:248220ms step_avg:255.63ms torch.cuda.memory_allocated()=369551872
14:40:51.679: step:982/1000 train_loss:5.1666 train_time:248478ms step_avg:255.64ms torch.cuda.memory_allocated()=369551872
14:40:51.948: step:983/1000 train_loss:5.0541 train_time:248747ms step_avg:255.65ms torch.cuda.memory_allocated()=369551872
14:40:52.209: step:984/1000 train_loss:4.9421 train_time:249008ms step_avg:255.65ms torch.cuda.memory_allocated()=369551872
14:40:52.477: step:985/1000 train_loss:4.8470 train_time:249276ms step_avg:255.67ms torch.cuda.memory_allocated()=369551872
14:40:52.734: step:986/1000 train_loss:4.8221 train_time:249533ms step_avg:255.67ms torch.cuda.memory_allocated()=369551872
14:40:52.995: step:987/1000 train_loss:4.7067 train_time:249794ms step_avg:255.67ms torch.cuda.memory_allocated()=369551872
14:40:53.257: step:988/1000 train_loss:5.1184 train_time:250056ms step_avg:255.68ms torch.cuda.memory_allocated()=369551872
14:40:53.525: step:989/1000 train_loss:4.7532 train_time:250324ms step_avg:255.69ms torch.cuda.memory_allocated()=369551872
14:40:53.797: step:990/1000 train_loss:4.9509 train_time:250596ms step_avg:255.71ms torch.cuda.memory_allocated()=369551872
14:40:54.073: step:991/1000 train_loss:4.7123 train_time:250872ms step_avg:255.73ms torch.cuda.memory_allocated()=369551872
14:40:54.335: step:992/1000 train_loss:4.7987 train_time:251134ms step_avg:255.74ms torch.cuda.memory_allocated()=369551872
14:40:54.593: step:993/1000 train_loss:4.8819 train_time:251391ms step_avg:255.74ms torch.cuda.memory_allocated()=369551872
14:40:54.851: step:994/1000 train_loss:4.7824 train_time:251650ms step_avg:255.74ms torch.cuda.memory_allocated()=369551872
14:40:55.128: step:995/1000 train_loss:4.8507 train_time:251927ms step_avg:255.76ms torch.cuda.memory_allocated()=369551872
14:40:55.412: step:996/1000 train_loss:4.2661 train_time:252211ms step_avg:255.79ms torch.cuda.memory_allocated()=369551872
14:40:55.682: step:997/1000 train_loss:4.6008 train_time:252480ms step_avg:255.81ms torch.cuda.memory_allocated()=369551872
14:40:55.948: step:998/1000 train_loss:4.6398 train_time:252747ms step_avg:255.82ms torch.cuda.memory_allocated()=369551872
14:40:56.204: step:999/1000 train_loss:4.7905 train_time:253003ms step_avg:255.82ms torch.cuda.memory_allocated()=369551872
14:40:56.457: step:1000/1000 train_loss:4.7509 train_time:253256ms step_avg:255.81ms torch.cuda.memory_allocated()=369551872
14:40:59.254: step:1000/1000 val_loss:5.0154 train_time:253257ms step_avg:255.81ms
14:41:52.451: Renamed logs/9fb0d6af-ee28-42cc-8f7a-592990148ca7.txt -> logs/20250202_MoEUTNoLNBias.txt
14:41:52.452: peak memory allocated: 6688 MiB reserved: 10484 MiB
