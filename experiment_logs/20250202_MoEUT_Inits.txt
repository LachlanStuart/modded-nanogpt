16:26:01.225: from collections import defaultdict
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import atexit

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.profiler import profile, record_function, ProfilerActivity
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
# torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)
import triton
import triton.language as tl

try:
    from lovely_tensors import monkey_patch
    monkey_patch()
except ImportError:
    pass


# -----------------------------------------------------------------------------
#region  Custom operators : FP8 matmul by @YouJiacheng
@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        # x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        x_f8 = x.mul(x_s).to(torch.float8_e5m2)
        # w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e5m2)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    # return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)
    return x @ w.t(), x.to(torch.float8_e5m2), w.to(torch.float8_e5m2)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)
#endregion
# -----------------------------------------------------------------------------
#region Muon optimizer
@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()
#endregion
# -----------------------------------------------------------------------------
#region PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve
#endregion
# -----------------------------------------------------------------------------
#region The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

def create_block_masks(input_seq: Tensor, sliding_window_num_blocks: Tensor):
    BLOCK_SIZE = 128
    docs = (input_seq == 50256).cumsum(0)

    def document_causal(b, h, q_idx, kv_idx):
        causal_mask = q_idx >= kv_idx
        document_mask = docs[q_idx] == docs[kv_idx]
        return causal_mask & document_mask

    def dense_to_ordered(dense_mask: Tensor):
        num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
        indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
        return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

    # manual block mask creation by @YouJiacheng
    assert len(input_seq) % BLOCK_SIZE == 0
    NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
    block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
    any_causal_bm = block_idx[:, None] >= block_idx
    all_causal_bm = block_idx[:, None] > block_idx
    docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
    docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
    any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
    all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
    any_bm = any_causal_bm & any_document_bm
    all_bm = all_causal_bm & all_document_bm
    partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
    full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
    def build_bm(sw_num_blocks: Tensor) -> BlockMask:
        return BlockMask.from_kv_blocks(
            torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
            partial_kv_indices,
            torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
            full_kv_indices,
            BLOCK_SIZE=BLOCK_SIZE,
            mask_mod=document_causal,
        )
    # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
    return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        # self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977


    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i, block in enumerate(self.blocks[:self.num_encoder_layers]):
            x = block(x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i, block in enumerate(self.blocks[self.num_encoder_layers:]):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = block(x, ve_dec[i], x0, block_masks[i])

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

#endregion
# -----------------------------------------------------------------------------
#region MoEUT Triton kernels
# From https://github.com/RobertCsordas/moeut/blob/master/moeut/cvmm.py

@dataclass
class CVMMSel:
    raw_sel: torch.Tensor
    sel: torch.Tensor
    sel_index: torch.Tensor
    out_index: torch.Tensor | None = None
    reduction_weight: torch.Tensor | None = None

    def clone(self) -> 'CVMMSel':
        return CVMMSel(self.raw_sel, self.sel, self.sel_index, self.out_index, self.reduction_weight)


def cvmm_prepare_sel(sel: torch.Tensor, n_experts: int) -> CVMMSel:
    fsel = sel.flatten()
    ssel, sel_index = fsel.sort()
    return CVMMSel(sel, ssel.view_as(sel), sel_index, None)


def dtype_to_type_id(dtype: torch.dtype):
    if dtype == torch.float32:
        return 0
    elif dtype == torch.float16:
        return 1
    elif dtype == torch.bfloat16:
        return 2

    raise ValueError("Unknown dtype")


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
    ],
    key=['M', 'N', 'K', 'dtype_id', 'allow_tf32']
)
@triton.jit
def cvmm_kernel(
    # Pointers to matrices
    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,
    # Matrix dimensions
    M, N, K,
    # The stride variables represent how much to increase the ptr by when moving by 1
    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`
    # by to get the element one row down (A has M rows).
    stride_am, stride_ak,
    stride_bo, stride_bk, stride_bn,
    stride_cm, stride_cn,
    stride_index, stride_sel, stride_out_index,
    out_index_is_none: tl.constexpr,
    dtype_id: tl.constexpr, allow_tf32: tl.constexpr,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr
):
    """Kernel for computing the matmul C = A x B.
    A has shape (M, K), B has shape (K, N) and C has shape (M, N)
    """
    # -----------------------------------------------------------
    # Map program ids `pid` to the block of C it should compute.
    # This is done in a grouped ordering to promote L2 data reuse.
    # See above `L2 Cache Optimizations` section for details.
    pid = tl.program_id(axis=0)

    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_n = (pid % num_pid_in_group) // group_size_m

    pid_m = first_pid_m + (pid % group_size_m)

    sel_first = tl.load(sel_ptr + pid_m * BLOCK_SIZE_M * stride_sel)
    sel_last = tl.load(sel_ptr + (min((pid_m + 1) * BLOCK_SIZE_M, M) - 1) * stride_sel)
    sel_all = tl.load(sel_ptr + stride_sel * ((pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M))

    for matrix_id in range(sel_first, sel_last + 1):
        # ----------------------------------------------------------
        # Create pointers for the first blocks of A and B.
        # We will advance this pointer as we move in the K direction
        # and accumulate
        # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
        # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
        # See above `Pointer Arithmetics` section for details
        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N

        remap_offs_am = tl.load(index_ptr + stride_index * offs_am)

        # Create offset pointers
        offs_k = tl.arange(0, BLOCK_SIZE_K)
        a_ptrs = a_ptr + (remap_offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
        b_ptrs = b_ptr + matrix_id * stride_bo + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

        # -----------------------------------------------------------
        # Iterate to compute a block of the C matrix.
        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
        # of fp32 values for higher accuracy.
        # `accumulator` will be converted back to fp16 after the loop.
        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
            # Load the next block of A and B, generate a mask by checking the K dimension.
            # If it is out of bounds, set it to 0.
            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
            # We accumulate along the K dimension.

            # Triton was unhappy with passing dtypes as vars.
            if dtype_id == 1:
                a = a.to(tl.float16)
                b = b.to(tl.float16)
            elif dtype_id == 2:
                a = a.to(tl.bfloat16)
                b = b.to(tl.bfloat16)

            accumulator += tl.dot(a, b, allow_tf32=allow_tf32)

            # Advance the ptrs to the next K block.
            a_ptrs += BLOCK_SIZE_K * stride_ak
            b_ptrs += BLOCK_SIZE_K * stride_bk


        if dtype_id == 1:
            c = accumulator.to(tl.float16)
        elif dtype_id == 2:
            c = accumulator.to(tl.bfloat16)
        else:
            c = accumulator

        # -----------------------------------------------------------
        # Write back the block of the output matrix C with masks.
        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)

        if out_index_is_none:
            remap_offs_cm = remap_offs_am
        else:
            remap_offs_cm = tl.load(out_index_ptr + stride_out_index * offs_am)

        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
        c_ptrs = c_ptr + stride_cm * remap_offs_cm[:, None] + stride_cn * offs_cn[None, :]
        c_mask = ((offs_cm[:, None] < M) & (sel_all[:, None] == matrix_id)) & (offs_cn[None, :] < N)
        tl.store(c_ptrs, c, mask=c_mask)


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 128}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 4}, num_stages=4, num_warps=4),

        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        # triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 128}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),

        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 16}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 16}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
    ],
    key=['M', 'N', 'K', 'out_dtype_id', 'allow_tf32', 'dtype_id'], reset_to_zero = ['c_ptr']
)
@triton.jit
def cvmm_backward_kernel3(
    # Pointers to matrices
    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,
    # Matrix dimensions
    M, N, K,
    # The stride variables represent how much to increase the ptr by when moving by 1
    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`
    # by to get the element one row down (A has M rows).
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_co, stride_cm, stride_cn,
    stride_index, stride_sel, stride_out_index,
    out_index_is_none: tl.constexpr,
    out_dtype_id: tl.constexpr, allow_tf32: tl.constexpr, dtype_id: tl.constexpr,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr, K_BLOCKS: tl.constexpr
):
    """Kernel for computing the matmul C = A x B.
    A has shape (M, K), B has shape (K, N) and C has shape (M, N)
    """
    # -----------------------------------------------------------
    # Map program ids `pid` to the block of C it should compute.
    # This is done in a grouped ordering to promote L2 data reuse.
    # See above `L2 Cache Optimizations` section for details.
    pid = tl.program_id(axis=0)
    k_block_id = tl.program_id(axis=1)

    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # ----------------------------------------------------------
    # Create pointers for the first blocks of A and B.
    # We will advance this pointer as we move in the K direction
    # and accumulate
    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
    # See above `Pointer Arithmetics` section for details
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N

    # -----------------------------------------------------------
    # Iterate to compute a block of the C matrix.
    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
    # of fp32 values for higher accuracy.
    # `accumulator` will be converted back to fp16 after the loop.

    a_ptrs_this = a_ptr + offs_am[:, None] * stride_am
    b_ptrs_this = b_ptr + offs_bn[None, :] * stride_bn

    # Kactual = end_i - start_i
    # Nblocks = (Kactual + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K

    # WORK_PER_WORKER = (Nblocks + K_BLOCKS - 1) // K_BLOCKS
    # WORK_PER_WORKER = WORK_PER_WORKER if WORK_PER_WORKER > MIN_WORK_SIZE else MIN_WORK_SIZE


    # # Kloop_start = (Kactual + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K

    # first_block_k = k_block_id * WORK_PER_WORKER
    # last_block_k = min((k_block_id+1) * WORK_PER_WORKER, Nblocks)

    block_start_index = k_block_id * BLOCK_SIZE_K * K_BLOCKS
    block_end_index = min(block_start_index + BLOCK_SIZE_K * K_BLOCKS, K) - 1

    first_mat = tl.load(sel_ptr + stride_sel * block_start_index)
    last_mat = tl.load(sel_ptr + stride_sel * block_end_index)


    for matrix_index in range(first_mat, last_mat + 1):
        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

        start_i = block_start_index
        end_i = block_end_index + 1
        while start_i < end_i:
            middle = (start_i + end_i) // 2
            middle_matrix = tl.load(sel_ptr + middle * stride_sel)
            if middle_matrix < matrix_index:
                start_i = middle + 1
            else:
                end_i = middle


        # # Continue binary search: find the first matrix that is > matrix_index
        start_i2 = start_i
        end_i = block_end_index + 1
        while start_i2 < end_i:
            middle = (start_i2 + end_i) // 2
            middle_matrix = tl.load(sel_ptr + middle * stride_sel)
            if middle_matrix <= matrix_index:
                start_i2 = middle + 1
            else:
                end_i = middle

        end_i = start_i2

        count = end_i - start_i

        block_mem_indices_f_base = start_i  + tl.arange(0, BLOCK_SIZE_K)

        if count > 0:
            for k in range((count + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K):
                # block_mem_indices = (k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)) % K
                block_mem_indices_f = block_mem_indices_f_base + k * BLOCK_SIZE_K
                block_mem_indices = block_mem_indices_f % K
                a_index = tl.load(index_ptr + stride_index * block_mem_indices)
                if out_index_is_none:
                    b_index = a_index
                else:
                    b_index = tl.load(out_index_ptr + stride_out_index * block_mem_indices)
                sel_ok = block_mem_indices_f < end_i

                a_ptrs = a_ptrs_this + a_index[None, :] * stride_ak
                b_ptrs = b_ptrs_this + b_index[:, None] * stride_bk

                # Load the next block of A and B, generate a mask by checking the K dimension.
                # If it is out of bounds, set it to 0.
                a = tl.load(a_ptrs, mask=sel_ok[None, :], other=0.0)
                b = tl.load(b_ptrs, mask=sel_ok[:, None], other=0.0)

                if dtype_id == 1:
                    a = a.to(tl.float16)
                    b = b.to(tl.float16)
                elif dtype_id == 2:
                    a = a.to(tl.bfloat16)
                    b = b.to(tl.bfloat16)

                # We accumulate along the K dimension.
                accumulator += tl.dot(a, b, allow_tf32=allow_tf32)

            if out_dtype_id == 1:
                c = accumulator.to(tl.float16)
            elif out_dtype_id == 2:
                c = accumulator.to(tl.bfloat16)
            else:
                c = accumulator

            # -----------------------------------------------------------
            # Write back the block of the output matrix C with masks.
            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
            c_ptrs = c_ptr + stride_co * matrix_index + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
            # tl.store(c_ptrs, c, mask=c_mask)
            tl.atomic_add(c_ptrs, c, mask=c_mask)


torch.library.define("mylib::cvmm_triton", "(Tensor x, Tensor sel_index, Tensor sel, Tensor keys, ScalarType out_dtype, Tensor out_index) -> Tensor")
@torch.library.impl("mylib::cvmm_triton", "default")
def cvmm_triton(
    x: torch.Tensor,
    sel_index: torch.Tensor,
    sel: torch.Tensor,
    keys: torch.Tensor,
    out_dtype: torch.dtype,
    out_index: torch.Tensor
):
    x = x.flatten(end_dim=-2)
    assert x.shape[-1] == keys.shape[1]

    sel_shape = sel.shape
    sel = sel.flatten()

    M = sel.shape[0]
    O, K, N = keys.shape
    # Allocates output.
    out = torch.empty((M, N), device=x.device, dtype=out_dtype)
    # out = torch.zeros((M, N), device=x.device, dtype=out_dtype)
    # 1D launch kernel where each block gets its own program.

    # expected_m_per_matrix = int(math.ceil(M / O * 1.5))
    # expected_m_per_matrix = M

    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )

    out_index_is_none = False
    if out_index.numel() == 1 and out_index == -1:
        out_index_is_none = True

    cvmm_kernel[grid](
        x, keys, out, sel_index, sel, out_index,
        M, N, K,
        x.stride(0), x.stride(1),
        keys.stride(0), keys.stride(1), keys.stride(2),
        out.stride(0), out.stride(1),
        sel_index.stride(0), sel.stride(0), 0 if out_index_is_none else out_index.stride(0),
        out_index_is_none=out_index_is_none,
        dtype_id = dtype_to_type_id(out.dtype), allow_tf32=False, #torch.backends.cuda.matmul.allow_tf32
    )

    return out.view(*sel_shape, N)


@torch.library.register_fake("mylib::cvmm_triton", cvmm_triton)
def cvmm_triton_abstract(x, sel_idx, sel, keys, out_dtype, out_index):
    sel_shape = sel.shape
    sel = sel.flatten()
    M = sel.shape[0]
    O, K, N = keys.shape
    out = torch.empty((M, N), device=x.device, dtype=out_dtype)
    sel_shape = sel.shape
    return out.view(*sel_shape, N)


def cvmm_triton_backward(
    x: torch.Tensor,
    sel_index: torch.Tensor,
    sel: torch.Tensor,
    grads: torch.Tensor,
    n_experts: int,
    key_dtype: torch.dtype,
    op_dtype: torch.dtype,
    out_index: torch.Tensor
):
    x = x.flatten(end_dim=-2)
    x = x.transpose(0, 1)
    grads = grads.flatten(end_dim=-2)
    sel = sel.flatten()
    M, _ = x.shape
    K, N = grads.shape
    # FIX: out must be atomic_add'able, which excludes bfloat16. Cast to key_dtype after. Maybe this could be f16
    out = torch.zeros((n_experts, M, N), device=x.device, dtype=torch.float32)
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), triton.cdiv(K, META['BLOCK_SIZE_K'] * META['K_BLOCKS'])
    )
    out_index_is_none = False
    if out_index.numel() == 1 and out_index == -1:
        out_index_is_none = True

    cvmm_backward_kernel3[grid](
        x, grads, out, sel_index, sel, out_index,
        M, N, K,
        x.stride(0), x.stride(1),
        grads.stride(0), grads.stride(1),
        out.stride(0), out.stride(1), out.stride(2),
        sel_index.stride(0), sel.stride(0), 0 if out_index_is_none else out_index.stride(0),
        out_index_is_none=out_index_is_none,
        out_dtype_id=dtype_to_type_id(out.dtype),
        dtype_id=dtype_to_type_id(op_dtype),
        allow_tf32=False #torch.backends.cuda.matmul.allow_tf32
    )
    return out.to(dtype=key_dtype)


class CVMM(torch.autograd.Function):
    warned = False

    @staticmethod
    def forward(
        ctx,
        x: torch.Tensor,
        sel_index: torch.Tensor,
        sel: torch.Tensor,
        keys: torch.Tensor,
        out_index: torch.Tensor | None = None,
        reduction_weight: torch.Tensor | None = None
    ):
        ctx.save_for_backward(x, keys, sel, sel_index, out_index, reduction_weight)

        # out_type = get_dtype()
        out_type = x.dtype
        # out_type = torch.float32
        if out_index is None:
            out_index = torch.tensor(-1).cuda()
        res = torch.ops.mylib.cvmm_triton(x, sel_index, sel, keys, out_type, out_index)

        if reduction_weight is not None:
            res = res.view(*reduction_weight.shape, res.shape[-1])
            res = (reduction_weight.unsqueeze(-2).type_as(res) @ res).squeeze(-2)

        ctx.op_type = out_type
        ctx.keys_type = keys.dtype
        ctx.dtype = out_type
        return res.type_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        x, keys, sel, sel_index, out_index, reduction_weight = ctx.saved_tensors
        keys_dt = keys

        # Backward for weight
        if reduction_weight is not None:
            # Project back the grads with he reduction weight, so the grad for the weight matrix is ok
            grad_output_w = reduction_weight.unsqueeze(-1).type_as(grad_output) @ grad_output.unsqueeze(-2)
        else:
            grad_output_w  = grad_output

        out_index_is_none = False
        if out_index is None:
            out_index_is_none = True
            out_index = torch.tensor(-1).cuda()

        grad_w = cvmm_triton_backward(
            x,
            sel_index,
            sel,
            grad_output_w,
            keys_dt.shape[0],
            ctx.keys_type,
            ctx.dtype,
            out_index=out_index
        )

        # Backward for input and reduction weight
        grad_w_off = None

        bw_index = sel_index if out_index_is_none else out_index
        bw_index_out = torch.tensor(-1).cuda()
        if reduction_weight is not None:
            # Hack the output indices to emulate repeats
            bw_index_out = bw_index
            bw_index = bw_index // reduction_weight.shape[-1]

        grad_x_full = torch.ops.mylib.cvmm_triton(
            grad_output,
            bw_index,
            sel,
            keys_dt.transpose(1,2),
            ctx.op_type,
            bw_index_out
        )

        grad_x_full = grad_x_full.view(*x.shape[:-1], -1, x.shape[-1])
        if reduction_weight is not None:
            # grad_x_full is the unscaled grad. For the input, we have to scale it, for the reduction wegiht,
            # we have to compute dot products with the input.
            grad_x = (reduction_weight.view(*grad_x_full.shape[:-1]).unsqueeze(-2).type_as(grad_x_full) @ grad_x_full).squeeze(-2)
            grad_w_off = (grad_x_full.type_as(reduction_weight) @ x.unsqueeze(-1).type_as(reduction_weight)).squeeze(-1).view_as(reduction_weight)
        elif grad_x_full.shape[-2] != 1:
            grad_x = grad_x_full.sum(-2)
        else:
            grad_x = grad_x_full

        grad_x = grad_x.view_as(x)

        return grad_x, None, None, grad_w, None, grad_w_off


def cvmm(x: torch.Tensor, sel: torch.Tensor | CVMMSel, keys: torch.Tensor):
    if not isinstance(sel, CVMMSel):
        sel = cvmm_prepare_sel(sel, keys.shape[0])
    assert x.dtype == keys.dtype, f"{x.dtype=} != {keys.dtype=}"

    return CVMM.apply(x, sel.sel_index, sel.sel, keys, sel.out_index, sel.reduction_weight)


def cvmm_prepare_sel2(sel: torch.Tensor, w: torch.Tensor | None = None) -> CVMMSel:
    # Has multiple selections for each batch element
    n_per_batch = sel.shape[-1]

    # indices = torch.arange(sel.nelement() // n_per_batch, device=sel.device, dtype=torch.int32)
    # indices = indices.repeat_interleave(n_per_batch).flatten()

    fsel = sel.flatten()
    ssel, sel_index = fsel.sort()

    # in_index = indices[sel_index]
    in_index = sel_index // n_per_batch

    return CVMMSel(sel, ssel.view_as(sel), in_index, sel_index, w)

#endregion
# -----------------------------------------------------------------------------
#region MoEUT
def log_mean(x: torch.Tensor, dim: int = 0):
    return x.logsumexp(dim) - torch.log(torch.tensor(x.shape[dim]))


def entropy_l(l: torch.Tensor) -> torch.Tensor:
    return - (l * l.exp()).sum(-1)


def entropy_reg(sel: torch.Tensor, dim: int) -> torch.Tensor:
    sel = F.log_softmax(sel, dim=-1)
    sel = log_mean(sel, dim)
    return - entropy_l(sel).mean()


class SigmaMoE(torch.nn.Module):
    def __init__(self, dmodel: int, n_experts: int, expert_size: int, k: int,
                 v_dim: int | None = None):

        super().__init__()
        self.k_dim = dmodel
        self.v_dim = v_dim if v_dim is not None else dmodel
        self.n_experts = n_experts
        self.expert_size = expert_size
        self.size = self.n_experts * self.expert_size
        self.k_vec_dim = self.k_dim
        self.num_heads = k

        self.keys = torch.nn.Parameter(torch.empty(self.n_experts, self.k_vec_dim, self.expert_size))
        self.values = torch.nn.Parameter(torch.empty(self.n_experts, self.expert_size, self.v_dim))
        self.sel_expert = torch.nn.Parameter(torch.empty(self.n_experts, self.k_vec_dim))

    @torch.no_grad
    def reset_parameters(self, std_scale: float):
        # nanogpt equivalence would be std_scale=(3 ** 0.5) * 0.5
        kbound = std_scale / (self.k_dim ** 0.5)
        self.keys.uniform_(-kbound, kbound)
        self.values.zero_()
        self.sel_expert.normal_()
        self.sel_expert.div_(self.sel_expert.norm(dim=1, keepdim=True))
        self.sel_expert.mul_(std_scale / (self.k_dim) ** 0.5)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        # Selection score calculation
        xnorm = norm(x)
        sel = F.linear(xnorm, self.sel_expert, None)
        sel_r = sel

        # Selection activation and topk
        sel = F.sigmoid(sel)

        sel_val, sel_index = sel.topk(self.num_heads, dim=-1, sorted=False)

        # Preprocess the selection indices. They will be needed for both layers and save some time
        sel_indices = cvmm_prepare_sel2(sel_index.int())

        # "Up-projection" layer for each head
        scores = cvmm(xnorm, sel_indices, self.keys)
        scores = F.relu(scores).square()

        # Down projection layer for each head
        sel_indices = sel_indices.clone()
        sel_indices.reduction_weight = sel_val
        sel_indices.sel_index = sel_indices.out_index
        sel_indices.out_index = None

        out = cvmm(scores, sel_indices, self.values)

        res = out.view(*x.shape[:-1], self.v_dim)
        return x + res, sel_r


class SwitchHeadRoPE(torch.nn.Module):
    def __init__(self, state_size: int, num_heads: int, n_experts: int, max_seq_len: int,
                 head_dim: int | None = None, moe_k: int = 2
                 ):

        super().__init__()

        self.input_size = state_size
        self.output_size = state_size
        self.pe_size = self.input_size
        self.moe_k = moe_k
        self.n_experts = n_experts

        self.num_heads = num_heads
        self.head_dim = head_dim or (state_size // num_heads)
        self.rotary = Rotary(self.head_dim, max_seq_len)

        self.q = torch.nn.Linear(self.input_size, self.head_dim * self.num_heads, bias=False)
        self.k = torch.nn.Linear(self.input_size, self.head_dim * self.num_heads, bias=False)

        self.v = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size, self.head_dim))
        self.o = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.head_dim, self.output_size))
        self.sel_v = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size))

        self.sel_o = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size))

    @torch.no_grad
    def reset_parameters(self, std_scale: float):
        bound = (3 ** 0.5) * 0.5 * (self.input_size ** -0.5)
        self.q.weight.uniform_(-bound, bound)
        self.k.weight.uniform_(-bound, bound)

        self.v.normal_(0, std_scale / self.input_size ** 0.5)
        self.o.zero_()

        self.sel_v.normal_()
        self.sel_v.div_(self.sel_v.norm(dim=1, keepdim=True))
        self.sel_v.mul_(std_scale / self.input_size ** 0.5)

        self.sel_o.normal_()
        self.sel_o.div_(self.sel_v.norm(dim=1, keepdim=True))
        self.sel_o.mul_(std_scale / self.input_size ** 0.5)


    def get_sel(self, t: torch.Tensor, w: torch.Tensor) -> tuple[CVMMSel, torch.Tensor]:
        sel = F.linear(t, w).float()
        sel = sel_raw = sel.view(*sel.shape[:-1], self.num_heads, -1)
        sel = sel.sigmoid()

        with torch.no_grad():
            _, sel_index = sel.topk(self.moe_k, dim=-1, sorted=False)
        sel_val = torch.gather(sel, -1, sel_index)

        sel_index_shifted = (torch.arange(self.num_heads, device=sel_index.device, dtype=sel_index.dtype) * self.n_experts).unsqueeze(-1) + sel_index
        return cvmm_prepare_sel2(sel_index_shifted.flatten(-2,-1), sel_val), sel_raw


    def forward(self, x: torch.Tensor, ve: torch.Tensor | None, block_mask: BlockMask
                ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        # *src: [batch_size, out_len, c]
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"

        xnorm = norm(x)
        q = self.q(xnorm)
        k = self.k(xnorm)

        v_sel, v_sel_r = self.get_sel(xnorm, self.sel_v)
        o_sel, o_sel_r = self.get_sel(xnorm, self.sel_o)

        v = cvmm(x, v_sel, self.v).transpose(-2, -3)

        q = q.view(B, T, self.num_heads, self.head_dim)
        k = k.view(B, T, self.num_heads, self.head_dim)

        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q).transpose(1,2), self.rotary(k).transpose(1,2)
        # if ve is not None:
        #     v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        # else: # skip mid-layers token value embeddings by @YouJiacheng
        #     v = self.lambdas[0] * v

        res = flex_attention(q, k, v, block_mask=block_mask, scale=0.12)
        res = res.transpose(1, 2)

        # The output selection indices are calculated from the current state and are also used for projecting "q".
        # But that projection needs to create multiple copies for the different heads. Here we already have the
        # heads, but we have to create copies for the top-k elements. We can calculate that from the reduction
        # weight. We also want to compute not only the weighted average between the top-k elements, but also
        # of the different heads. So reshape the reduction weight accordingly.
        o_sel.sel_index = o_sel.out_index // o_sel.reduction_weight.shape[-1]
        o_sel.reduction_weight = o_sel.reduction_weight.flatten(-2)
        out = cvmm(res, o_sel, self.o)

        return x + out, o_sel_r, v_sel_r


class MoEUTLayer(torch.nn.Module):
    def __init__(self, d_model: int, num_heads: int, ff_expert_size: int, ff_n_experts: int,
                 att_n_experts: int, max_seq_len: int, head_dim: int | None = None, att_k: int = 2,
                 ff_k: int = 8):

        super().__init__()
        self.attention = SwitchHeadRoPE(
            d_model, num_heads, att_n_experts, max_seq_len=max_seq_len, head_dim=head_dim, moe_k=att_k)
        self.ffn = SigmaMoE(d_model, ff_n_experts, ff_expert_size, k=ff_k)

    def forward(self, x: torch.Tensor, ve: torch.Tensor | None, block_mask: BlockMask) -> torch.Tensor:
        x, o_sel_r, v_sel_r = self.attention(x, ve, block_mask)
        x, ffn_sel_r = self.ffn(x)
        return x, o_sel_r, v_sel_r, ffn_sel_r


class MoEUT(torch.nn.Module):
    def __init__(self, d_model: int, n_layers: int, num_heads: int, ff_expert_size: int, ff_n_experts: int,
                 att_n_experts: int, max_seq_len: int, head_dim: int | None = None, att_k: int = 2,
                 ff_k: int = 8,
                 entropy_reg: float = 0.01, att_entropy_reg: float = 0.001,
                 group_size: int = 2):
        super().__init__()

        self.entropy_reg = entropy_reg
        self.att_entropy_reg = att_entropy_reg

        self.n_repeats = n_layers // group_size
        self.layers = torch.nn.ModuleList([
            MoEUTLayer(d_model, num_heads, ff_expert_size, ff_n_experts, att_n_experts,
                       max_seq_len, head_dim, att_k, ff_k)
            for _ in range(group_size)
        ])

        self.reset_parameters()

    def forward(self, x: torch.Tensor, block_mask: BlockMask) -> tuple[torch.Tensor, torch.Tensor]:
        # Run the model
        o_sels = defaultdict(list)
        v_sels = defaultdict(list)
        ffn_sels = defaultdict(list)
        for r in range(self.n_repeats):
            for li, layer in enumerate(self.layers):
                x, o_sel_r, v_sel_r, ffn_sel_r = layer(x, ve=None, block_mask=block_mask)
                o_sels[li].append(o_sel_r)
                v_sels[li].append(v_sel_r)
                ffn_sels[li].append(ffn_sel_r)

        ffn_reg_loss = torch.stack([
            entropy_reg(torch.stack(sel_hist, dim=-2).flatten(-3, -2), -2)
            for sel_hist in ffn_sels.values()
        ]).sum()
        att_reg_loss = torch.stack([
            entropy_reg(torch.stack(sel_hist, dim=-3).flatten(-4, -3), -3)
            for sel_hist in [*o_sels.values(), *v_sels.values()]
        ]).sum()

        reg_loss = self.entropy_reg * ffn_reg_loss + self.att_entropy_reg * att_reg_loss

        return x, reg_loss

    @torch.no_grad
    def reset_parameters(self):
        scale = (2 / (self.n_repeats * len(self.layers))) ** 0.5
        for layer in self.modules():
            if isinstance(layer, (SwitchHeadRoPE, SigmaMoE)):
                layer.reset_parameters(scale)



class MoEUTWrapper(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.moeut = MoEUT(
            d_model=model_dim,
            n_layers=num_layers,
            num_heads=num_heads,
            ff_expert_size=128,
            ff_n_experts=64,  # FIXME: Arbitrary decision
            att_n_experts=10, # !! From MoEUT paper, but they also used really weird numbers like d_head=41 so I don't trust their judgement
            max_seq_len=max_seq_len,
            head_dim=None,
            att_k=2,
            ff_k=8,
            entropy_reg=0.01,
            att_entropy_reg=0.001,
            group_size=2,
        )
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977


    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, _ = create_block_masks(input_seq, sliding_window_num_blocks)

        x = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x, reg_loss = self.moeut(x, long_bm)
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        if self.training:
            loss = loss + reg_loss
        return loss


#endregion
# -----------------------------------------------------------------------------
#region Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets
#endregion
# -----------------------------------------------------------------------------
#region utils, hyperparams
def print0(s, console=True):
    if master_process:
        timestamp = time.strftime("%H:%M:%S.") + f"{time.time() % 1:.3f}"[2:]
        s = f"{timestamp}: {s}"
        if console:
            print(s)
        if logfile:
            with open(logfile, "a") as f:
                print(s, file=f)

def log_mem():
    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )

@dataclass(frozen=True, kw_only=True)
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations: int = 1770 # number of iterations to run
    cooldown_frac: float = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len: int = 48*1024 # FlexAttention sequence length
    val_seq_len: int = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint: bool = False
    dev: bool = False

TEST_HPARAMS = Hyperparameters(
    train_files = "data/fineweb1B/fineweb_train_*.bin",
    val_files = "data/fineweb1B/fineweb_val_*.bin",
    val_tokens = 1048576,
    num_iterations = 1000, #770,
    cooldown_frac = 0.4,
    val_loss_every = 125,
    seq_len = 16*1024,
    val_seq_len = 4*16*1024,
    save_checkpoint = False,
    dev=False,
)
DEV_HPARAMS = Hyperparameters(
    train_files = "data/fineweb1B/fineweb_train_*.bin",
    val_files = "data/fineweb1B/fineweb_val_*.bin",
    val_tokens = 1024,
    num_iterations = 20,
    cooldown_frac = 0.4,
    val_loss_every = 125,
    seq_len = 512,
    val_seq_len = 512,
    save_checkpoint = False,
    dev=True,
)

#endregion
# -----------------------------------------------------------------------------
#region main()
master_process = None
logfile = None
def main(args = TEST_HPARAMS):
# def main(args = DEV_HPARAMS):
    global master_process, logfile
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    atexit.register(dist.destroy_process_group)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    if master_process and not args.dev:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)


    # begin by printing this file (the Python code)
    print0(code, console=False)
    print0("="*100, console=False)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}", console=False)
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}", console=False)
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi(), console=False)
    print0("="*100, console=False)
    atexit.register(log_mem)

    torch.random.manual_seed(0)
    torch.cuda.synchronize()
    print0("Init data")
    # load data
    train_batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

    torch.cuda.synchronize()
    print0("Init model")
    # REF: model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    model: nn.Module = MoEUTWrapper(vocab_size=50257, num_layers=12, num_heads=3, model_dim=384, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    model.bfloat16()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()

    # count parameters
    n_params_by_dtype = defaultdict(lambda: 0)
    for name, param in model.named_parameters():
        dist.broadcast(param.detach(), 0)
        n_params_by_dtype[param.dtype] += param.numel()
    for dt, n_params in n_params_by_dtype.items():
        print0(f"{dt}: {n_params/1024/1024:.3f}Mi params")
    print0(f"total: {sum(n_params_by_dtype.values())/1024/1024:.3f}Mi params")


    torch.cuda.synchronize()
    print0("Init optimizers")
    # collect the parameters to optimize
    hidden_matrix_params = [p for n, p in model.named_parameters() if p.ndim >= 2 and "embed" not in n and "lm_head" not in n]
    embed_params = [p for n, p in model.named_parameters() if "embed" in n]
    scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]
    params_sets = [hidden_matrix_params, embed_params, scalar_params, head_params]
    assert all(set(a).isdisjoint(b) for a in params_sets for b in params_sets if a is not b)

    # init the optimizer(s)
    lr_mod = (args.seq_len/Hyperparameters().seq_len/8) ** 0.5  # Correct LR based on difference in batch size vs original w/ 8 nodes
    print(f"{lr_mod=}")
    adam_params = [dict(params=head_params, lr=0.008*lr_mod), dict(params=embed_params, lr=0.6*lr_mod), dict(params=scalar_params, lr=0.04*lr_mod)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05*lr_mod, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(step: int):
        t = 1 - step / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    if not args.dev:
        model: nn.Module = torch.compile(model) #, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    print0("Starting train loop")
    train_steps = args.num_iterations
    prof = None
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # if step == 5:
        #     prof = profile(record_shapes=True, profile_memory=True, with_stack=True)
        #     prof.__enter__()
        #     prof.start()
        # if prof is not None:
        #     if step == 9:
        #         prof.__exit__(None, None, None)
        #         prof.export_chrome_trace("trace.json")
        #         prof = None
        #     else:
        #         prof.step()

        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_batch_size = world_size * args.val_seq_len
            assert args.val_tokens % val_batch_size == 0
            val_steps = args.val_tokens // val_batch_size
            val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for i in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION -----------------
        inputs, targets = next(train_loader)
        train_losses = []
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            loss = model(input_seq, target_seq, sw_num_blks(window_size))
            loss.backward()
            dist.all_reduce(loss, op=dist.ReduceOp.AVG)
            train_losses.append(loss.item())
            del loss
        train_loss = sum(train_losses or [torch.nan]) / max(len(train_losses), 1)
        for param in model.parameters():
            if param.grad is not None:
                dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        del param
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)

        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_loss:{train_loss:.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms {torch.cuda.memory_allocated()=}", console=True)

    if master_process and logfile is not None:
        try:
            new_logfile = input("Name run? ")
        except KeyboardInterrupt:
            breakpoint()
        if new_logfile:
            old_logfile = logfile
            logfile = f"logs/{new_logfile}.txt"
            os.rename(old_logfile, logfile)
            print0(f"Renamed {old_logfile} -> {logfile}")
    else:
        print(logfile)
#endregion
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    main()

16:26:01.226: ====================================================================================================
16:26:01.226: Running Python 3.12.7 (main, Oct 16 2024, 04:37:19) [Clang 18.1.8 ]
16:26:01.226: Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
16:26:01.307: Sun Feb  2 16:26:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090 Ti     On  |   00000000:2D:00.0 Off |                  Off |
|  0%   46C    P2             97W /  450W |     884MiB /  24564MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A        26      G   /Xwayland                                   N/A      |
|    0   N/A  N/A    272159      C   /python3.12                                 N/A      |
+-----------------------------------------------------------------------------------------+

16:26:01.307: ====================================================================================================
16:26:01.308: Init data
16:26:01.308: Init model
16:26:01.709: torch.bfloat16: 55.105Mi params
16:26:01.709: total: 55.105Mi params
16:26:01.709: Init optimizers
16:26:01.722: Starting train loop
16:26:17.970: step:0/1000 val_loss:10.8258 train_time:0ms step_avg:nanms
16:26:38.696: step:1/1000 train_loss:10.7327 train_time:20726ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:39.056: step:2/1000 train_loss:10.6462 train_time:21086ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:39.279: step:3/1000 train_loss:10.5144 train_time:21309ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:39.500: step:4/1000 train_loss:10.3530 train_time:21530ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:39.723: step:5/1000 train_loss:10.0108 train_time:21753ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:39.945: step:6/1000 train_loss:9.5963 train_time:21975ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:40.167: step:7/1000 train_loss:9.2666 train_time:22197ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:40.388: step:8/1000 train_loss:8.6302 train_time:22418ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:40.609: step:9/1000 train_loss:8.2339 train_time:22639ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:40.831: step:10/1000 train_loss:8.2267 train_time:22861ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:41.052: step:11/1000 train_loss:7.7596 train_time:222ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:41.276: step:12/1000 train_loss:7.2022 train_time:445ms step_avg:nanms torch.cuda.memory_allocated()=369551872
16:26:41.498: step:13/1000 train_loss:6.8233 train_time:668ms step_avg:222.51ms torch.cuda.memory_allocated()=369551872
16:26:41.722: step:14/1000 train_loss:7.3991 train_time:891ms step_avg:222.87ms torch.cuda.memory_allocated()=369551872
16:26:41.946: step:15/1000 train_loss:7.5875 train_time:1115ms step_avg:222.95ms torch.cuda.memory_allocated()=369551872
16:26:42.168: step:16/1000 train_loss:7.6023 train_time:1338ms step_avg:222.93ms torch.cuda.memory_allocated()=369551872
16:26:42.390: step:17/1000 train_loss:7.7041 train_time:1559ms step_avg:222.72ms torch.cuda.memory_allocated()=369551872
16:26:42.612: step:18/1000 train_loss:7.9551 train_time:1781ms step_avg:222.64ms torch.cuda.memory_allocated()=369551872
16:26:42.834: step:19/1000 train_loss:7.3151 train_time:2003ms step_avg:222.58ms torch.cuda.memory_allocated()=369551872
16:26:43.055: step:20/1000 train_loss:7.2649 train_time:2224ms step_avg:222.41ms torch.cuda.memory_allocated()=369551872
16:26:43.277: step:21/1000 train_loss:7.2210 train_time:2446ms step_avg:222.37ms torch.cuda.memory_allocated()=369551872
16:26:43.499: step:22/1000 train_loss:7.0840 train_time:2668ms step_avg:222.34ms torch.cuda.memory_allocated()=369551872
16:26:43.721: step:23/1000 train_loss:6.8920 train_time:2890ms step_avg:222.33ms torch.cuda.memory_allocated()=369551872
16:26:43.943: step:24/1000 train_loss:6.8549 train_time:3112ms step_avg:222.31ms torch.cuda.memory_allocated()=369551872
16:26:44.166: step:25/1000 train_loss:6.8738 train_time:3335ms step_avg:222.32ms torch.cuda.memory_allocated()=369551872
16:26:44.388: step:26/1000 train_loss:6.7955 train_time:3557ms step_avg:222.30ms torch.cuda.memory_allocated()=369551872
16:26:44.610: step:27/1000 train_loss:6.8877 train_time:3779ms step_avg:222.28ms torch.cuda.memory_allocated()=369551872
16:26:44.832: step:28/1000 train_loss:7.1250 train_time:4001ms step_avg:222.28ms torch.cuda.memory_allocated()=369551872
16:26:45.054: step:29/1000 train_loss:7.0972 train_time:4223ms step_avg:222.28ms torch.cuda.memory_allocated()=369551872
16:26:45.278: step:30/1000 train_loss:6.7824 train_time:4447ms step_avg:222.37ms torch.cuda.memory_allocated()=369551872
16:26:45.500: step:31/1000 train_loss:6.8759 train_time:4670ms step_avg:222.36ms torch.cuda.memory_allocated()=369551872
16:26:45.722: step:32/1000 train_loss:6.5973 train_time:4891ms step_avg:222.34ms torch.cuda.memory_allocated()=369551872
16:26:45.945: step:33/1000 train_loss:6.7588 train_time:5114ms step_avg:222.33ms torch.cuda.memory_allocated()=369551872
16:26:46.167: step:34/1000 train_loss:6.4606 train_time:5336ms step_avg:222.35ms torch.cuda.memory_allocated()=369551872
16:26:46.389: step:35/1000 train_loss:6.8992 train_time:5558ms step_avg:222.33ms torch.cuda.memory_allocated()=369551872
16:26:46.610: step:36/1000 train_loss:6.5711 train_time:5779ms step_avg:222.26ms torch.cuda.memory_allocated()=369551872
16:26:46.831: step:37/1000 train_loss:6.6157 train_time:6000ms step_avg:222.23ms torch.cuda.memory_allocated()=369551872
16:26:47.054: step:38/1000 train_loss:6.4585 train_time:6223ms step_avg:222.24ms torch.cuda.memory_allocated()=369551872
16:26:47.277: step:39/1000 train_loss:6.6666 train_time:6446ms step_avg:222.26ms torch.cuda.memory_allocated()=369551872
16:26:47.498: step:40/1000 train_loss:6.4388 train_time:6667ms step_avg:222.23ms torch.cuda.memory_allocated()=369551872
16:26:47.719: step:41/1000 train_loss:6.4493 train_time:6888ms step_avg:222.19ms torch.cuda.memory_allocated()=369551872
16:26:47.940: step:42/1000 train_loss:6.3468 train_time:7109ms step_avg:222.17ms torch.cuda.memory_allocated()=369551872
16:26:48.162: step:43/1000 train_loss:6.4778 train_time:7331ms step_avg:222.15ms torch.cuda.memory_allocated()=369551872
16:26:48.384: step:44/1000 train_loss:6.2004 train_time:7553ms step_avg:222.14ms torch.cuda.memory_allocated()=369551872
16:26:48.605: step:45/1000 train_loss:7.0896 train_time:7774ms step_avg:222.11ms torch.cuda.memory_allocated()=369551872
16:26:48.825: step:46/1000 train_loss:6.3512 train_time:7994ms step_avg:222.05ms torch.cuda.memory_allocated()=369551872
16:26:49.046: step:47/1000 train_loss:6.3880 train_time:8216ms step_avg:222.04ms torch.cuda.memory_allocated()=369551872
16:26:49.269: step:48/1000 train_loss:6.7248 train_time:8438ms step_avg:222.05ms torch.cuda.memory_allocated()=369551872
16:26:49.491: step:49/1000 train_loss:6.4538 train_time:8660ms step_avg:222.05ms torch.cuda.memory_allocated()=369551872
16:26:49.711: step:50/1000 train_loss:6.4895 train_time:8880ms step_avg:222.01ms torch.cuda.memory_allocated()=369551872
16:26:49.935: step:51/1000 train_loss:6.3380 train_time:9104ms step_avg:222.05ms torch.cuda.memory_allocated()=369551872
16:26:50.157: step:52/1000 train_loss:6.6177 train_time:9326ms step_avg:222.06ms torch.cuda.memory_allocated()=369551872
16:26:50.381: step:53/1000 train_loss:5.9505 train_time:9550ms step_avg:222.09ms torch.cuda.memory_allocated()=369551872
16:26:50.603: step:54/1000 train_loss:6.3524 train_time:9772ms step_avg:222.09ms torch.cuda.memory_allocated()=369551872
16:26:50.825: step:55/1000 train_loss:6.8486 train_time:9994ms step_avg:222.08ms torch.cuda.memory_allocated()=369551872
16:26:51.048: step:56/1000 train_loss:6.2162 train_time:10217ms step_avg:222.12ms torch.cuda.memory_allocated()=369551872
16:26:51.273: step:57/1000 train_loss:6.4750 train_time:10442ms step_avg:222.17ms torch.cuda.memory_allocated()=369551872
16:26:51.495: step:58/1000 train_loss:6.4052 train_time:10664ms step_avg:222.17ms torch.cuda.memory_allocated()=369551872
16:26:51.716: step:59/1000 train_loss:6.1454 train_time:10885ms step_avg:222.14ms torch.cuda.memory_allocated()=369551872
16:26:51.937: step:60/1000 train_loss:6.5258 train_time:11106ms step_avg:222.12ms torch.cuda.memory_allocated()=369551872
16:26:52.157: step:61/1000 train_loss:6.7109 train_time:11326ms step_avg:222.09ms torch.cuda.memory_allocated()=369551872
16:26:52.379: step:62/1000 train_loss:6.5931 train_time:11548ms step_avg:222.08ms torch.cuda.memory_allocated()=369551872
16:26:52.602: step:63/1000 train_loss:6.4916 train_time:11771ms step_avg:222.10ms torch.cuda.memory_allocated()=369551872
16:26:52.823: step:64/1000 train_loss:6.4374 train_time:11992ms step_avg:222.07ms torch.cuda.memory_allocated()=369551872
16:26:53.044: step:65/1000 train_loss:6.5447 train_time:12213ms step_avg:222.05ms torch.cuda.memory_allocated()=369551872
16:26:53.266: step:66/1000 train_loss:6.2027 train_time:12435ms step_avg:222.05ms torch.cuda.memory_allocated()=369551872
16:26:53.488: step:67/1000 train_loss:6.6389 train_time:12657ms step_avg:222.05ms torch.cuda.memory_allocated()=369551872
16:26:53.710: step:68/1000 train_loss:6.2426 train_time:12879ms step_avg:222.06ms torch.cuda.memory_allocated()=369551872
16:26:53.937: step:69/1000 train_loss:6.1216 train_time:13106ms step_avg:222.14ms torch.cuda.memory_allocated()=369551872
16:26:54.160: step:70/1000 train_loss:6.2340 train_time:13329ms step_avg:222.15ms torch.cuda.memory_allocated()=369551872
16:26:54.382: step:71/1000 train_loss:6.5094 train_time:13551ms step_avg:222.15ms torch.cuda.memory_allocated()=369551872
16:26:54.604: step:72/1000 train_loss:6.3770 train_time:13773ms step_avg:222.14ms torch.cuda.memory_allocated()=369551872
16:26:54.826: step:73/1000 train_loss:6.2933 train_time:13995ms step_avg:222.15ms torch.cuda.memory_allocated()=369551872
16:26:55.047: step:74/1000 train_loss:6.4145 train_time:14217ms step_avg:222.13ms torch.cuda.memory_allocated()=369551872
16:26:55.269: step:75/1000 train_loss:6.0508 train_time:14438ms step_avg:222.13ms torch.cuda.memory_allocated()=369551872
16:26:55.497: step:76/1000 train_loss:6.2807 train_time:14667ms step_avg:222.22ms torch.cuda.memory_allocated()=369551872
16:26:55.726: step:77/1000 train_loss:6.1852 train_time:14895ms step_avg:222.32ms torch.cuda.memory_allocated()=369551872
16:26:55.953: step:78/1000 train_loss:6.0186 train_time:15122ms step_avg:222.38ms torch.cuda.memory_allocated()=369551872
16:26:56.181: step:79/1000 train_loss:6.3082 train_time:15350ms step_avg:222.46ms torch.cuda.memory_allocated()=369551872
16:26:56.409: step:80/1000 train_loss:6.0566 train_time:15579ms step_avg:222.55ms torch.cuda.memory_allocated()=369551872
16:26:56.638: step:81/1000 train_loss:6.0387 train_time:15807ms step_avg:222.63ms torch.cuda.memory_allocated()=369551872
16:26:56.867: step:82/1000 train_loss:6.1286 train_time:16036ms step_avg:222.72ms torch.cuda.memory_allocated()=369551872
16:26:57.099: step:83/1000 train_loss:6.4042 train_time:16268ms step_avg:222.85ms torch.cuda.memory_allocated()=369551872
16:26:57.327: step:84/1000 train_loss:6.6843 train_time:16496ms step_avg:222.92ms torch.cuda.memory_allocated()=369551872
16:26:57.556: step:85/1000 train_loss:6.0934 train_time:16725ms step_avg:223.00ms torch.cuda.memory_allocated()=369551872
16:26:57.783: step:86/1000 train_loss:6.3746 train_time:16952ms step_avg:223.06ms torch.cuda.memory_allocated()=369551872
16:26:58.011: step:87/1000 train_loss:6.3225 train_time:17180ms step_avg:223.12ms torch.cuda.memory_allocated()=369551872
16:26:58.239: step:88/1000 train_loss:6.1889 train_time:17408ms step_avg:223.18ms torch.cuda.memory_allocated()=369551872
16:26:58.466: step:89/1000 train_loss:6.1418 train_time:17636ms step_avg:223.24ms torch.cuda.memory_allocated()=369551872
16:26:58.696: step:90/1000 train_loss:6.2794 train_time:17865ms step_avg:223.32ms torch.cuda.memory_allocated()=369551872
16:26:58.926: step:91/1000 train_loss:6.1360 train_time:18095ms step_avg:223.39ms torch.cuda.memory_allocated()=369551872
16:26:59.155: step:92/1000 train_loss:6.4309 train_time:18324ms step_avg:223.46ms torch.cuda.memory_allocated()=369551872
16:26:59.384: step:93/1000 train_loss:6.9234 train_time:18553ms step_avg:223.53ms torch.cuda.memory_allocated()=369551872
16:26:59.612: step:94/1000 train_loss:6.0877 train_time:18781ms step_avg:223.58ms torch.cuda.memory_allocated()=369551872
16:26:59.839: step:95/1000 train_loss:6.2500 train_time:19008ms step_avg:223.63ms torch.cuda.memory_allocated()=369551872
16:27:00.069: step:96/1000 train_loss:5.9130 train_time:19238ms step_avg:223.69ms torch.cuda.memory_allocated()=369551872
16:27:00.298: step:97/1000 train_loss:5.9884 train_time:19467ms step_avg:223.76ms torch.cuda.memory_allocated()=369551872
16:27:00.527: step:98/1000 train_loss:6.2592 train_time:19696ms step_avg:223.82ms torch.cuda.memory_allocated()=369551872
16:27:00.757: step:99/1000 train_loss:5.7176 train_time:19926ms step_avg:223.89ms torch.cuda.memory_allocated()=369551872
16:27:00.984: step:100/1000 train_loss:6.1819 train_time:20154ms step_avg:223.93ms torch.cuda.memory_allocated()=369551872
16:27:01.214: step:101/1000 train_loss:6.1059 train_time:20383ms step_avg:223.99ms torch.cuda.memory_allocated()=369551872
16:27:01.442: step:102/1000 train_loss:5.8722 train_time:20611ms step_avg:224.03ms torch.cuda.memory_allocated()=369551872
16:27:01.669: step:103/1000 train_loss:6.0065 train_time:20838ms step_avg:224.06ms torch.cuda.memory_allocated()=369551872
16:27:01.897: step:104/1000 train_loss:5.9777 train_time:21066ms step_avg:224.10ms torch.cuda.memory_allocated()=369551872
16:27:02.125: step:105/1000 train_loss:6.0354 train_time:21294ms step_avg:224.15ms torch.cuda.memory_allocated()=369551872
16:27:02.356: step:106/1000 train_loss:6.3460 train_time:21525ms step_avg:224.22ms torch.cuda.memory_allocated()=369551872
16:27:02.584: step:107/1000 train_loss:6.0974 train_time:21753ms step_avg:224.26ms torch.cuda.memory_allocated()=369551872
16:27:02.812: step:108/1000 train_loss:6.1684 train_time:21981ms step_avg:224.29ms torch.cuda.memory_allocated()=369551872
16:27:03.041: step:109/1000 train_loss:6.1535 train_time:22210ms step_avg:224.35ms torch.cuda.memory_allocated()=369551872
16:27:03.270: step:110/1000 train_loss:5.7024 train_time:22439ms step_avg:224.39ms torch.cuda.memory_allocated()=369551872
16:27:03.498: step:111/1000 train_loss:6.0098 train_time:22667ms step_avg:224.43ms torch.cuda.memory_allocated()=369551872
16:27:03.726: step:112/1000 train_loss:6.1674 train_time:22895ms step_avg:224.47ms torch.cuda.memory_allocated()=369551872
16:27:03.955: step:113/1000 train_loss:5.8955 train_time:23124ms step_avg:224.50ms torch.cuda.memory_allocated()=369551872
16:27:04.185: step:114/1000 train_loss:5.9131 train_time:23354ms step_avg:224.56ms torch.cuda.memory_allocated()=369551872
16:27:04.415: step:115/1000 train_loss:5.8406 train_time:23584ms step_avg:224.61ms torch.cuda.memory_allocated()=369551872
16:27:04.642: step:116/1000 train_loss:6.0496 train_time:23811ms step_avg:224.63ms torch.cuda.memory_allocated()=369551872
16:27:04.870: step:117/1000 train_loss:6.1333 train_time:24039ms step_avg:224.66ms torch.cuda.memory_allocated()=369551872
16:27:05.097: step:118/1000 train_loss:5.9440 train_time:24267ms step_avg:224.69ms torch.cuda.memory_allocated()=369551872
16:27:05.326: step:119/1000 train_loss:5.8107 train_time:24495ms step_avg:224.72ms torch.cuda.memory_allocated()=369551872
16:27:05.556: step:120/1000 train_loss:5.6781 train_time:24726ms step_avg:224.78ms torch.cuda.memory_allocated()=369551872
16:27:05.786: step:121/1000 train_loss:6.4633 train_time:24955ms step_avg:224.82ms torch.cuda.memory_allocated()=369551872
16:27:06.014: step:122/1000 train_loss:6.2825 train_time:25183ms step_avg:224.85ms torch.cuda.memory_allocated()=369551872
16:27:06.243: step:123/1000 train_loss:6.0612 train_time:25412ms step_avg:224.89ms torch.cuda.memory_allocated()=369551872
16:27:06.472: step:124/1000 train_loss:6.0385 train_time:25641ms step_avg:224.92ms torch.cuda.memory_allocated()=369551872
16:27:06.699: step:125/1000 train_loss:6.0633 train_time:25868ms step_avg:224.94ms torch.cuda.memory_allocated()=369551872
16:27:09.690: step:125/1000 val_loss:6.1801 train_time:25869ms step_avg:224.95ms
16:27:09.921: step:126/1000 train_loss:6.4463 train_time:26099ms step_avg:225.00ms torch.cuda.memory_allocated()=369551872
16:27:10.151: step:127/1000 train_loss:6.2642 train_time:26329ms step_avg:225.03ms torch.cuda.memory_allocated()=369551872
16:27:10.378: step:128/1000 train_loss:5.9894 train_time:26556ms step_avg:225.05ms torch.cuda.memory_allocated()=369551872
16:27:10.608: step:129/1000 train_loss:5.7862 train_time:26786ms step_avg:225.09ms torch.cuda.memory_allocated()=369551872
16:27:10.837: step:130/1000 train_loss:6.2665 train_time:27015ms step_avg:225.13ms torch.cuda.memory_allocated()=369551872
16:27:11.066: step:131/1000 train_loss:5.9678 train_time:27244ms step_avg:225.15ms torch.cuda.memory_allocated()=369551872
16:27:11.296: step:132/1000 train_loss:6.0769 train_time:27474ms step_avg:225.19ms torch.cuda.memory_allocated()=369551872
16:27:11.524: step:133/1000 train_loss:6.0520 train_time:27702ms step_avg:225.22ms torch.cuda.memory_allocated()=369551872
16:27:11.753: step:134/1000 train_loss:5.9552 train_time:27931ms step_avg:225.25ms torch.cuda.memory_allocated()=369551872
16:27:11.983: step:135/1000 train_loss:6.4616 train_time:28161ms step_avg:225.29ms torch.cuda.memory_allocated()=369551872
16:27:12.212: step:136/1000 train_loss:6.0908 train_time:28390ms step_avg:225.32ms torch.cuda.memory_allocated()=369551872
16:27:12.441: step:137/1000 train_loss:5.9587 train_time:28619ms step_avg:225.34ms torch.cuda.memory_allocated()=369551872
16:27:12.668: step:138/1000 train_loss:5.9212 train_time:28846ms step_avg:225.36ms torch.cuda.memory_allocated()=369551872
16:27:12.897: step:139/1000 train_loss:6.2030 train_time:29075ms step_avg:225.39ms torch.cuda.memory_allocated()=369551872
16:27:13.126: step:140/1000 train_loss:5.6983 train_time:29304ms step_avg:225.41ms torch.cuda.memory_allocated()=369551872
16:27:13.354: step:141/1000 train_loss:6.0084 train_time:29532ms step_avg:225.43ms torch.cuda.memory_allocated()=369551872
16:27:13.581: step:142/1000 train_loss:5.9394 train_time:29759ms step_avg:225.45ms torch.cuda.memory_allocated()=369551872
16:27:13.808: step:143/1000 train_loss:6.0134 train_time:29986ms step_avg:225.46ms torch.cuda.memory_allocated()=369551872
16:27:14.036: step:144/1000 train_loss:6.4121 train_time:30214ms step_avg:225.48ms torch.cuda.memory_allocated()=369551872
16:27:14.265: step:145/1000 train_loss:5.9410 train_time:30443ms step_avg:225.50ms torch.cuda.memory_allocated()=369551872
16:27:14.494: step:146/1000 train_loss:6.1077 train_time:30672ms step_avg:225.53ms torch.cuda.memory_allocated()=369551872
16:27:14.723: step:147/1000 train_loss:5.9397 train_time:30901ms step_avg:225.56ms torch.cuda.memory_allocated()=369551872
16:27:14.952: step:148/1000 train_loss:5.6417 train_time:31130ms step_avg:225.58ms torch.cuda.memory_allocated()=369551872
16:27:15.183: step:149/1000 train_loss:5.6687 train_time:31361ms step_avg:225.62ms torch.cuda.memory_allocated()=369551872
16:27:15.417: step:150/1000 train_loss:5.6955 train_time:31595ms step_avg:225.68ms torch.cuda.memory_allocated()=369551872
16:27:15.650: step:151/1000 train_loss:5.7994 train_time:31828ms step_avg:225.73ms torch.cuda.memory_allocated()=369551872
16:27:15.883: step:152/1000 train_loss:6.0290 train_time:32061ms step_avg:225.78ms torch.cuda.memory_allocated()=369551872
16:27:16.116: step:153/1000 train_loss:5.8909 train_time:32294ms step_avg:225.83ms torch.cuda.memory_allocated()=369551872
16:27:16.350: step:154/1000 train_loss:5.9428 train_time:32528ms step_avg:225.89ms torch.cuda.memory_allocated()=369551872
16:27:16.583: step:155/1000 train_loss:5.7390 train_time:32761ms step_avg:225.94ms torch.cuda.memory_allocated()=369551872
16:27:16.816: step:156/1000 train_loss:6.0389 train_time:32994ms step_avg:225.99ms torch.cuda.memory_allocated()=369551872
16:27:17.049: step:157/1000 train_loss:5.9697 train_time:33227ms step_avg:226.03ms torch.cuda.memory_allocated()=369551872
16:27:17.282: step:158/1000 train_loss:6.0005 train_time:33460ms step_avg:226.08ms torch.cuda.memory_allocated()=369551872
16:27:17.516: step:159/1000 train_loss:5.7972 train_time:33693ms step_avg:226.13ms torch.cuda.memory_allocated()=369551872
16:27:17.750: step:160/1000 train_loss:5.8117 train_time:33928ms step_avg:226.19ms torch.cuda.memory_allocated()=369551872
16:27:17.984: step:161/1000 train_loss:5.6895 train_time:34162ms step_avg:226.24ms torch.cuda.memory_allocated()=369551872
16:27:18.219: step:162/1000 train_loss:5.8809 train_time:34397ms step_avg:226.30ms torch.cuda.memory_allocated()=369551872
16:27:18.451: step:163/1000 train_loss:5.7735 train_time:34629ms step_avg:226.34ms torch.cuda.memory_allocated()=369551872
16:27:18.684: step:164/1000 train_loss:5.6492 train_time:34862ms step_avg:226.37ms torch.cuda.memory_allocated()=369551872
16:27:18.918: step:165/1000 train_loss:5.7649 train_time:35096ms step_avg:226.43ms torch.cuda.memory_allocated()=369551872
16:27:19.151: step:166/1000 train_loss:5.6748 train_time:35329ms step_avg:226.47ms torch.cuda.memory_allocated()=369551872
16:27:19.385: step:167/1000 train_loss:5.9139 train_time:35563ms step_avg:226.52ms torch.cuda.memory_allocated()=369551872
16:27:19.620: step:168/1000 train_loss:6.0337 train_time:35798ms step_avg:226.57ms torch.cuda.memory_allocated()=369551872
16:27:19.855: step:169/1000 train_loss:5.6401 train_time:36033ms step_avg:226.62ms torch.cuda.memory_allocated()=369551872
16:27:20.088: step:170/1000 train_loss:5.6769 train_time:36266ms step_avg:226.66ms torch.cuda.memory_allocated()=369551872
16:27:20.321: step:171/1000 train_loss:5.8272 train_time:36499ms step_avg:226.70ms torch.cuda.memory_allocated()=369551872
16:27:20.555: step:172/1000 train_loss:5.7758 train_time:36733ms step_avg:226.75ms torch.cuda.memory_allocated()=369551872
16:27:20.790: step:173/1000 train_loss:5.6169 train_time:36968ms step_avg:226.80ms torch.cuda.memory_allocated()=369551872
16:27:21.024: step:174/1000 train_loss:5.9301 train_time:37202ms step_avg:226.84ms torch.cuda.memory_allocated()=369551872
16:27:21.259: step:175/1000 train_loss:5.8803 train_time:37437ms step_avg:226.89ms torch.cuda.memory_allocated()=369551872
16:27:21.493: step:176/1000 train_loss:5.8581 train_time:37671ms step_avg:226.93ms torch.cuda.memory_allocated()=369551872
16:27:21.727: step:177/1000 train_loss:5.7055 train_time:37905ms step_avg:226.98ms torch.cuda.memory_allocated()=369551872
16:27:21.961: step:178/1000 train_loss:5.9024 train_time:38139ms step_avg:227.02ms torch.cuda.memory_allocated()=369551872
16:27:22.195: step:179/1000 train_loss:5.7341 train_time:38373ms step_avg:227.06ms torch.cuda.memory_allocated()=369551872
16:27:22.428: step:180/1000 train_loss:5.6525 train_time:38606ms step_avg:227.09ms torch.cuda.memory_allocated()=369551872
16:27:22.662: step:181/1000 train_loss:5.8214 train_time:38840ms step_avg:227.13ms torch.cuda.memory_allocated()=369551872
16:27:22.894: step:182/1000 train_loss:5.5736 train_time:39072ms step_avg:227.16ms torch.cuda.memory_allocated()=369551872
16:27:23.128: step:183/1000 train_loss:5.7398 train_time:39306ms step_avg:227.20ms torch.cuda.memory_allocated()=369551872
16:27:23.361: step:184/1000 train_loss:5.8083 train_time:39539ms step_avg:227.24ms torch.cuda.memory_allocated()=369551872
16:27:23.596: step:185/1000 train_loss:5.6477 train_time:39774ms step_avg:227.28ms torch.cuda.memory_allocated()=369551872
16:27:23.829: step:186/1000 train_loss:5.8466 train_time:40007ms step_avg:227.31ms torch.cuda.memory_allocated()=369551872
16:27:24.063: step:187/1000 train_loss:5.8735 train_time:40241ms step_avg:227.35ms torch.cuda.memory_allocated()=369551872
16:27:24.299: step:188/1000 train_loss:6.0234 train_time:40477ms step_avg:227.40ms torch.cuda.memory_allocated()=369551872
16:27:24.533: step:189/1000 train_loss:5.8575 train_time:40710ms step_avg:227.43ms torch.cuda.memory_allocated()=369551872
16:27:24.766: step:190/1000 train_loss:5.8988 train_time:40944ms step_avg:227.47ms torch.cuda.memory_allocated()=369551872
16:27:24.000: step:191/1000 train_loss:5.9049 train_time:41178ms step_avg:227.50ms torch.cuda.memory_allocated()=369551872
16:27:25.234: step:192/1000 train_loss:6.0486 train_time:41412ms step_avg:227.54ms torch.cuda.memory_allocated()=369551872
16:27:25.468: step:193/1000 train_loss:5.8464 train_time:41646ms step_avg:227.57ms torch.cuda.memory_allocated()=369551872
16:27:25.702: step:194/1000 train_loss:5.7827 train_time:41880ms step_avg:227.61ms torch.cuda.memory_allocated()=369551872
16:27:25.936: step:195/1000 train_loss:6.4281 train_time:42114ms step_avg:227.64ms torch.cuda.memory_allocated()=369551872
16:27:26.170: step:196/1000 train_loss:6.1286 train_time:42348ms step_avg:227.68ms torch.cuda.memory_allocated()=369551872
16:27:26.405: step:197/1000 train_loss:5.7049 train_time:42583ms step_avg:227.71ms torch.cuda.memory_allocated()=369551872
16:27:26.639: step:198/1000 train_loss:5.7461 train_time:42816ms step_avg:227.75ms torch.cuda.memory_allocated()=369551872
16:27:26.872: step:199/1000 train_loss:5.7555 train_time:43050ms step_avg:227.78ms torch.cuda.memory_allocated()=369551872
16:27:27.106: step:200/1000 train_loss:5.7924 train_time:43284ms step_avg:227.81ms torch.cuda.memory_allocated()=369551872
16:27:27.339: step:201/1000 train_loss:5.6814 train_time:43517ms step_avg:227.84ms torch.cuda.memory_allocated()=369551872
16:27:27.574: step:202/1000 train_loss:5.9386 train_time:43752ms step_avg:227.87ms torch.cuda.memory_allocated()=369551872
16:27:27.807: step:203/1000 train_loss:5.9846 train_time:43985ms step_avg:227.90ms torch.cuda.memory_allocated()=369551872
16:27:28.041: step:204/1000 train_loss:5.4863 train_time:44219ms step_avg:227.93ms torch.cuda.memory_allocated()=369551872
16:27:28.275: step:205/1000 train_loss:6.1823 train_time:44453ms step_avg:227.96ms torch.cuda.memory_allocated()=369551872
16:27:28.507: step:206/1000 train_loss:6.1408 train_time:44685ms step_avg:227.98ms torch.cuda.memory_allocated()=369551872
16:27:28.741: step:207/1000 train_loss:5.9602 train_time:44918ms step_avg:228.01ms torch.cuda.memory_allocated()=369551872
16:27:28.973: step:208/1000 train_loss:5.9486 train_time:45151ms step_avg:228.04ms torch.cuda.memory_allocated()=369551872
16:27:29.207: step:209/1000 train_loss:5.7125 train_time:45385ms step_avg:228.06ms torch.cuda.memory_allocated()=369551872
16:27:29.441: step:210/1000 train_loss:5.9951 train_time:45619ms step_avg:228.09ms torch.cuda.memory_allocated()=369551872
16:27:29.673: step:211/1000 train_loss:5.9930 train_time:45851ms step_avg:228.11ms torch.cuda.memory_allocated()=369551872
16:27:29.907: step:212/1000 train_loss:5.9936 train_time:46085ms step_avg:228.15ms torch.cuda.memory_allocated()=369551872
16:27:30.142: step:213/1000 train_loss:5.8214 train_time:46320ms step_avg:228.18ms torch.cuda.memory_allocated()=369551872
16:27:30.376: step:214/1000 train_loss:5.6624 train_time:46554ms step_avg:228.20ms torch.cuda.memory_allocated()=369551872
16:27:30.610: step:215/1000 train_loss:5.8623 train_time:46788ms step_avg:228.23ms torch.cuda.memory_allocated()=369551872
16:27:30.843: step:216/1000 train_loss:5.9890 train_time:47021ms step_avg:228.26ms torch.cuda.memory_allocated()=369551872
16:27:31.075: step:217/1000 train_loss:5.6846 train_time:47253ms step_avg:228.28ms torch.cuda.memory_allocated()=369551872
16:27:31.310: step:218/1000 train_loss:5.7466 train_time:47488ms step_avg:228.31ms torch.cuda.memory_allocated()=369551872
16:27:31.543: step:219/1000 train_loss:5.8040 train_time:47721ms step_avg:228.33ms torch.cuda.memory_allocated()=369551872
16:27:31.776: step:220/1000 train_loss:5.7955 train_time:47954ms step_avg:228.35ms torch.cuda.memory_allocated()=369551872
16:27:32.009: step:221/1000 train_loss:5.9671 train_time:48187ms step_avg:228.38ms torch.cuda.memory_allocated()=369551872
16:27:32.243: step:222/1000 train_loss:5.8718 train_time:48421ms step_avg:228.40ms torch.cuda.memory_allocated()=369551872
16:27:32.477: step:223/1000 train_loss:5.8192 train_time:48655ms step_avg:228.43ms torch.cuda.memory_allocated()=369551872
16:27:32.715: step:224/1000 train_loss:5.6562 train_time:48893ms step_avg:228.47ms torch.cuda.memory_allocated()=369551872
16:27:32.951: step:225/1000 train_loss:5.6131 train_time:49129ms step_avg:228.51ms torch.cuda.memory_allocated()=369551872
16:27:33.189: step:226/1000 train_loss:6.4052 train_time:49367ms step_avg:228.55ms torch.cuda.memory_allocated()=369551872
16:27:33.427: step:227/1000 train_loss:5.7226 train_time:49605ms step_avg:228.59ms torch.cuda.memory_allocated()=369551872
16:27:33.663: step:228/1000 train_loss:5.6707 train_time:49841ms step_avg:228.63ms torch.cuda.memory_allocated()=369551872
16:27:33.899: step:229/1000 train_loss:5.6838 train_time:50077ms step_avg:228.66ms torch.cuda.memory_allocated()=369551872
16:27:34.137: step:230/1000 train_loss:5.7362 train_time:50315ms step_avg:228.70ms torch.cuda.memory_allocated()=369551872
16:27:34.373: step:231/1000 train_loss:5.9146 train_time:50551ms step_avg:228.74ms torch.cuda.memory_allocated()=369551872
16:27:34.611: step:232/1000 train_loss:5.5635 train_time:50789ms step_avg:228.78ms torch.cuda.memory_allocated()=369551872
16:27:34.850: step:233/1000 train_loss:5.9046 train_time:51028ms step_avg:228.82ms torch.cuda.memory_allocated()=369551872
16:27:35.086: step:234/1000 train_loss:5.5358 train_time:51264ms step_avg:228.86ms torch.cuda.memory_allocated()=369551872
16:27:35.323: step:235/1000 train_loss:5.8281 train_time:51501ms step_avg:228.90ms torch.cuda.memory_allocated()=369551872
16:27:35.562: step:236/1000 train_loss:5.6017 train_time:51740ms step_avg:228.94ms torch.cuda.memory_allocated()=369551872
16:27:35.799: step:237/1000 train_loss:5.4833 train_time:51977ms step_avg:228.97ms torch.cuda.memory_allocated()=369551872
16:27:36.037: step:238/1000 train_loss:5.6446 train_time:52215ms step_avg:229.01ms torch.cuda.memory_allocated()=369551872
16:27:36.275: step:239/1000 train_loss:5.7529 train_time:52453ms step_avg:229.05ms torch.cuda.memory_allocated()=369551872
16:27:36.512: step:240/1000 train_loss:5.5823 train_time:52690ms step_avg:229.09ms torch.cuda.memory_allocated()=369551872
16:27:36.752: step:241/1000 train_loss:4.7233 train_time:52930ms step_avg:229.13ms torch.cuda.memory_allocated()=369551872
16:27:36.990: step:242/1000 train_loss:5.7175 train_time:53168ms step_avg:229.17ms torch.cuda.memory_allocated()=369551872
16:27:37.228: step:243/1000 train_loss:5.8927 train_time:53406ms step_avg:229.21ms torch.cuda.memory_allocated()=369551872
16:27:37.465: step:244/1000 train_loss:5.6064 train_time:53643ms step_avg:229.24ms torch.cuda.memory_allocated()=369551872
16:27:37.701: step:245/1000 train_loss:5.6116 train_time:53879ms step_avg:229.27ms torch.cuda.memory_allocated()=369551872
16:27:37.938: step:246/1000 train_loss:5.6734 train_time:54116ms step_avg:229.31ms torch.cuda.memory_allocated()=369551872
16:27:38.176: step:247/1000 train_loss:5.8688 train_time:54354ms step_avg:229.34ms torch.cuda.memory_allocated()=369551872
16:27:38.414: step:248/1000 train_loss:5.6167 train_time:54592ms step_avg:229.38ms torch.cuda.memory_allocated()=369551872
16:27:38.652: step:249/1000 train_loss:6.1085 train_time:54829ms step_avg:229.41ms torch.cuda.memory_allocated()=369551872
16:27:38.888: step:250/1000 train_loss:5.7221 train_time:55066ms step_avg:229.44ms torch.cuda.memory_allocated()=369551872
16:27:41.623: step:250/1000 val_loss:5.8739 train_time:55066ms step_avg:229.44ms
16:27:41.861: step:251/1000 train_loss:5.5076 train_time:55305ms step_avg:229.48ms torch.cuda.memory_allocated()=369551872
16:27:42.102: step:252/1000 train_loss:5.8035 train_time:55546ms step_avg:229.53ms torch.cuda.memory_allocated()=369551872
16:27:42.344: step:253/1000 train_loss:5.4824 train_time:55787ms step_avg:229.58ms torch.cuda.memory_allocated()=369551872
16:27:42.584: step:254/1000 train_loss:5.4451 train_time:56027ms step_avg:229.62ms torch.cuda.memory_allocated()=369551872
16:27:42.822: step:255/1000 train_loss:5.5569 train_time:56265ms step_avg:229.65ms torch.cuda.memory_allocated()=369551872
16:27:43.060: step:256/1000 train_loss:5.7098 train_time:56503ms step_avg:229.69ms torch.cuda.memory_allocated()=369551872
16:27:43.298: step:257/1000 train_loss:5.8968 train_time:56741ms step_avg:229.72ms torch.cuda.memory_allocated()=369551872
16:27:43.537: step:258/1000 train_loss:5.6842 train_time:56980ms step_avg:229.76ms torch.cuda.memory_allocated()=369551872
16:27:43.775: step:259/1000 train_loss:5.7373 train_time:57218ms step_avg:229.79ms torch.cuda.memory_allocated()=369551872
16:27:44.012: step:260/1000 train_loss:5.7610 train_time:57455ms step_avg:229.82ms torch.cuda.memory_allocated()=369551872
16:27:44.252: step:261/1000 train_loss:5.9484 train_time:57695ms step_avg:229.86ms torch.cuda.memory_allocated()=369551872
16:27:44.490: step:262/1000 train_loss:5.6165 train_time:57933ms step_avg:229.89ms torch.cuda.memory_allocated()=369551872
16:27:44.729: step:263/1000 train_loss:5.7305 train_time:58172ms step_avg:229.93ms torch.cuda.memory_allocated()=369551872
16:27:44.970: step:264/1000 train_loss:5.6653 train_time:58414ms step_avg:229.97ms torch.cuda.memory_allocated()=369551872
16:27:45.212: step:265/1000 train_loss:5.8254 train_time:58655ms step_avg:230.02ms torch.cuda.memory_allocated()=369551872
16:27:45.450: step:266/1000 train_loss:5.5680 train_time:58893ms step_avg:230.05ms torch.cuda.memory_allocated()=369551872
16:27:45.689: step:267/1000 train_loss:5.8246 train_time:59132ms step_avg:230.09ms torch.cuda.memory_allocated()=369551872
16:27:45.927: step:268/1000 train_loss:5.7404 train_time:59370ms step_avg:230.12ms torch.cuda.memory_allocated()=369551872
16:27:46.166: step:269/1000 train_loss:5.7751 train_time:59609ms step_avg:230.15ms torch.cuda.memory_allocated()=369551872
16:27:46.404: step:270/1000 train_loss:5.8299 train_time:59847ms step_avg:230.18ms torch.cuda.memory_allocated()=369551872
16:27:46.642: step:271/1000 train_loss:5.7498 train_time:60086ms step_avg:230.21ms torch.cuda.memory_allocated()=369551872
16:27:46.881: step:272/1000 train_loss:6.0690 train_time:60324ms step_avg:230.24ms torch.cuda.memory_allocated()=369551872
16:27:47.119: step:273/1000 train_loss:5.8226 train_time:60562ms step_avg:230.27ms torch.cuda.memory_allocated()=369551872
16:27:47.357: step:274/1000 train_loss:5.7730 train_time:60800ms step_avg:230.30ms torch.cuda.memory_allocated()=369551872
16:27:47.596: step:275/1000 train_loss:5.8226 train_time:61039ms step_avg:230.34ms torch.cuda.memory_allocated()=369551872
16:27:47.833: step:276/1000 train_loss:6.0362 train_time:61276ms step_avg:230.36ms torch.cuda.memory_allocated()=369551872
16:27:48.073: step:277/1000 train_loss:5.6127 train_time:61516ms step_avg:230.40ms torch.cuda.memory_allocated()=369551872
16:27:48.313: step:278/1000 train_loss:5.5759 train_time:61756ms step_avg:230.43ms torch.cuda.memory_allocated()=369551872
16:27:48.552: step:279/1000 train_loss:5.6919 train_time:61995ms step_avg:230.46ms torch.cuda.memory_allocated()=369551872
16:27:48.789: step:280/1000 train_loss:5.7335 train_time:62233ms step_avg:230.49ms torch.cuda.memory_allocated()=369551872
16:27:49.026: step:281/1000 train_loss:5.5347 train_time:62469ms step_avg:230.51ms torch.cuda.memory_allocated()=369551872
16:27:49.264: step:282/1000 train_loss:5.5907 train_time:62707ms step_avg:230.54ms torch.cuda.memory_allocated()=369551872
16:27:49.503: step:283/1000 train_loss:5.4736 train_time:62946ms step_avg:230.57ms torch.cuda.memory_allocated()=369551872
16:27:49.741: step:284/1000 train_loss:5.6765 train_time:63184ms step_avg:230.60ms torch.cuda.memory_allocated()=369551872
16:27:49.981: step:285/1000 train_loss:5.9523 train_time:63424ms step_avg:230.63ms torch.cuda.memory_allocated()=369551872
16:27:50.223: step:286/1000 train_loss:6.7428 train_time:63666ms step_avg:230.67ms torch.cuda.memory_allocated()=369551872
16:27:50.463: step:287/1000 train_loss:6.2610 train_time:63906ms step_avg:230.71ms torch.cuda.memory_allocated()=369551872
16:27:50.701: step:288/1000 train_loss:5.9227 train_time:64144ms step_avg:230.73ms torch.cuda.memory_allocated()=369551872
16:27:50.939: step:289/1000 train_loss:5.9632 train_time:64382ms step_avg:230.76ms torch.cuda.memory_allocated()=369551872
16:27:51.177: step:290/1000 train_loss:5.6893 train_time:64621ms step_avg:230.79ms torch.cuda.memory_allocated()=369551872
16:27:51.415: step:291/1000 train_loss:5.2785 train_time:64858ms step_avg:230.81ms torch.cuda.memory_allocated()=369551872
16:27:51.653: step:292/1000 train_loss:5.9681 train_time:65096ms step_avg:230.84ms torch.cuda.memory_allocated()=369551872
16:27:51.891: step:293/1000 train_loss:5.4053 train_time:65334ms step_avg:230.86ms torch.cuda.memory_allocated()=369551872
16:27:52.129: step:294/1000 train_loss:5.3491 train_time:65572ms step_avg:230.89ms torch.cuda.memory_allocated()=369551872
16:27:52.368: step:295/1000 train_loss:5.6656 train_time:65811ms step_avg:230.92ms torch.cuda.memory_allocated()=369551872
16:27:52.607: step:296/1000 train_loss:5.6867 train_time:66051ms step_avg:230.95ms torch.cuda.memory_allocated()=369551872
16:27:52.845: step:297/1000 train_loss:5.5988 train_time:66289ms step_avg:230.97ms torch.cuda.memory_allocated()=369551872
16:27:53.087: step:298/1000 train_loss:5.4115 train_time:66530ms step_avg:231.01ms torch.cuda.memory_allocated()=369551872
16:27:53.330: step:299/1000 train_loss:5.8779 train_time:66774ms step_avg:231.05ms torch.cuda.memory_allocated()=369551872
16:27:53.572: step:300/1000 train_loss:5.7270 train_time:67016ms step_avg:231.09ms torch.cuda.memory_allocated()=369551872
16:27:53.816: step:301/1000 train_loss:5.5833 train_time:67259ms step_avg:231.13ms torch.cuda.memory_allocated()=369551872
16:27:54.058: step:302/1000 train_loss:5.2867 train_time:67502ms step_avg:231.17ms torch.cuda.memory_allocated()=369551872
16:27:54.302: step:303/1000 train_loss:5.5064 train_time:67745ms step_avg:231.21ms torch.cuda.memory_allocated()=369551872
16:27:54.544: step:304/1000 train_loss:5.7113 train_time:67988ms step_avg:231.25ms torch.cuda.memory_allocated()=369551872
16:27:54.787: step:305/1000 train_loss:5.5404 train_time:68230ms step_avg:231.29ms torch.cuda.memory_allocated()=369551872
16:27:55.029: step:306/1000 train_loss:5.8133 train_time:68472ms step_avg:231.33ms torch.cuda.memory_allocated()=369551872
16:27:55.270: step:307/1000 train_loss:5.5997 train_time:68713ms step_avg:231.36ms torch.cuda.memory_allocated()=369551872
16:27:55.511: step:308/1000 train_loss:5.7468 train_time:68955ms step_avg:231.39ms torch.cuda.memory_allocated()=369551872
16:27:55.753: step:309/1000 train_loss:5.4956 train_time:69197ms step_avg:231.43ms torch.cuda.memory_allocated()=369551872
16:27:55.993: step:310/1000 train_loss:5.6477 train_time:69436ms step_avg:231.45ms torch.cuda.memory_allocated()=369551872
16:27:56.234: step:311/1000 train_loss:5.5002 train_time:69677ms step_avg:231.49ms torch.cuda.memory_allocated()=369551872
16:27:56.475: step:312/1000 train_loss:5.5971 train_time:69918ms step_avg:231.52ms torch.cuda.memory_allocated()=369551872
16:27:56.716: step:313/1000 train_loss:5.4161 train_time:70159ms step_avg:231.55ms torch.cuda.memory_allocated()=369551872
16:27:56.959: step:314/1000 train_loss:5.5810 train_time:70402ms step_avg:231.59ms torch.cuda.memory_allocated()=369551872
16:27:57.204: step:315/1000 train_loss:5.8227 train_time:70648ms step_avg:231.63ms torch.cuda.memory_allocated()=369551872
16:27:57.446: step:316/1000 train_loss:5.7272 train_time:70890ms step_avg:231.67ms torch.cuda.memory_allocated()=369551872
16:27:57.686: step:317/1000 train_loss:5.5456 train_time:71130ms step_avg:231.69ms torch.cuda.memory_allocated()=369551872
16:27:57.928: step:318/1000 train_loss:5.5418 train_time:71371ms step_avg:231.72ms torch.cuda.memory_allocated()=369551872
16:27:58.169: step:319/1000 train_loss:5.6761 train_time:71612ms step_avg:231.76ms torch.cuda.memory_allocated()=369551872
16:27:58.412: step:320/1000 train_loss:5.4933 train_time:71855ms step_avg:231.79ms torch.cuda.memory_allocated()=369551872
16:27:58.654: step:321/1000 train_loss:5.4537 train_time:72097ms step_avg:231.82ms torch.cuda.memory_allocated()=369551872
16:27:58.896: step:322/1000 train_loss:5.6417 train_time:72339ms step_avg:231.86ms torch.cuda.memory_allocated()=369551872
16:27:59.137: step:323/1000 train_loss:5.7915 train_time:72580ms step_avg:231.89ms torch.cuda.memory_allocated()=369551872
16:27:59.377: step:324/1000 train_loss:5.7119 train_time:72820ms step_avg:231.91ms torch.cuda.memory_allocated()=369551872
16:27:59.619: step:325/1000 train_loss:5.5582 train_time:73062ms step_avg:231.94ms torch.cuda.memory_allocated()=369551872
16:27:59.861: step:326/1000 train_loss:5.6243 train_time:73304ms step_avg:231.97ms torch.cuda.memory_allocated()=369551872
16:28:00.105: step:327/1000 train_loss:5.6280 train_time:73548ms step_avg:232.01ms torch.cuda.memory_allocated()=369551872
16:28:00.348: step:328/1000 train_loss:5.5719 train_time:73791ms step_avg:232.05ms torch.cuda.memory_allocated()=369551872
16:28:00.589: step:329/1000 train_loss:5.6056 train_time:74033ms step_avg:232.08ms torch.cuda.memory_allocated()=369551872
16:28:00.832: step:330/1000 train_loss:5.5041 train_time:74275ms step_avg:232.11ms torch.cuda.memory_allocated()=369551872
16:28:01.073: step:331/1000 train_loss:5.5086 train_time:74516ms step_avg:232.14ms torch.cuda.memory_allocated()=369551872
16:28:01.315: step:332/1000 train_loss:5.9000 train_time:74758ms step_avg:232.17ms torch.cuda.memory_allocated()=369551872
16:28:01.559: step:333/1000 train_loss:5.5057 train_time:75002ms step_avg:232.21ms torch.cuda.memory_allocated()=369551872
16:28:01.799: step:334/1000 train_loss:5.3795 train_time:75243ms step_avg:232.23ms torch.cuda.memory_allocated()=369551872
16:28:02.041: step:335/1000 train_loss:5.7171 train_time:75485ms step_avg:232.26ms torch.cuda.memory_allocated()=369551872
16:28:02.285: step:336/1000 train_loss:5.4516 train_time:75728ms step_avg:232.29ms torch.cuda.memory_allocated()=369551872
16:28:02.527: step:337/1000 train_loss:5.6454 train_time:75970ms step_avg:232.33ms torch.cuda.memory_allocated()=369551872
16:28:02.959: step:338/1000 train_loss:5.5830 train_time:76402ms step_avg:232.93ms torch.cuda.memory_allocated()=369551872
16:28:03.201: step:339/1000 train_loss:5.6740 train_time:76644ms step_avg:232.96ms torch.cuda.memory_allocated()=369551872
16:28:03.440: step:340/1000 train_loss:5.5579 train_time:76883ms step_avg:232.98ms torch.cuda.memory_allocated()=369551872
16:28:03.681: step:341/1000 train_loss:5.3993 train_time:77124ms step_avg:233.00ms torch.cuda.memory_allocated()=369551872
16:28:03.924: step:342/1000 train_loss:5.7639 train_time:77368ms step_avg:233.04ms torch.cuda.memory_allocated()=369551872
16:28:04.165: step:343/1000 train_loss:5.4901 train_time:77608ms step_avg:233.06ms torch.cuda.memory_allocated()=369551872
16:28:04.403: step:344/1000 train_loss:5.6999 train_time:77846ms step_avg:233.07ms torch.cuda.memory_allocated()=369551872
16:28:04.644: step:345/1000 train_loss:5.6230 train_time:78087ms step_avg:233.09ms torch.cuda.memory_allocated()=369551872
16:28:04.886: step:346/1000 train_loss:5.3716 train_time:78329ms step_avg:233.12ms torch.cuda.memory_allocated()=369551872
16:28:05.125: step:347/1000 train_loss:5.5472 train_time:78568ms step_avg:233.14ms torch.cuda.memory_allocated()=369551872
16:28:05.366: step:348/1000 train_loss:5.4959 train_time:78809ms step_avg:233.16ms torch.cuda.memory_allocated()=369551872
16:28:05.607: step:349/1000 train_loss:6.2979 train_time:79050ms step_avg:233.19ms torch.cuda.memory_allocated()=369551872
16:28:05.846: step:350/1000 train_loss:5.5270 train_time:79289ms step_avg:233.20ms torch.cuda.memory_allocated()=369551872
16:28:06.086: step:351/1000 train_loss:5.6642 train_time:79530ms step_avg:233.22ms torch.cuda.memory_allocated()=369551872
16:28:06.327: step:352/1000 train_loss:5.4617 train_time:79770ms step_avg:233.25ms torch.cuda.memory_allocated()=369551872
16:28:06.566: step:353/1000 train_loss:5.5508 train_time:80009ms step_avg:233.26ms torch.cuda.memory_allocated()=369551872
16:28:06.809: step:354/1000 train_loss:5.7175 train_time:80252ms step_avg:233.29ms torch.cuda.memory_allocated()=369551872
16:28:07.050: step:355/1000 train_loss:5.4881 train_time:80493ms step_avg:233.31ms torch.cuda.memory_allocated()=369551872
16:28:07.293: step:356/1000 train_loss:5.5134 train_time:80737ms step_avg:233.34ms torch.cuda.memory_allocated()=369551872
16:28:07.534: step:357/1000 train_loss:5.6329 train_time:80977ms step_avg:233.36ms torch.cuda.memory_allocated()=369551872
16:28:07.775: step:358/1000 train_loss:5.3086 train_time:81218ms step_avg:233.39ms torch.cuda.memory_allocated()=369551872
16:28:08.015: step:359/1000 train_loss:5.7641 train_time:81458ms step_avg:233.40ms torch.cuda.memory_allocated()=369551872
16:28:08.255: step:360/1000 train_loss:5.6180 train_time:81698ms step_avg:233.42ms torch.cuda.memory_allocated()=369551872
16:28:08.495: step:361/1000 train_loss:5.6546 train_time:81938ms step_avg:233.44ms torch.cuda.memory_allocated()=369551872
16:28:08.736: step:362/1000 train_loss:5.6757 train_time:82179ms step_avg:233.46ms torch.cuda.memory_allocated()=369551872
16:28:08.976: step:363/1000 train_loss:5.5653 train_time:82419ms step_avg:233.48ms torch.cuda.memory_allocated()=369551872
16:28:09.218: step:364/1000 train_loss:5.3795 train_time:82662ms step_avg:233.51ms torch.cuda.memory_allocated()=369551872
16:28:09.459: step:365/1000 train_loss:5.4934 train_time:82903ms step_avg:233.53ms torch.cuda.memory_allocated()=369551872
16:28:09.701: step:366/1000 train_loss:5.5519 train_time:83144ms step_avg:233.55ms torch.cuda.memory_allocated()=369551872
16:28:09.943: step:367/1000 train_loss:5.2614 train_time:83386ms step_avg:233.57ms torch.cuda.memory_allocated()=369551872
16:28:10.184: step:368/1000 train_loss:5.9497 train_time:83628ms step_avg:233.60ms torch.cuda.memory_allocated()=369551872
16:28:10.426: step:369/1000 train_loss:5.5067 train_time:83869ms step_avg:233.62ms torch.cuda.memory_allocated()=369551872
16:28:10.669: step:370/1000 train_loss:5.4621 train_time:84112ms step_avg:233.64ms torch.cuda.memory_allocated()=369551872
16:28:10.911: step:371/1000 train_loss:5.6729 train_time:84354ms step_avg:233.67ms torch.cuda.memory_allocated()=369551872
16:28:11.154: step:372/1000 train_loss:5.3984 train_time:84598ms step_avg:233.70ms torch.cuda.memory_allocated()=369551872
16:28:11.397: step:373/1000 train_loss:5.6578 train_time:84840ms step_avg:233.72ms torch.cuda.memory_allocated()=369551872
16:28:11.642: step:374/1000 train_loss:5.6741 train_time:85085ms step_avg:233.75ms torch.cuda.memory_allocated()=369551872
16:28:11.885: step:375/1000 train_loss:5.8698 train_time:85328ms step_avg:233.78ms torch.cuda.memory_allocated()=369551872
16:28:14.649: step:375/1000 val_loss:5.6886 train_time:85329ms step_avg:233.78ms
16:28:14.897: step:376/1000 train_loss:5.8760 train_time:85577ms step_avg:233.82ms torch.cuda.memory_allocated()=369551872
16:28:15.145: step:377/1000 train_loss:5.5015 train_time:85825ms step_avg:233.86ms torch.cuda.memory_allocated()=369551872
16:28:15.388: step:378/1000 train_loss:5.7548 train_time:86067ms step_avg:233.88ms torch.cuda.memory_allocated()=369551872
16:28:15.632: step:379/1000 train_loss:5.6349 train_time:86312ms step_avg:233.91ms torch.cuda.memory_allocated()=369551872
16:28:15.878: step:380/1000 train_loss:5.4313 train_time:86558ms step_avg:233.94ms torch.cuda.memory_allocated()=369551872
16:28:16.124: step:381/1000 train_loss:5.3214 train_time:86804ms step_avg:233.97ms torch.cuda.memory_allocated()=369551872
16:28:16.370: step:382/1000 train_loss:5.6667 train_time:87050ms step_avg:234.00ms torch.cuda.memory_allocated()=369551872
16:28:16.614: step:383/1000 train_loss:5.7015 train_time:87294ms step_avg:234.03ms torch.cuda.memory_allocated()=369551872
16:28:16.862: step:384/1000 train_loss:5.4648 train_time:87541ms step_avg:234.07ms torch.cuda.memory_allocated()=369551872
16:28:17.107: step:385/1000 train_loss:5.4756 train_time:87787ms step_avg:234.10ms torch.cuda.memory_allocated()=369551872
16:28:17.348: step:386/1000 train_loss:5.4484 train_time:88028ms step_avg:234.12ms torch.cuda.memory_allocated()=369551872
16:28:17.592: step:387/1000 train_loss:5.5472 train_time:88272ms step_avg:234.14ms torch.cuda.memory_allocated()=369551872
16:28:17.836: step:388/1000 train_loss:5.5220 train_time:88515ms step_avg:234.17ms torch.cuda.memory_allocated()=369551872
16:28:18.077: step:389/1000 train_loss:5.4429 train_time:88757ms step_avg:234.19ms torch.cuda.memory_allocated()=369551872
16:28:18.323: step:390/1000 train_loss:5.6611 train_time:89003ms step_avg:234.22ms torch.cuda.memory_allocated()=369551872
16:28:18.566: step:391/1000 train_loss:5.5108 train_time:89246ms step_avg:234.24ms torch.cuda.memory_allocated()=369551872
16:28:18.809: step:392/1000 train_loss:5.5448 train_time:89489ms step_avg:234.27ms torch.cuda.memory_allocated()=369551872
16:28:19.054: step:393/1000 train_loss:5.6698 train_time:89733ms step_avg:234.29ms torch.cuda.memory_allocated()=369551872
16:28:19.301: step:394/1000 train_loss:5.9007 train_time:89981ms step_avg:234.33ms torch.cuda.memory_allocated()=369551872
16:28:19.545: step:395/1000 train_loss:5.6531 train_time:90225ms step_avg:234.35ms torch.cuda.memory_allocated()=369551872
16:28:19.787: step:396/1000 train_loss:5.5463 train_time:90466ms step_avg:234.37ms torch.cuda.memory_allocated()=369551872
16:28:20.035: step:397/1000 train_loss:5.6660 train_time:90715ms step_avg:234.40ms torch.cuda.memory_allocated()=369551872
16:28:20.287: step:398/1000 train_loss:5.4646 train_time:90967ms step_avg:234.45ms torch.cuda.memory_allocated()=369551872
16:28:20.531: step:399/1000 train_loss:5.8400 train_time:91211ms step_avg:234.48ms torch.cuda.memory_allocated()=369551872
16:28:20.775: step:400/1000 train_loss:5.4768 train_time:91455ms step_avg:234.50ms torch.cuda.memory_allocated()=369551872
16:28:21.020: step:401/1000 train_loss:5.5290 train_time:91700ms step_avg:234.53ms torch.cuda.memory_allocated()=369551872
16:28:21.262: step:402/1000 train_loss:5.5604 train_time:91941ms step_avg:234.54ms torch.cuda.memory_allocated()=369551872
16:28:21.506: step:403/1000 train_loss:5.6271 train_time:92186ms step_avg:234.57ms torch.cuda.memory_allocated()=369551872
16:28:21.749: step:404/1000 train_loss:5.4738 train_time:92429ms step_avg:234.59ms torch.cuda.memory_allocated()=369551872
16:28:21.996: step:405/1000 train_loss:5.4756 train_time:92675ms step_avg:234.62ms torch.cuda.memory_allocated()=369551872
16:28:22.240: step:406/1000 train_loss:5.3500 train_time:92920ms step_avg:234.65ms torch.cuda.memory_allocated()=369551872
16:28:22.483: step:407/1000 train_loss:5.6197 train_time:93163ms step_avg:234.67ms torch.cuda.memory_allocated()=369551872
16:28:22.725: step:408/1000 train_loss:5.3343 train_time:93405ms step_avg:234.69ms torch.cuda.memory_allocated()=369551872
16:28:22.968: step:409/1000 train_loss:5.8060 train_time:93647ms step_avg:234.71ms torch.cuda.memory_allocated()=369551872
16:28:23.212: step:410/1000 train_loss:5.5291 train_time:93892ms step_avg:234.73ms torch.cuda.memory_allocated()=369551872
16:28:23.455: step:411/1000 train_loss:5.5267 train_time:94134ms step_avg:234.75ms torch.cuda.memory_allocated()=369551872
16:28:23.700: step:412/1000 train_loss:5.7294 train_time:94380ms step_avg:234.78ms torch.cuda.memory_allocated()=369551872
16:28:23.943: step:413/1000 train_loss:5.5923 train_time:94622ms step_avg:234.79ms torch.cuda.memory_allocated()=369551872
16:28:24.188: step:414/1000 train_loss:5.3129 train_time:94868ms step_avg:234.82ms torch.cuda.memory_allocated()=369551872
16:28:24.435: step:415/1000 train_loss:5.4897 train_time:95115ms step_avg:234.85ms torch.cuda.memory_allocated()=369551872
16:28:24.678: step:416/1000 train_loss:5.4640 train_time:95358ms step_avg:234.87ms torch.cuda.memory_allocated()=369551872
16:28:24.924: step:417/1000 train_loss:5.3419 train_time:95604ms step_avg:234.90ms torch.cuda.memory_allocated()=369551872
16:28:25.167: step:418/1000 train_loss:5.4925 train_time:95847ms step_avg:234.92ms torch.cuda.memory_allocated()=369551872
16:28:25.410: step:419/1000 train_loss:5.6478 train_time:96090ms step_avg:234.94ms torch.cuda.memory_allocated()=369551872
16:28:25.654: step:420/1000 train_loss:5.4873 train_time:96334ms step_avg:234.96ms torch.cuda.memory_allocated()=369551872
16:28:25.900: step:421/1000 train_loss:5.5908 train_time:96579ms step_avg:234.99ms torch.cuda.memory_allocated()=369551872
16:28:26.141: step:422/1000 train_loss:5.6043 train_time:96821ms step_avg:235.00ms torch.cuda.memory_allocated()=369551872
16:28:26.387: step:423/1000 train_loss:5.4655 train_time:97067ms step_avg:235.03ms torch.cuda.memory_allocated()=369551872
16:28:26.628: step:424/1000 train_loss:5.4625 train_time:97308ms step_avg:235.04ms torch.cuda.memory_allocated()=369551872
16:28:26.873: step:425/1000 train_loss:5.1145 train_time:97553ms step_avg:235.07ms torch.cuda.memory_allocated()=369551872
16:28:27.119: step:426/1000 train_loss:5.2921 train_time:97798ms step_avg:235.09ms torch.cuda.memory_allocated()=369551872
16:28:27.366: step:427/1000 train_loss:5.3418 train_time:98045ms step_avg:235.12ms torch.cuda.memory_allocated()=369551872
16:28:27.614: step:428/1000 train_loss:5.3056 train_time:98294ms step_avg:235.15ms torch.cuda.memory_allocated()=369551872
16:28:27.858: step:429/1000 train_loss:5.7366 train_time:98538ms step_avg:235.17ms torch.cuda.memory_allocated()=369551872
16:28:28.104: step:430/1000 train_loss:6.0857 train_time:98783ms step_avg:235.20ms torch.cuda.memory_allocated()=369551872
16:28:28.351: step:431/1000 train_loss:5.5314 train_time:99030ms step_avg:235.23ms torch.cuda.memory_allocated()=369551872
16:28:28.594: step:432/1000 train_loss:5.3796 train_time:99274ms step_avg:235.25ms torch.cuda.memory_allocated()=369551872
16:28:28.837: step:433/1000 train_loss:5.5078 train_time:99517ms step_avg:235.26ms torch.cuda.memory_allocated()=369551872
16:28:29.081: step:434/1000 train_loss:5.6979 train_time:99761ms step_avg:235.28ms torch.cuda.memory_allocated()=369551872
16:28:29.327: step:435/1000 train_loss:5.3387 train_time:100007ms step_avg:235.31ms torch.cuda.memory_allocated()=369551872
16:28:29.572: step:436/1000 train_loss:5.2795 train_time:100252ms step_avg:235.33ms torch.cuda.memory_allocated()=369551872
16:28:29.816: step:437/1000 train_loss:5.3176 train_time:100496ms step_avg:235.35ms torch.cuda.memory_allocated()=369551872
16:28:30.057: step:438/1000 train_loss:5.3247 train_time:100737ms step_avg:235.37ms torch.cuda.memory_allocated()=369551872
16:28:30.302: step:439/1000 train_loss:5.0863 train_time:100982ms step_avg:235.39ms torch.cuda.memory_allocated()=369551872
16:28:30.548: step:440/1000 train_loss:5.1820 train_time:101228ms step_avg:235.41ms torch.cuda.memory_allocated()=369551872
16:28:30.793: step:441/1000 train_loss:5.4490 train_time:101473ms step_avg:235.44ms torch.cuda.memory_allocated()=369551872
16:28:31.038: step:442/1000 train_loss:5.3735 train_time:101717ms step_avg:235.46ms torch.cuda.memory_allocated()=369551872
16:28:31.280: step:443/1000 train_loss:5.5057 train_time:101960ms step_avg:235.47ms torch.cuda.memory_allocated()=369551872
16:28:31.529: step:444/1000 train_loss:5.2688 train_time:102208ms step_avg:235.50ms torch.cuda.memory_allocated()=369551872
16:28:31.773: step:445/1000 train_loss:5.5474 train_time:102453ms step_avg:235.52ms torch.cuda.memory_allocated()=369551872
16:28:32.019: step:446/1000 train_loss:5.4567 train_time:102699ms step_avg:235.55ms torch.cuda.memory_allocated()=369551872
16:28:32.265: step:447/1000 train_loss:5.4931 train_time:102945ms step_avg:235.57ms torch.cuda.memory_allocated()=369551872
16:28:32.512: step:448/1000 train_loss:5.5087 train_time:103192ms step_avg:235.60ms torch.cuda.memory_allocated()=369551872
16:28:32.758: step:449/1000 train_loss:5.3192 train_time:103437ms step_avg:235.62ms torch.cuda.memory_allocated()=369551872
16:28:33.005: step:450/1000 train_loss:5.4469 train_time:103684ms step_avg:235.65ms torch.cuda.memory_allocated()=369551872
16:28:33.256: step:451/1000 train_loss:5.4412 train_time:103936ms step_avg:235.68ms torch.cuda.memory_allocated()=369551872
16:28:33.508: step:452/1000 train_loss:5.5038 train_time:104188ms step_avg:235.72ms torch.cuda.memory_allocated()=369551872
16:28:33.754: step:453/1000 train_loss:5.2787 train_time:104434ms step_avg:235.74ms torch.cuda.memory_allocated()=369551872
16:28:33.999: step:454/1000 train_loss:5.4942 train_time:104679ms step_avg:235.76ms torch.cuda.memory_allocated()=369551872
16:28:34.249: step:455/1000 train_loss:5.4764 train_time:104929ms step_avg:235.80ms torch.cuda.memory_allocated()=369551872
16:28:34.500: step:456/1000 train_loss:5.6247 train_time:105179ms step_avg:235.83ms torch.cuda.memory_allocated()=369551872
16:28:34.748: step:457/1000 train_loss:5.4870 train_time:105427ms step_avg:235.86ms torch.cuda.memory_allocated()=369551872
16:28:34.993: step:458/1000 train_loss:5.4167 train_time:105673ms step_avg:235.88ms torch.cuda.memory_allocated()=369551872
16:28:35.240: step:459/1000 train_loss:5.3704 train_time:105920ms step_avg:235.90ms torch.cuda.memory_allocated()=369551872
16:28:35.488: step:460/1000 train_loss:5.4446 train_time:106168ms step_avg:235.93ms torch.cuda.memory_allocated()=369551872
16:28:35.731: step:461/1000 train_loss:5.3077 train_time:106411ms step_avg:235.94ms torch.cuda.memory_allocated()=369551872
16:28:35.981: step:462/1000 train_loss:5.3974 train_time:106661ms step_avg:235.98ms torch.cuda.memory_allocated()=369551872
16:28:36.232: step:463/1000 train_loss:5.7119 train_time:106912ms step_avg:236.01ms torch.cuda.memory_allocated()=369551872
16:28:36.477: step:464/1000 train_loss:5.6112 train_time:107157ms step_avg:236.03ms torch.cuda.memory_allocated()=369551872
16:28:36.722: step:465/1000 train_loss:5.4676 train_time:107402ms step_avg:236.05ms torch.cuda.memory_allocated()=369551872
16:28:36.967: step:466/1000 train_loss:5.6399 train_time:107647ms step_avg:236.07ms torch.cuda.memory_allocated()=369551872
16:28:37.213: step:467/1000 train_loss:5.4214 train_time:107893ms step_avg:236.09ms torch.cuda.memory_allocated()=369551872
16:28:37.458: step:468/1000 train_loss:5.3083 train_time:108138ms step_avg:236.11ms torch.cuda.memory_allocated()=369551872
16:28:37.703: step:469/1000 train_loss:5.5170 train_time:108383ms step_avg:236.13ms torch.cuda.memory_allocated()=369551872
16:28:37.951: step:470/1000 train_loss:5.2313 train_time:108630ms step_avg:236.15ms torch.cuda.memory_allocated()=369551872
16:28:38.195: step:471/1000 train_loss:5.3420 train_time:108874ms step_avg:236.17ms torch.cuda.memory_allocated()=369551872
16:28:38.437: step:472/1000 train_loss:5.5403 train_time:109117ms step_avg:236.18ms torch.cuda.memory_allocated()=369551872
16:28:38.682: step:473/1000 train_loss:5.3305 train_time:109362ms step_avg:236.20ms torch.cuda.memory_allocated()=369551872
16:28:38.929: step:474/1000 train_loss:5.6347 train_time:109609ms step_avg:236.23ms torch.cuda.memory_allocated()=369551872
16:28:39.180: step:475/1000 train_loss:5.6568 train_time:109860ms step_avg:236.26ms torch.cuda.memory_allocated()=369551872
16:28:39.425: step:476/1000 train_loss:5.6205 train_time:110105ms step_avg:236.28ms torch.cuda.memory_allocated()=369551872
16:28:39.669: step:477/1000 train_loss:5.2201 train_time:110349ms step_avg:236.29ms torch.cuda.memory_allocated()=369551872
16:28:39.918: step:478/1000 train_loss:5.3935 train_time:110598ms step_avg:236.32ms torch.cuda.memory_allocated()=369551872
16:28:40.162: step:479/1000 train_loss:5.8182 train_time:110842ms step_avg:236.34ms torch.cuda.memory_allocated()=369551872
16:28:40.407: step:480/1000 train_loss:5.2788 train_time:111087ms step_avg:236.35ms torch.cuda.memory_allocated()=369551872
16:28:40.649: step:481/1000 train_loss:5.6506 train_time:111329ms step_avg:236.37ms torch.cuda.memory_allocated()=369551872
16:28:40.897: step:482/1000 train_loss:5.4265 train_time:111577ms step_avg:236.39ms torch.cuda.memory_allocated()=369551872
16:28:41.142: step:483/1000 train_loss:5.2703 train_time:111821ms step_avg:236.41ms torch.cuda.memory_allocated()=369551872
16:28:41.391: step:484/1000 train_loss:5.4675 train_time:112071ms step_avg:236.44ms torch.cuda.memory_allocated()=369551872
16:28:41.634: step:485/1000 train_loss:5.5275 train_time:112314ms step_avg:236.45ms torch.cuda.memory_allocated()=369551872
16:28:41.882: step:486/1000 train_loss:5.4605 train_time:112561ms step_avg:236.47ms torch.cuda.memory_allocated()=369551872
16:28:42.130: step:487/1000 train_loss:5.4857 train_time:112809ms step_avg:236.50ms torch.cuda.memory_allocated()=369551872
16:28:42.377: step:488/1000 train_loss:5.3393 train_time:113056ms step_avg:236.52ms torch.cuda.memory_allocated()=369551872
16:28:42.620: step:489/1000 train_loss:5.2900 train_time:113300ms step_avg:236.53ms torch.cuda.memory_allocated()=369551872
16:28:42.867: step:490/1000 train_loss:5.4866 train_time:113547ms step_avg:236.56ms torch.cuda.memory_allocated()=369551872
16:28:43.113: step:491/1000 train_loss:5.2092 train_time:113793ms step_avg:236.58ms torch.cuda.memory_allocated()=369551872
16:28:43.361: step:492/1000 train_loss:5.2678 train_time:114041ms step_avg:236.60ms torch.cuda.memory_allocated()=369551872
16:28:43.607: step:493/1000 train_loss:5.1612 train_time:114287ms step_avg:236.62ms torch.cuda.memory_allocated()=369551872
16:28:43.852: step:494/1000 train_loss:5.8318 train_time:114532ms step_avg:236.64ms torch.cuda.memory_allocated()=369551872
16:28:44.105: step:495/1000 train_loss:4.9965 train_time:114785ms step_avg:236.67ms torch.cuda.memory_allocated()=369551872
16:28:44.349: step:496/1000 train_loss:5.1966 train_time:115029ms step_avg:236.69ms torch.cuda.memory_allocated()=369551872
16:28:44.596: step:497/1000 train_loss:5.2006 train_time:115276ms step_avg:236.71ms torch.cuda.memory_allocated()=369551872
16:28:44.841: step:498/1000 train_loss:5.2704 train_time:115520ms step_avg:236.72ms torch.cuda.memory_allocated()=369551872
16:28:45.090: step:499/1000 train_loss:5.4521 train_time:115770ms step_avg:236.75ms torch.cuda.memory_allocated()=369551872
16:28:45.335: step:500/1000 train_loss:5.1194 train_time:116015ms step_avg:236.77ms torch.cuda.memory_allocated()=369551872
16:28:48.121: step:500/1000 val_loss:5.5300 train_time:116016ms step_avg:236.77ms
16:28:48.372: step:501/1000 train_loss:4.9583 train_time:116265ms step_avg:236.79ms torch.cuda.memory_allocated()=369551872
16:28:48.620: step:502/1000 train_loss:5.4221 train_time:116514ms step_avg:236.82ms torch.cuda.memory_allocated()=369551872
16:28:48.866: step:503/1000 train_loss:5.3696 train_time:116760ms step_avg:236.84ms torch.cuda.memory_allocated()=369551872
16:28:49.109: step:504/1000 train_loss:5.3812 train_time:117003ms step_avg:236.85ms torch.cuda.memory_allocated()=369551872
16:28:49.355: step:505/1000 train_loss:5.5770 train_time:117249ms step_avg:236.87ms torch.cuda.memory_allocated()=369551872
16:28:49.601: step:506/1000 train_loss:5.5050 train_time:117495ms step_avg:236.88ms torch.cuda.memory_allocated()=369551872
16:28:49.853: step:507/1000 train_loss:5.6113 train_time:117747ms step_avg:236.92ms torch.cuda.memory_allocated()=369551872
16:28:50.098: step:508/1000 train_loss:5.3715 train_time:117992ms step_avg:236.93ms torch.cuda.memory_allocated()=369551872
16:28:50.343: step:509/1000 train_loss:5.3038 train_time:118237ms step_avg:236.95ms torch.cuda.memory_allocated()=369551872
16:28:50.594: step:510/1000 train_loss:5.0911 train_time:118488ms step_avg:236.98ms torch.cuda.memory_allocated()=369551872
16:28:50.843: step:511/1000 train_loss:5.6073 train_time:118737ms step_avg:237.00ms torch.cuda.memory_allocated()=369551872
16:28:51.092: step:512/1000 train_loss:5.4479 train_time:118986ms step_avg:237.02ms torch.cuda.memory_allocated()=369551872
16:28:51.344: step:513/1000 train_loss:5.3924 train_time:119237ms step_avg:237.05ms torch.cuda.memory_allocated()=369551872
16:28:51.590: step:514/1000 train_loss:5.7171 train_time:119484ms step_avg:237.07ms torch.cuda.memory_allocated()=369551872
16:28:51.836: step:515/1000 train_loss:5.4477 train_time:119730ms step_avg:237.09ms torch.cuda.memory_allocated()=369551872
16:28:52.086: step:516/1000 train_loss:5.5117 train_time:119979ms step_avg:237.11ms torch.cuda.memory_allocated()=369551872
16:28:52.330: step:517/1000 train_loss:5.2796 train_time:120224ms step_avg:237.13ms torch.cuda.memory_allocated()=369551872
16:28:52.574: step:518/1000 train_loss:5.2545 train_time:120467ms step_avg:237.14ms torch.cuda.memory_allocated()=369551872
16:28:52.821: step:519/1000 train_loss:5.4942 train_time:120714ms step_avg:237.16ms torch.cuda.memory_allocated()=369551872
16:28:53.071: step:520/1000 train_loss:5.3370 train_time:120965ms step_avg:237.19ms torch.cuda.memory_allocated()=369551872
16:28:53.324: step:521/1000 train_loss:4.7768 train_time:121218ms step_avg:237.22ms torch.cuda.memory_allocated()=369551872
16:28:53.575: step:522/1000 train_loss:5.2037 train_time:121469ms step_avg:237.24ms torch.cuda.memory_allocated()=369551872
16:28:53.826: step:523/1000 train_loss:5.3260 train_time:121720ms step_avg:237.27ms torch.cuda.memory_allocated()=369551872
16:28:54.079: step:524/1000 train_loss:5.5960 train_time:121973ms step_avg:237.30ms torch.cuda.memory_allocated()=369551872
16:28:54.326: step:525/1000 train_loss:5.1835 train_time:122220ms step_avg:237.32ms torch.cuda.memory_allocated()=369551872
16:28:54.572: step:526/1000 train_loss:5.2689 train_time:122466ms step_avg:237.34ms torch.cuda.memory_allocated()=369551872
16:28:54.820: step:527/1000 train_loss:5.5739 train_time:122714ms step_avg:237.36ms torch.cuda.memory_allocated()=369551872
16:28:55.077: step:528/1000 train_loss:5.3745 train_time:122971ms step_avg:237.40ms torch.cuda.memory_allocated()=369551872
16:28:55.324: step:529/1000 train_loss:5.1300 train_time:123218ms step_avg:237.41ms torch.cuda.memory_allocated()=369551872
16:28:55.570: step:530/1000 train_loss:5.3113 train_time:123464ms step_avg:237.43ms torch.cuda.memory_allocated()=369551872
16:28:55.839: step:531/1000 train_loss:5.1739 train_time:123733ms step_avg:237.49ms torch.cuda.memory_allocated()=369551872
16:28:56.113: step:532/1000 train_loss:5.4034 train_time:124007ms step_avg:237.56ms torch.cuda.memory_allocated()=369551872
16:28:56.395: step:533/1000 train_loss:5.6028 train_time:124289ms step_avg:237.65ms torch.cuda.memory_allocated()=369551872
16:28:56.639: step:534/1000 train_loss:5.4537 train_time:124533ms step_avg:237.66ms torch.cuda.memory_allocated()=369551872
16:28:56.890: step:535/1000 train_loss:5.7352 train_time:124784ms step_avg:237.68ms torch.cuda.memory_allocated()=369551872
16:28:57.137: step:536/1000 train_loss:5.6279 train_time:125031ms step_avg:237.70ms torch.cuda.memory_allocated()=369551872
16:28:57.394: step:537/1000 train_loss:5.1807 train_time:125287ms step_avg:237.74ms torch.cuda.memory_allocated()=369551872
16:28:57.640: step:538/1000 train_loss:5.1652 train_time:125534ms step_avg:237.75ms torch.cuda.memory_allocated()=369551872
16:28:57.891: step:539/1000 train_loss:5.9804 train_time:125785ms step_avg:237.78ms torch.cuda.memory_allocated()=369551872
16:28:58.146: step:540/1000 train_loss:5.4266 train_time:126039ms step_avg:237.81ms torch.cuda.memory_allocated()=369551872
16:28:58.395: step:541/1000 train_loss:5.2130 train_time:126288ms step_avg:237.83ms torch.cuda.memory_allocated()=369551872
16:28:58.645: step:542/1000 train_loss:5.3353 train_time:126539ms step_avg:237.86ms torch.cuda.memory_allocated()=369551872
16:28:58.915: step:543/1000 train_loss:5.9527 train_time:126809ms step_avg:237.92ms torch.cuda.memory_allocated()=369551872
16:28:59.212: step:544/1000 train_loss:5.1797 train_time:127106ms step_avg:238.03ms torch.cuda.memory_allocated()=369551872
16:28:59.474: step:545/1000 train_loss:5.4071 train_time:127367ms step_avg:238.07ms torch.cuda.memory_allocated()=369551872
16:28:59.750: step:546/1000 train_loss:5.5028 train_time:127643ms step_avg:238.14ms torch.cuda.memory_allocated()=369551872
16:29:00.018: step:547/1000 train_loss:5.3400 train_time:127911ms step_avg:238.20ms torch.cuda.memory_allocated()=369551872
16:29:00.268: step:548/1000 train_loss:5.1355 train_time:128162ms step_avg:238.22ms torch.cuda.memory_allocated()=369551872
16:29:00.521: step:549/1000 train_loss:5.4286 train_time:128415ms step_avg:238.25ms torch.cuda.memory_allocated()=369551872
16:29:00.771: step:550/1000 train_loss:5.0912 train_time:128665ms step_avg:238.27ms torch.cuda.memory_allocated()=369551872
16:29:01.022: step:551/1000 train_loss:5.6440 train_time:128916ms step_avg:238.29ms torch.cuda.memory_allocated()=369551872
16:29:01.273: step:552/1000 train_loss:5.3466 train_time:129166ms step_avg:238.31ms torch.cuda.memory_allocated()=369551872
16:29:01.533: step:553/1000 train_loss:5.5675 train_time:129427ms step_avg:238.36ms torch.cuda.memory_allocated()=369551872
16:29:01.794: step:554/1000 train_loss:5.1516 train_time:129687ms step_avg:238.40ms torch.cuda.memory_allocated()=369551872
16:29:02.043: step:555/1000 train_loss:5.5696 train_time:129937ms step_avg:238.42ms torch.cuda.memory_allocated()=369551872
16:29:02.294: step:556/1000 train_loss:5.4178 train_time:130188ms step_avg:238.44ms torch.cuda.memory_allocated()=369551872
16:29:02.542: step:557/1000 train_loss:5.4378 train_time:130435ms step_avg:238.46ms torch.cuda.memory_allocated()=369551872
16:29:02.795: step:558/1000 train_loss:5.6089 train_time:130689ms step_avg:238.48ms torch.cuda.memory_allocated()=369551872
16:29:03.050: step:559/1000 train_loss:5.8344 train_time:130943ms step_avg:238.51ms torch.cuda.memory_allocated()=369551872
16:29:03.298: step:560/1000 train_loss:5.3815 train_time:131191ms step_avg:238.53ms torch.cuda.memory_allocated()=369551872
16:29:03.548: step:561/1000 train_loss:5.5187 train_time:131441ms step_avg:238.55ms torch.cuda.memory_allocated()=369551872
16:29:03.799: step:562/1000 train_loss:5.6147 train_time:131693ms step_avg:238.57ms torch.cuda.memory_allocated()=369551872
16:29:04.058: step:563/1000 train_loss:6.0197 train_time:131952ms step_avg:238.61ms torch.cuda.memory_allocated()=369551872
16:29:04.320: step:564/1000 train_loss:6.0116 train_time:132214ms step_avg:238.65ms torch.cuda.memory_allocated()=369551872
16:29:04.569: step:565/1000 train_loss:5.3160 train_time:132463ms step_avg:238.67ms torch.cuda.memory_allocated()=369551872
16:29:04.820: step:566/1000 train_loss:5.5707 train_time:132713ms step_avg:238.69ms torch.cuda.memory_allocated()=369551872
16:29:05.069: step:567/1000 train_loss:5.2329 train_time:132963ms step_avg:238.71ms torch.cuda.memory_allocated()=369551872
16:29:05.324: step:568/1000 train_loss:5.1325 train_time:133218ms step_avg:238.74ms torch.cuda.memory_allocated()=369551872
16:29:05.574: step:569/1000 train_loss:5.7627 train_time:133467ms step_avg:238.76ms torch.cuda.memory_allocated()=369551872
16:29:05.821: step:570/1000 train_loss:5.2005 train_time:133714ms step_avg:238.78ms torch.cuda.memory_allocated()=369551872
16:29:06.068: step:571/1000 train_loss:5.3432 train_time:133962ms step_avg:238.79ms torch.cuda.memory_allocated()=369551872
16:29:06.322: step:572/1000 train_loss:5.3626 train_time:134215ms step_avg:238.82ms torch.cuda.memory_allocated()=369551872
16:29:06.573: step:573/1000 train_loss:5.3781 train_time:134467ms step_avg:238.84ms torch.cuda.memory_allocated()=369551872
16:29:06.819: step:574/1000 train_loss:5.3355 train_time:134713ms step_avg:238.85ms torch.cuda.memory_allocated()=369551872
16:29:07.065: step:575/1000 train_loss:5.4495 train_time:134959ms step_avg:238.86ms torch.cuda.memory_allocated()=369551872
16:29:07.316: step:576/1000 train_loss:5.5443 train_time:135209ms step_avg:238.89ms torch.cuda.memory_allocated()=369551872
16:29:07.563: step:577/1000 train_loss:5.3049 train_time:135457ms step_avg:238.90ms torch.cuda.memory_allocated()=369551872
16:29:07.810: step:578/1000 train_loss:5.6979 train_time:135704ms step_avg:238.91ms torch.cuda.memory_allocated()=369551872
16:29:08.053: step:579/1000 train_loss:5.4076 train_time:135947ms step_avg:238.92ms torch.cuda.memory_allocated()=369551872
16:29:08.307: step:580/1000 train_loss:5.0785 train_time:136201ms step_avg:238.95ms torch.cuda.memory_allocated()=369551872
16:29:08.562: step:581/1000 train_loss:5.4081 train_time:136455ms step_avg:238.98ms torch.cuda.memory_allocated()=369551872
16:29:08.809: step:582/1000 train_loss:5.6250 train_time:136703ms step_avg:238.99ms torch.cuda.memory_allocated()=369551872
16:29:09.062: step:583/1000 train_loss:5.1983 train_time:136955ms step_avg:239.01ms torch.cuda.memory_allocated()=369551872
16:29:09.310: step:584/1000 train_loss:5.5520 train_time:137204ms step_avg:239.03ms torch.cuda.memory_allocated()=369551872
16:29:09.555: step:585/1000 train_loss:5.4706 train_time:137448ms step_avg:239.04ms torch.cuda.memory_allocated()=369551872
16:29:09.814: step:586/1000 train_loss:5.6231 train_time:137707ms step_avg:239.08ms torch.cuda.memory_allocated()=369551872
16:29:10.077: step:587/1000 train_loss:5.4388 train_time:137971ms step_avg:239.12ms torch.cuda.memory_allocated()=369551872
16:29:10.359: step:588/1000 train_loss:5.3564 train_time:138252ms step_avg:239.19ms torch.cuda.memory_allocated()=369551872
16:29:10.624: step:589/1000 train_loss:5.7276 train_time:138518ms step_avg:239.24ms torch.cuda.memory_allocated()=369551872
16:29:10.888: step:590/1000 train_loss:6.6660 train_time:138782ms step_avg:239.28ms torch.cuda.memory_allocated()=369551872
16:29:11.135: step:591/1000 train_loss:5.6502 train_time:139029ms step_avg:239.29ms torch.cuda.memory_allocated()=369551872
16:29:11.391: step:592/1000 train_loss:5.3555 train_time:139285ms step_avg:239.32ms torch.cuda.memory_allocated()=369551872
16:29:11.675: step:593/1000 train_loss:5.0126 train_time:139569ms step_avg:239.40ms torch.cuda.memory_allocated()=369551872
16:29:11.955: step:594/1000 train_loss:5.3499 train_time:139849ms step_avg:239.47ms torch.cuda.memory_allocated()=369551872
16:29:12.249: step:595/1000 train_loss:5.6457 train_time:140143ms step_avg:239.56ms torch.cuda.memory_allocated()=369551872
16:29:12.528: step:596/1000 train_loss:5.1691 train_time:140422ms step_avg:239.63ms torch.cuda.memory_allocated()=369551872
16:29:12.789: step:597/1000 train_loss:5.4909 train_time:140683ms step_avg:239.66ms torch.cuda.memory_allocated()=369551872
16:29:13.050: step:598/1000 train_loss:5.3377 train_time:140943ms step_avg:239.70ms torch.cuda.memory_allocated()=369551872
16:29:13.299: step:599/1000 train_loss:5.2726 train_time:141192ms step_avg:239.72ms torch.cuda.memory_allocated()=369551872
16:29:13.578: step:600/1000 train_loss:6.1372 train_time:141472ms step_avg:239.78ms torch.cuda.memory_allocated()=369551872
16:29:13.844: step:601/1000 train_loss:4.9422 train_time:141738ms step_avg:239.83ms torch.cuda.memory_allocated()=369551872
16:29:14.101: step:602/1000 train_loss:5.2392 train_time:141995ms step_avg:239.86ms torch.cuda.memory_allocated()=369551872
16:29:14.350: step:603/1000 train_loss:5.3831 train_time:142244ms step_avg:239.87ms torch.cuda.memory_allocated()=369551872
16:29:14.599: step:604/1000 train_loss:5.4760 train_time:142493ms step_avg:239.89ms torch.cuda.memory_allocated()=369551872
16:29:14.848: step:605/1000 train_loss:5.3217 train_time:142742ms step_avg:239.90ms torch.cuda.memory_allocated()=369551872
16:29:15.102: step:606/1000 train_loss:5.5085 train_time:142996ms step_avg:239.93ms torch.cuda.memory_allocated()=369551872
16:29:15.350: step:607/1000 train_loss:5.4496 train_time:143244ms step_avg:239.94ms torch.cuda.memory_allocated()=369551872
16:29:15.608: step:608/1000 train_loss:5.5893 train_time:143502ms step_avg:239.97ms torch.cuda.memory_allocated()=369551872
16:29:15.858: step:609/1000 train_loss:5.2726 train_time:143751ms step_avg:239.99ms torch.cuda.memory_allocated()=369551872
16:29:16.111: step:610/1000 train_loss:5.2215 train_time:144004ms step_avg:240.01ms torch.cuda.memory_allocated()=369551872
16:29:16.367: step:611/1000 train_loss:4.7651 train_time:144261ms step_avg:240.03ms torch.cuda.memory_allocated()=369551872
16:29:16.619: step:612/1000 train_loss:4.9946 train_time:144513ms step_avg:240.05ms torch.cuda.memory_allocated()=369551872
16:29:16.871: step:613/1000 train_loss:5.1266 train_time:144764ms step_avg:240.07ms torch.cuda.memory_allocated()=369551872
16:29:17.124: step:614/1000 train_loss:5.1996 train_time:145018ms step_avg:240.10ms torch.cuda.memory_allocated()=369551872
16:29:17.378: step:615/1000 train_loss:5.0858 train_time:145271ms step_avg:240.12ms torch.cuda.memory_allocated()=369551872
16:29:17.623: step:616/1000 train_loss:5.2321 train_time:145516ms step_avg:240.13ms torch.cuda.memory_allocated()=369551872
16:29:17.875: step:617/1000 train_loss:5.2179 train_time:145768ms step_avg:240.15ms torch.cuda.memory_allocated()=369551872
16:29:18.121: step:618/1000 train_loss:5.4624 train_time:146014ms step_avg:240.16ms torch.cuda.memory_allocated()=369551872
16:29:18.372: step:619/1000 train_loss:5.3827 train_time:146265ms step_avg:240.17ms torch.cuda.memory_allocated()=369551872
16:29:18.621: step:620/1000 train_loss:5.2379 train_time:146515ms step_avg:240.19ms torch.cuda.memory_allocated()=369551872
16:29:18.871: step:621/1000 train_loss:5.3362 train_time:146765ms step_avg:240.20ms torch.cuda.memory_allocated()=369551872
16:29:19.125: step:622/1000 train_loss:5.2632 train_time:147019ms step_avg:240.23ms torch.cuda.memory_allocated()=369551872
16:29:19.377: step:623/1000 train_loss:5.2141 train_time:147271ms step_avg:240.25ms torch.cuda.memory_allocated()=369551872
16:29:19.625: step:624/1000 train_loss:5.3960 train_time:147519ms step_avg:240.26ms torch.cuda.memory_allocated()=369551872
16:29:19.872: step:625/1000 train_loss:5.3299 train_time:147766ms step_avg:240.27ms torch.cuda.memory_allocated()=369551872
16:29:22.683: step:625/1000 val_loss:5.4233 train_time:147766ms step_avg:240.27ms
16:29:22.931: step:626/1000 train_loss:5.4542 train_time:148013ms step_avg:240.28ms torch.cuda.memory_allocated()=369551872
16:29:23.180: step:627/1000 train_loss:5.3795 train_time:148263ms step_avg:240.30ms torch.cuda.memory_allocated()=369551872
16:29:23.433: step:628/1000 train_loss:5.6710 train_time:148516ms step_avg:240.32ms torch.cuda.memory_allocated()=369551872
16:29:23.688: step:629/1000 train_loss:5.1601 train_time:148771ms step_avg:240.34ms torch.cuda.memory_allocated()=369551872
16:29:23.941: step:630/1000 train_loss:5.4398 train_time:149024ms step_avg:240.36ms torch.cuda.memory_allocated()=369551872
16:29:24.192: step:631/1000 train_loss:5.3950 train_time:149275ms step_avg:240.38ms torch.cuda.memory_allocated()=369551872
16:29:24.445: step:632/1000 train_loss:5.1197 train_time:149528ms step_avg:240.40ms torch.cuda.memory_allocated()=369551872
16:29:24.700: step:633/1000 train_loss:5.3007 train_time:149783ms step_avg:240.42ms torch.cuda.memory_allocated()=369551872
16:29:24.958: step:634/1000 train_loss:5.0910 train_time:150041ms step_avg:240.45ms torch.cuda.memory_allocated()=369551872
16:29:25.207: step:635/1000 train_loss:5.2728 train_time:150289ms step_avg:240.46ms torch.cuda.memory_allocated()=369551872
16:29:25.462: step:636/1000 train_loss:5.4039 train_time:150545ms step_avg:240.49ms torch.cuda.memory_allocated()=369551872
16:29:25.708: step:637/1000 train_loss:5.4497 train_time:150791ms step_avg:240.50ms torch.cuda.memory_allocated()=369551872
16:29:25.957: step:638/1000 train_loss:5.3832 train_time:151040ms step_avg:240.51ms torch.cuda.memory_allocated()=369551872
16:29:26.202: step:639/1000 train_loss:5.2749 train_time:151285ms step_avg:240.52ms torch.cuda.memory_allocated()=369551872
16:29:26.447: step:640/1000 train_loss:5.1501 train_time:151529ms step_avg:240.52ms torch.cuda.memory_allocated()=369551872
16:29:26.698: step:641/1000 train_loss:5.1130 train_time:151780ms step_avg:240.54ms torch.cuda.memory_allocated()=369551872
16:29:26.949: step:642/1000 train_loss:5.4110 train_time:152032ms step_avg:240.56ms torch.cuda.memory_allocated()=369551872
16:29:27.204: step:643/1000 train_loss:5.3174 train_time:152287ms step_avg:240.58ms torch.cuda.memory_allocated()=369551872
16:29:27.460: step:644/1000 train_loss:5.4283 train_time:152543ms step_avg:240.60ms torch.cuda.memory_allocated()=369551872
16:29:27.709: step:645/1000 train_loss:5.4328 train_time:152792ms step_avg:240.62ms torch.cuda.memory_allocated()=369551872
16:29:27.962: step:646/1000 train_loss:5.2439 train_time:153045ms step_avg:240.64ms torch.cuda.memory_allocated()=369551872
16:29:28.219: step:647/1000 train_loss:5.2544 train_time:153302ms step_avg:240.66ms torch.cuda.memory_allocated()=369551872
16:29:28.469: step:648/1000 train_loss:5.2481 train_time:153552ms step_avg:240.68ms torch.cuda.memory_allocated()=369551872
16:29:28.716: step:649/1000 train_loss:5.3753 train_time:153799ms step_avg:240.69ms torch.cuda.memory_allocated()=369551872
16:29:28.965: step:650/1000 train_loss:5.1954 train_time:154048ms step_avg:240.70ms torch.cuda.memory_allocated()=369551872
16:29:29.220: step:651/1000 train_loss:5.1305 train_time:154303ms step_avg:240.72ms torch.cuda.memory_allocated()=369551872
16:29:29.475: step:652/1000 train_loss:5.1788 train_time:154558ms step_avg:240.74ms torch.cuda.memory_allocated()=369551872
16:29:29.726: step:653/1000 train_loss:4.9782 train_time:154809ms step_avg:240.76ms torch.cuda.memory_allocated()=369551872
16:29:29.977: step:654/1000 train_loss:5.1208 train_time:155060ms step_avg:240.78ms torch.cuda.memory_allocated()=369551872
16:29:30.228: step:655/1000 train_loss:5.0923 train_time:155311ms step_avg:240.79ms torch.cuda.memory_allocated()=369551872
16:29:30.477: step:656/1000 train_loss:5.3352 train_time:155559ms step_avg:240.80ms torch.cuda.memory_allocated()=369551872
16:29:30.723: step:657/1000 train_loss:5.2396 train_time:155806ms step_avg:240.81ms torch.cuda.memory_allocated()=369551872
16:29:30.976: step:658/1000 train_loss:5.2170 train_time:156058ms step_avg:240.83ms torch.cuda.memory_allocated()=369551872
16:29:31.229: step:659/1000 train_loss:5.5833 train_time:156312ms step_avg:240.85ms torch.cuda.memory_allocated()=369551872
16:29:31.482: step:660/1000 train_loss:5.5200 train_time:156564ms step_avg:240.87ms torch.cuda.memory_allocated()=369551872
16:29:31.726: step:661/1000 train_loss:5.2422 train_time:156809ms step_avg:240.87ms torch.cuda.memory_allocated()=369551872
16:29:31.976: step:662/1000 train_loss:5.1072 train_time:157059ms step_avg:240.89ms torch.cuda.memory_allocated()=369551872
16:29:32.237: step:663/1000 train_loss:5.2431 train_time:157320ms step_avg:240.92ms torch.cuda.memory_allocated()=369551872
16:29:32.511: step:664/1000 train_loss:5.2209 train_time:157594ms step_avg:240.97ms torch.cuda.memory_allocated()=369551872
16:29:32.776: step:665/1000 train_loss:5.4344 train_time:157859ms step_avg:241.01ms torch.cuda.memory_allocated()=369551872
16:29:33.050: step:666/1000 train_loss:5.6917 train_time:158133ms step_avg:241.06ms torch.cuda.memory_allocated()=369551872
16:29:33.326: step:667/1000 train_loss:5.1196 train_time:158409ms step_avg:241.11ms torch.cuda.memory_allocated()=369551872
16:29:33.602: step:668/1000 train_loss:5.3858 train_time:158685ms step_avg:241.16ms torch.cuda.memory_allocated()=369551872
16:29:33.865: step:669/1000 train_loss:5.3952 train_time:158948ms step_avg:241.20ms torch.cuda.memory_allocated()=369551872
16:29:34.113: step:670/1000 train_loss:5.2282 train_time:159196ms step_avg:241.21ms torch.cuda.memory_allocated()=369551872
16:29:34.365: step:671/1000 train_loss:5.3917 train_time:159448ms step_avg:241.22ms torch.cuda.memory_allocated()=369551872
16:29:34.611: step:672/1000 train_loss:5.1835 train_time:159693ms step_avg:241.23ms torch.cuda.memory_allocated()=369551872
16:29:34.866: step:673/1000 train_loss:5.5567 train_time:159949ms step_avg:241.25ms torch.cuda.memory_allocated()=369551872
16:29:35.151: step:674/1000 train_loss:5.9596 train_time:160234ms step_avg:241.32ms torch.cuda.memory_allocated()=369551872
16:29:35.415: step:675/1000 train_loss:5.2751 train_time:160498ms step_avg:241.35ms torch.cuda.memory_allocated()=369551872
16:29:35.672: step:676/1000 train_loss:5.2721 train_time:160754ms step_avg:241.37ms torch.cuda.memory_allocated()=369551872
16:29:35.936: step:677/1000 train_loss:5.3057 train_time:161019ms step_avg:241.41ms torch.cuda.memory_allocated()=369551872
16:29:36.206: step:678/1000 train_loss:5.4677 train_time:161289ms step_avg:241.45ms torch.cuda.memory_allocated()=369551872
16:29:36.453: step:679/1000 train_loss:5.3412 train_time:161536ms step_avg:241.46ms torch.cuda.memory_allocated()=369551872
16:29:36.726: step:680/1000 train_loss:5.3290 train_time:161809ms step_avg:241.51ms torch.cuda.memory_allocated()=369551872
16:29:37.011: step:681/1000 train_loss:5.2427 train_time:162094ms step_avg:241.57ms torch.cuda.memory_allocated()=369551872
16:29:37.292: step:682/1000 train_loss:5.3010 train_time:162375ms step_avg:241.63ms torch.cuda.memory_allocated()=369551872
16:29:37.556: step:683/1000 train_loss:5.2380 train_time:162639ms step_avg:241.66ms torch.cuda.memory_allocated()=369551872
16:29:37.830: step:684/1000 train_loss:5.3189 train_time:162913ms step_avg:241.71ms torch.cuda.memory_allocated()=369551872
16:29:38.127: step:685/1000 train_loss:5.6911 train_time:163210ms step_avg:241.79ms torch.cuda.memory_allocated()=369551872
16:29:38.429: step:686/1000 train_loss:5.2337 train_time:163512ms step_avg:241.88ms torch.cuda.memory_allocated()=369551872
16:29:38.689: step:687/1000 train_loss:5.1464 train_time:163772ms step_avg:241.91ms torch.cuda.memory_allocated()=369551872
16:29:38.945: step:688/1000 train_loss:5.8891 train_time:164028ms step_avg:241.93ms torch.cuda.memory_allocated()=369551872
16:29:39.193: step:689/1000 train_loss:5.0185 train_time:164276ms step_avg:241.94ms torch.cuda.memory_allocated()=369551872
16:29:39.446: step:690/1000 train_loss:5.0900 train_time:164529ms step_avg:241.95ms torch.cuda.memory_allocated()=369551872
16:29:39.702: step:691/1000 train_loss:5.3430 train_time:164784ms step_avg:241.97ms torch.cuda.memory_allocated()=369551872
16:29:39.956: step:692/1000 train_loss:4.9705 train_time:165039ms step_avg:241.99ms torch.cuda.memory_allocated()=369551872
16:29:40.248: step:693/1000 train_loss:5.0397 train_time:165331ms step_avg:242.07ms torch.cuda.memory_allocated()=369551872
16:29:40.548: step:694/1000 train_loss:5.4434 train_time:165631ms step_avg:242.15ms torch.cuda.memory_allocated()=369551872
16:29:40.823: step:695/1000 train_loss:5.1433 train_time:165905ms step_avg:242.20ms torch.cuda.memory_allocated()=369551872
16:29:41.081: step:696/1000 train_loss:5.3058 train_time:166164ms step_avg:242.22ms torch.cuda.memory_allocated()=369551872
16:29:41.334: step:697/1000 train_loss:5.4204 train_time:166417ms step_avg:242.24ms torch.cuda.memory_allocated()=369551872
16:29:41.582: step:698/1000 train_loss:5.0441 train_time:166665ms step_avg:242.25ms torch.cuda.memory_allocated()=369551872
16:29:41.833: step:699/1000 train_loss:5.3487 train_time:166916ms step_avg:242.26ms torch.cuda.memory_allocated()=369551872
16:29:42.099: step:700/1000 train_loss:6.1479 train_time:167181ms step_avg:242.29ms torch.cuda.memory_allocated()=369551872
16:29:42.361: step:701/1000 train_loss:5.0164 train_time:167444ms step_avg:242.32ms torch.cuda.memory_allocated()=369551872
16:29:42.643: step:702/1000 train_loss:4.9519 train_time:167726ms step_avg:242.38ms torch.cuda.memory_allocated()=369551872
16:29:42.909: step:703/1000 train_loss:5.2593 train_time:167992ms step_avg:242.41ms torch.cuda.memory_allocated()=369551872
16:29:43.162: step:704/1000 train_loss:5.3029 train_time:168245ms step_avg:242.43ms torch.cuda.memory_allocated()=369551872
16:29:43.413: step:705/1000 train_loss:5.3711 train_time:168496ms step_avg:242.44ms torch.cuda.memory_allocated()=369551872
16:29:43.658: step:706/1000 train_loss:5.1200 train_time:168741ms step_avg:242.44ms torch.cuda.memory_allocated()=369551872
16:29:43.908: step:707/1000 train_loss:5.4375 train_time:168991ms step_avg:242.45ms torch.cuda.memory_allocated()=369551872
16:29:44.162: step:708/1000 train_loss:5.1208 train_time:169245ms step_avg:242.47ms torch.cuda.memory_allocated()=369551872
16:29:44.414: step:709/1000 train_loss:5.1299 train_time:169497ms step_avg:242.49ms torch.cuda.memory_allocated()=369551872
16:29:44.668: step:710/1000 train_loss:5.2705 train_time:169751ms step_avg:242.50ms torch.cuda.memory_allocated()=369551872
16:29:44.934: step:711/1000 train_loss:5.3605 train_time:170017ms step_avg:242.53ms torch.cuda.memory_allocated()=369551872
16:29:45.183: step:712/1000 train_loss:5.2656 train_time:170266ms step_avg:242.54ms torch.cuda.memory_allocated()=369551872
16:29:45.437: step:713/1000 train_loss:5.6217 train_time:170520ms step_avg:242.56ms torch.cuda.memory_allocated()=369551872
16:29:45.692: step:714/1000 train_loss:5.1907 train_time:170775ms step_avg:242.58ms torch.cuda.memory_allocated()=369551872
16:29:45.971: step:715/1000 train_loss:4.9641 train_time:171054ms step_avg:242.63ms torch.cuda.memory_allocated()=369551872
16:29:46.274: step:716/1000 train_loss:5.1727 train_time:171357ms step_avg:242.72ms torch.cuda.memory_allocated()=369551872
16:29:46.539: step:717/1000 train_loss:4.9580 train_time:171622ms step_avg:242.75ms torch.cuda.memory_allocated()=369551872
16:29:46.791: step:718/1000 train_loss:5.1299 train_time:171874ms step_avg:242.76ms torch.cuda.memory_allocated()=369551872
16:29:47.049: step:719/1000 train_loss:5.2096 train_time:172132ms step_avg:242.78ms torch.cuda.memory_allocated()=369551872
16:29:47.304: step:720/1000 train_loss:5.0581 train_time:172386ms step_avg:242.80ms torch.cuda.memory_allocated()=369551872
16:29:47.553: step:721/1000 train_loss:5.2463 train_time:172636ms step_avg:242.81ms torch.cuda.memory_allocated()=369551872
16:29:47.808: step:722/1000 train_loss:5.3520 train_time:172890ms step_avg:242.82ms torch.cuda.memory_allocated()=369551872
16:29:48.082: step:723/1000 train_loss:4.9162 train_time:173165ms step_avg:242.87ms torch.cuda.memory_allocated()=369551872
16:29:48.380: step:724/1000 train_loss:5.0355 train_time:173463ms step_avg:242.95ms torch.cuda.memory_allocated()=369551872
16:29:48.675: step:725/1000 train_loss:5.3179 train_time:173758ms step_avg:243.02ms torch.cuda.memory_allocated()=369551872
16:29:48.925: step:726/1000 train_loss:5.2552 train_time:174008ms step_avg:243.03ms torch.cuda.memory_allocated()=369551872
16:29:49.175: step:727/1000 train_loss:5.1674 train_time:174257ms step_avg:243.04ms torch.cuda.memory_allocated()=369551872
16:29:49.424: step:728/1000 train_loss:5.2401 train_time:174507ms step_avg:243.05ms torch.cuda.memory_allocated()=369551872
16:29:49.688: step:729/1000 train_loss:4.9542 train_time:174771ms step_avg:243.08ms torch.cuda.memory_allocated()=369551872
16:29:49.999: step:730/1000 train_loss:5.1624 train_time:175082ms step_avg:243.17ms torch.cuda.memory_allocated()=369551872
16:29:50.303: step:731/1000 train_loss:4.9772 train_time:175385ms step_avg:243.25ms torch.cuda.memory_allocated()=369551872
16:29:50.575: step:732/1000 train_loss:4.9851 train_time:175658ms step_avg:243.29ms torch.cuda.memory_allocated()=369551872
16:29:50.856: step:733/1000 train_loss:4.6284 train_time:175939ms step_avg:243.35ms torch.cuda.memory_allocated()=369551872
16:29:51.138: step:734/1000 train_loss:4.8553 train_time:176221ms step_avg:243.40ms torch.cuda.memory_allocated()=369551872
16:29:51.416: step:735/1000 train_loss:5.1573 train_time:176499ms step_avg:243.45ms torch.cuda.memory_allocated()=369551872
16:29:51.705: step:736/1000 train_loss:5.1408 train_time:176788ms step_avg:243.51ms torch.cuda.memory_allocated()=369551872
16:29:51.004: step:737/1000 train_loss:6.0176 train_time:177087ms step_avg:243.59ms torch.cuda.memory_allocated()=369551872
16:29:52.293: step:738/1000 train_loss:5.4048 train_time:177376ms step_avg:243.65ms torch.cuda.memory_allocated()=369551872
16:29:52.574: step:739/1000 train_loss:5.1552 train_time:177657ms step_avg:243.70ms torch.cuda.memory_allocated()=369551872
16:29:52.870: step:740/1000 train_loss:5.0804 train_time:177953ms step_avg:243.77ms torch.cuda.memory_allocated()=369551872
16:29:53.123: step:741/1000 train_loss:5.0822 train_time:178206ms step_avg:243.78ms torch.cuda.memory_allocated()=369551872
16:29:53.377: step:742/1000 train_loss:5.2252 train_time:178460ms step_avg:243.80ms torch.cuda.memory_allocated()=369551872
16:29:53.634: step:743/1000 train_loss:5.1501 train_time:178717ms step_avg:243.82ms torch.cuda.memory_allocated()=369551872
16:29:53.894: step:744/1000 train_loss:5.0326 train_time:178977ms step_avg:243.84ms torch.cuda.memory_allocated()=369551872
16:29:54.177: step:745/1000 train_loss:5.0802 train_time:179260ms step_avg:243.89ms torch.cuda.memory_allocated()=369551872
16:29:54.474: step:746/1000 train_loss:5.2042 train_time:179557ms step_avg:243.96ms torch.cuda.memory_allocated()=369551872
16:29:54.745: step:747/1000 train_loss:5.1654 train_time:179828ms step_avg:244.00ms torch.cuda.memory_allocated()=369551872
16:29:55.024: step:748/1000 train_loss:5.0317 train_time:180107ms step_avg:244.05ms torch.cuda.memory_allocated()=369551872
16:29:55.300: step:749/1000 train_loss:5.1575 train_time:180383ms step_avg:244.09ms torch.cuda.memory_allocated()=369551872
16:29:55.569: step:750/1000 train_loss:5.1242 train_time:180652ms step_avg:244.12ms torch.cuda.memory_allocated()=369551872
16:29:58.428: step:750/1000 val_loss:5.2596 train_time:180653ms step_avg:244.13ms
16:29:58.677: step:751/1000 train_loss:5.1101 train_time:180901ms step_avg:244.13ms torch.cuda.memory_allocated()=369551872
16:29:58.938: step:752/1000 train_loss:5.1584 train_time:181162ms step_avg:244.15ms torch.cuda.memory_allocated()=369551872
16:29:59.200: step:753/1000 train_loss:4.9580 train_time:181425ms step_avg:244.18ms torch.cuda.memory_allocated()=369551872
16:29:59.451: step:754/1000 train_loss:5.4319 train_time:181675ms step_avg:244.19ms torch.cuda.memory_allocated()=369551872
16:29:59.698: step:755/1000 train_loss:5.1150 train_time:181923ms step_avg:244.19ms torch.cuda.memory_allocated()=369551872
16:29:59.950: step:756/1000 train_loss:5.2379 train_time:182174ms step_avg:244.20ms torch.cuda.memory_allocated()=369551872
16:30:00.203: step:757/1000 train_loss:5.2737 train_time:182427ms step_avg:244.21ms torch.cuda.memory_allocated()=369551872
16:30:00.455: step:758/1000 train_loss:5.3833 train_time:182679ms step_avg:244.22ms torch.cuda.memory_allocated()=369551872
16:30:00.723: step:759/1000 train_loss:4.8227 train_time:182947ms step_avg:244.26ms torch.cuda.memory_allocated()=369551872
16:30:01.020: step:760/1000 train_loss:5.1554 train_time:183245ms step_avg:244.33ms torch.cuda.memory_allocated()=369551872
16:30:01.287: step:761/1000 train_loss:4.9738 train_time:183512ms step_avg:244.36ms torch.cuda.memory_allocated()=369551872
16:30:01.540: step:762/1000 train_loss:4.9045 train_time:183764ms step_avg:244.37ms torch.cuda.memory_allocated()=369551872
16:30:01.798: step:763/1000 train_loss:4.9421 train_time:184023ms step_avg:244.39ms torch.cuda.memory_allocated()=369551872
16:30:02.050: step:764/1000 train_loss:5.0560 train_time:184275ms step_avg:244.40ms torch.cuda.memory_allocated()=369551872
16:30:02.304: step:765/1000 train_loss:5.0073 train_time:184529ms step_avg:244.41ms torch.cuda.memory_allocated()=369551872
16:30:02.553: step:766/1000 train_loss:5.2957 train_time:184778ms step_avg:244.42ms torch.cuda.memory_allocated()=369551872
16:30:02.808: step:767/1000 train_loss:5.1112 train_time:185033ms step_avg:244.43ms torch.cuda.memory_allocated()=369551872
16:30:03.064: step:768/1000 train_loss:5.3972 train_time:185288ms step_avg:244.44ms torch.cuda.memory_allocated()=369551872
16:30:03.317: step:769/1000 train_loss:4.9093 train_time:185541ms step_avg:244.45ms torch.cuda.memory_allocated()=369551872
16:30:03.611: step:770/1000 train_loss:5.2337 train_time:185835ms step_avg:244.52ms torch.cuda.memory_allocated()=369551872
16:30:03.863: step:771/1000 train_loss:5.0932 train_time:186087ms step_avg:244.53ms torch.cuda.memory_allocated()=369551872
16:30:04.127: step:772/1000 train_loss:5.0405 train_time:186351ms step_avg:244.56ms torch.cuda.memory_allocated()=369551872
16:30:04.372: step:773/1000 train_loss:5.2379 train_time:186596ms step_avg:244.56ms torch.cuda.memory_allocated()=369551872
16:30:04.633: step:774/1000 train_loss:5.1591 train_time:186857ms step_avg:244.58ms torch.cuda.memory_allocated()=369551872
16:30:04.884: step:775/1000 train_loss:5.0286 train_time:187108ms step_avg:244.59ms torch.cuda.memory_allocated()=369551872
16:30:05.136: step:776/1000 train_loss:5.0539 train_time:187360ms step_avg:244.60ms torch.cuda.memory_allocated()=369551872
16:30:05.387: step:777/1000 train_loss:4.9499 train_time:187611ms step_avg:244.60ms torch.cuda.memory_allocated()=369551872
16:30:05.636: step:778/1000 train_loss:5.0081 train_time:187861ms step_avg:244.61ms torch.cuda.memory_allocated()=369551872
16:30:05.902: step:779/1000 train_loss:4.5630 train_time:188126ms step_avg:244.64ms torch.cuda.memory_allocated()=369551872
16:30:06.153: step:780/1000 train_loss:5.0340 train_time:188378ms step_avg:244.65ms torch.cuda.memory_allocated()=369551872
16:30:06.399: step:781/1000 train_loss:5.1213 train_time:188624ms step_avg:244.65ms torch.cuda.memory_allocated()=369551872
16:30:06.646: step:782/1000 train_loss:4.9691 train_time:188871ms step_avg:244.65ms torch.cuda.memory_allocated()=369551872
16:30:06.905: step:783/1000 train_loss:5.4093 train_time:189129ms step_avg:244.67ms torch.cuda.memory_allocated()=369551872
16:30:07.166: step:784/1000 train_loss:5.0074 train_time:189390ms step_avg:244.69ms torch.cuda.memory_allocated()=369551872
16:30:07.429: step:785/1000 train_loss:5.1403 train_time:189653ms step_avg:244.71ms torch.cuda.memory_allocated()=369551872
16:30:07.680: step:786/1000 train_loss:4.9440 train_time:189905ms step_avg:244.72ms torch.cuda.memory_allocated()=369551872
16:30:07.946: step:787/1000 train_loss:4.7933 train_time:190170ms step_avg:244.75ms torch.cuda.memory_allocated()=369551872
16:30:08.205: step:788/1000 train_loss:5.0092 train_time:190430ms step_avg:244.77ms torch.cuda.memory_allocated()=369551872
16:30:08.458: step:789/1000 train_loss:4.9235 train_time:190682ms step_avg:244.78ms torch.cuda.memory_allocated()=369551872
16:30:08.714: step:790/1000 train_loss:4.8318 train_time:190939ms step_avg:244.79ms torch.cuda.memory_allocated()=369551872
16:30:08.967: step:791/1000 train_loss:5.2745 train_time:191191ms step_avg:244.80ms torch.cuda.memory_allocated()=369551872
16:30:09.216: step:792/1000 train_loss:5.0325 train_time:191440ms step_avg:244.81ms torch.cuda.memory_allocated()=369551872
16:30:09.467: step:793/1000 train_loss:5.0811 train_time:191691ms step_avg:244.82ms torch.cuda.memory_allocated()=369551872
16:30:09.718: step:794/1000 train_loss:5.1025 train_time:191942ms step_avg:244.82ms torch.cuda.memory_allocated()=369551872
16:30:09.969: step:795/1000 train_loss:5.1698 train_time:192193ms step_avg:244.83ms torch.cuda.memory_allocated()=369551872
16:30:10.223: step:796/1000 train_loss:4.9934 train_time:192447ms step_avg:244.84ms torch.cuda.memory_allocated()=369551872
16:30:10.483: step:797/1000 train_loss:5.5268 train_time:192707ms step_avg:244.86ms torch.cuda.memory_allocated()=369551872
16:30:10.737: step:798/1000 train_loss:5.0329 train_time:192961ms step_avg:244.87ms torch.cuda.memory_allocated()=369551872
16:30:10.991: step:799/1000 train_loss:5.2419 train_time:193215ms step_avg:244.89ms torch.cuda.memory_allocated()=369551872
16:30:11.241: step:800/1000 train_loss:5.2785 train_time:193466ms step_avg:244.89ms torch.cuda.memory_allocated()=369551872
16:30:11.490: step:801/1000 train_loss:5.2797 train_time:193715ms step_avg:244.90ms torch.cuda.memory_allocated()=369551872
16:30:11.751: step:802/1000 train_loss:4.6805 train_time:193976ms step_avg:244.92ms torch.cuda.memory_allocated()=369551872
16:30:12.007: step:803/1000 train_loss:4.8586 train_time:194231ms step_avg:244.93ms torch.cuda.memory_allocated()=369551872
16:30:12.280: step:804/1000 train_loss:5.1894 train_time:194505ms step_avg:244.97ms torch.cuda.memory_allocated()=369551872
16:30:12.548: step:805/1000 train_loss:5.6386 train_time:194772ms step_avg:245.00ms torch.cuda.memory_allocated()=369551872
16:30:12.803: step:806/1000 train_loss:5.2675 train_time:195027ms step_avg:245.01ms torch.cuda.memory_allocated()=369551872
16:30:13.056: step:807/1000 train_loss:5.2714 train_time:195281ms step_avg:245.02ms torch.cuda.memory_allocated()=369551872
16:30:13.307: step:808/1000 train_loss:5.0315 train_time:195531ms step_avg:245.03ms torch.cuda.memory_allocated()=369551872
16:30:13.555: step:809/1000 train_loss:4.8801 train_time:195780ms step_avg:245.03ms torch.cuda.memory_allocated()=369551872
16:30:13.816: step:810/1000 train_loss:4.7118 train_time:196040ms step_avg:245.05ms torch.cuda.memory_allocated()=369551872
16:30:14.064: step:811/1000 train_loss:5.2736 train_time:196288ms step_avg:245.05ms torch.cuda.memory_allocated()=369551872
16:30:14.319: step:812/1000 train_loss:5.2101 train_time:196543ms step_avg:245.07ms torch.cuda.memory_allocated()=369551872
16:30:14.567: step:813/1000 train_loss:5.0418 train_time:196791ms step_avg:245.07ms torch.cuda.memory_allocated()=369551872
16:30:14.812: step:814/1000 train_loss:5.2402 train_time:197036ms step_avg:245.07ms torch.cuda.memory_allocated()=369551872
16:30:15.062: step:815/1000 train_loss:5.1864 train_time:197286ms step_avg:245.08ms torch.cuda.memory_allocated()=369551872
16:30:15.310: step:816/1000 train_loss:4.9259 train_time:197535ms step_avg:245.08ms torch.cuda.memory_allocated()=369551872
16:30:15.568: step:817/1000 train_loss:5.0342 train_time:197793ms step_avg:245.10ms torch.cuda.memory_allocated()=369551872
16:30:15.825: step:818/1000 train_loss:4.9275 train_time:198049ms step_avg:245.11ms torch.cuda.memory_allocated()=369551872
16:30:16.072: step:819/1000 train_loss:5.1473 train_time:198296ms step_avg:245.11ms torch.cuda.memory_allocated()=369551872
16:30:16.332: step:820/1000 train_loss:5.2369 train_time:198557ms step_avg:245.13ms torch.cuda.memory_allocated()=369551872
16:30:16.581: step:821/1000 train_loss:4.9606 train_time:198805ms step_avg:245.14ms torch.cuda.memory_allocated()=369551872
16:30:16.833: step:822/1000 train_loss:4.9986 train_time:199058ms step_avg:245.15ms torch.cuda.memory_allocated()=369551872
16:30:17.090: step:823/1000 train_loss:4.9217 train_time:199314ms step_avg:245.16ms torch.cuda.memory_allocated()=369551872
16:30:17.353: step:824/1000 train_loss:4.9032 train_time:199578ms step_avg:245.18ms torch.cuda.memory_allocated()=369551872
16:30:17.610: step:825/1000 train_loss:5.1223 train_time:199834ms step_avg:245.20ms torch.cuda.memory_allocated()=369551872
16:30:17.861: step:826/1000 train_loss:5.0333 train_time:200085ms step_avg:245.20ms torch.cuda.memory_allocated()=369551872
16:30:18.114: step:827/1000 train_loss:4.9234 train_time:200338ms step_avg:245.21ms torch.cuda.memory_allocated()=369551872
16:30:18.376: step:828/1000 train_loss:4.9027 train_time:200600ms step_avg:245.23ms torch.cuda.memory_allocated()=369551872
16:30:18.647: step:829/1000 train_loss:4.8341 train_time:200871ms step_avg:245.26ms torch.cuda.memory_allocated()=369551872
16:30:18.905: step:830/1000 train_loss:5.0903 train_time:201129ms step_avg:245.28ms torch.cuda.memory_allocated()=369551872
16:30:19.162: step:831/1000 train_loss:5.2884 train_time:201386ms step_avg:245.29ms torch.cuda.memory_allocated()=369551872
16:30:19.434: step:832/1000 train_loss:5.8322 train_time:201658ms step_avg:245.33ms torch.cuda.memory_allocated()=369551872
16:30:19.706: step:833/1000 train_loss:5.5463 train_time:201930ms step_avg:245.36ms torch.cuda.memory_allocated()=369551872
16:30:19.961: step:834/1000 train_loss:4.9475 train_time:202185ms step_avg:245.37ms torch.cuda.memory_allocated()=369551872
16:30:20.218: step:835/1000 train_loss:4.9678 train_time:202442ms step_avg:245.38ms torch.cuda.memory_allocated()=369551872
16:30:20.483: step:836/1000 train_loss:5.0984 train_time:202708ms step_avg:245.41ms torch.cuda.memory_allocated()=369551872
16:30:20.742: step:837/1000 train_loss:5.0548 train_time:202967ms step_avg:245.43ms torch.cuda.memory_allocated()=369551872
16:30:20.997: step:838/1000 train_loss:4.8914 train_time:203221ms step_avg:245.44ms torch.cuda.memory_allocated()=369551872
16:30:21.249: step:839/1000 train_loss:4.9973 train_time:203473ms step_avg:245.44ms torch.cuda.memory_allocated()=369551872
16:30:21.504: step:840/1000 train_loss:4.8342 train_time:203728ms step_avg:245.46ms torch.cuda.memory_allocated()=369551872
16:30:21.757: step:841/1000 train_loss:5.0338 train_time:203982ms step_avg:245.47ms torch.cuda.memory_allocated()=369551872
16:30:22.017: step:842/1000 train_loss:5.1423 train_time:204242ms step_avg:245.48ms torch.cuda.memory_allocated()=369551872
16:30:22.275: step:843/1000 train_loss:4.8397 train_time:204499ms step_avg:245.50ms torch.cuda.memory_allocated()=369551872
16:30:22.528: step:844/1000 train_loss:5.0767 train_time:204752ms step_avg:245.51ms torch.cuda.memory_allocated()=369551872
16:30:22.779: step:845/1000 train_loss:5.1834 train_time:205003ms step_avg:245.51ms torch.cuda.memory_allocated()=369551872
16:30:23.029: step:846/1000 train_loss:5.2035 train_time:205253ms step_avg:245.52ms torch.cuda.memory_allocated()=369551872
16:30:23.281: step:847/1000 train_loss:5.1827 train_time:205505ms step_avg:245.53ms torch.cuda.memory_allocated()=369551872
16:30:23.530: step:848/1000 train_loss:5.1447 train_time:205754ms step_avg:245.53ms torch.cuda.memory_allocated()=369551872
16:30:23.775: step:849/1000 train_loss:5.0287 train_time:206000ms step_avg:245.53ms torch.cuda.memory_allocated()=369551872
16:30:24.027: step:850/1000 train_loss:5.1925 train_time:206252ms step_avg:245.54ms torch.cuda.memory_allocated()=369551872
16:30:24.290: step:851/1000 train_loss:4.9650 train_time:206514ms step_avg:245.56ms torch.cuda.memory_allocated()=369551872
16:30:24.544: step:852/1000 train_loss:4.9830 train_time:206768ms step_avg:245.57ms torch.cuda.memory_allocated()=369551872
16:30:24.798: step:853/1000 train_loss:4.7618 train_time:207022ms step_avg:245.58ms torch.cuda.memory_allocated()=369551872
16:30:25.053: step:854/1000 train_loss:5.3041 train_time:207277ms step_avg:245.59ms torch.cuda.memory_allocated()=369551872
16:30:25.297: step:855/1000 train_loss:4.9726 train_time:207521ms step_avg:245.59ms torch.cuda.memory_allocated()=369551872
16:30:25.546: step:856/1000 train_loss:5.0178 train_time:207770ms step_avg:245.59ms torch.cuda.memory_allocated()=369551872
16:30:25.794: step:857/1000 train_loss:5.1847 train_time:208018ms step_avg:245.59ms torch.cuda.memory_allocated()=369551872
16:30:26.040: step:858/1000 train_loss:4.9565 train_time:208264ms step_avg:245.59ms torch.cuda.memory_allocated()=369551872
16:30:26.290: step:859/1000 train_loss:4.9578 train_time:208515ms step_avg:245.60ms torch.cuda.memory_allocated()=369551872
16:30:26.541: step:860/1000 train_loss:5.4214 train_time:208765ms step_avg:245.61ms torch.cuda.memory_allocated()=369551872
16:30:26.788: step:861/1000 train_loss:5.1656 train_time:209013ms step_avg:245.61ms torch.cuda.memory_allocated()=369551872
16:30:27.038: step:862/1000 train_loss:4.7969 train_time:209262ms step_avg:245.61ms torch.cuda.memory_allocated()=369551872
16:30:27.296: step:863/1000 train_loss:4.9694 train_time:209520ms step_avg:245.63ms torch.cuda.memory_allocated()=369551872
16:30:27.548: step:864/1000 train_loss:4.9522 train_time:209772ms step_avg:245.63ms torch.cuda.memory_allocated()=369551872
16:30:27.802: step:865/1000 train_loss:4.8810 train_time:210027ms step_avg:245.65ms torch.cuda.memory_allocated()=369551872
16:30:28.067: step:866/1000 train_loss:4.5409 train_time:210292ms step_avg:245.67ms torch.cuda.memory_allocated()=369551872
16:30:28.315: step:867/1000 train_loss:5.0257 train_time:210539ms step_avg:245.67ms torch.cuda.memory_allocated()=369551872
16:30:28.564: step:868/1000 train_loss:4.8015 train_time:210788ms step_avg:245.67ms torch.cuda.memory_allocated()=369551872
16:30:28.817: step:869/1000 train_loss:5.0072 train_time:211041ms step_avg:245.68ms torch.cuda.memory_allocated()=369551872
16:30:29.075: step:870/1000 train_loss:5.0474 train_time:211299ms step_avg:245.70ms torch.cuda.memory_allocated()=369551872
16:30:29.326: step:871/1000 train_loss:5.0162 train_time:211550ms step_avg:245.70ms torch.cuda.memory_allocated()=369551872
16:30:29.589: step:872/1000 train_loss:4.8969 train_time:211813ms step_avg:245.72ms torch.cuda.memory_allocated()=369551872
16:30:29.868: step:873/1000 train_loss:4.8659 train_time:212092ms step_avg:245.76ms torch.cuda.memory_allocated()=369551872
16:30:30.126: step:874/1000 train_loss:4.8930 train_time:212351ms step_avg:245.78ms torch.cuda.memory_allocated()=369551872
16:30:30.400: step:875/1000 train_loss:4.7108 train_time:212624ms step_avg:245.81ms torch.cuda.memory_allocated()=369551872
16:30:33.250: step:875/1000 val_loss:5.1198 train_time:212625ms step_avg:245.81ms
16:30:33.498: step:876/1000 train_loss:4.9777 train_time:212873ms step_avg:245.81ms torch.cuda.memory_allocated()=369551872
16:30:33.747: step:877/1000 train_loss:4.9610 train_time:213121ms step_avg:245.81ms torch.cuda.memory_allocated()=369551872
16:30:34.020: step:878/1000 train_loss:4.5701 train_time:213394ms step_avg:245.85ms torch.cuda.memory_allocated()=369551872
16:30:34.275: step:879/1000 train_loss:4.8618 train_time:213650ms step_avg:245.86ms torch.cuda.memory_allocated()=369551872
16:30:34.531: step:880/1000 train_loss:4.8391 train_time:213905ms step_avg:245.87ms torch.cuda.memory_allocated()=369551872
16:30:34.785: step:881/1000 train_loss:4.9389 train_time:214160ms step_avg:245.88ms torch.cuda.memory_allocated()=369551872
16:30:35.044: step:882/1000 train_loss:5.1928 train_time:214419ms step_avg:245.89ms torch.cuda.memory_allocated()=369551872
16:30:35.291: step:883/1000 train_loss:4.9501 train_time:214666ms step_avg:245.89ms torch.cuda.memory_allocated()=369551872
16:30:35.542: step:884/1000 train_loss:4.9137 train_time:214917ms step_avg:245.90ms torch.cuda.memory_allocated()=369551872
16:30:35.797: step:885/1000 train_loss:4.9064 train_time:215172ms step_avg:245.91ms torch.cuda.memory_allocated()=369551872
16:30:36.045: step:886/1000 train_loss:4.9052 train_time:215420ms step_avg:245.91ms torch.cuda.memory_allocated()=369551872
16:30:36.298: step:887/1000 train_loss:5.2399 train_time:215673ms step_avg:245.92ms torch.cuda.memory_allocated()=369551872
16:30:36.550: step:888/1000 train_loss:5.1118 train_time:215925ms step_avg:245.93ms torch.cuda.memory_allocated()=369551872
16:30:36.807: step:889/1000 train_loss:5.0023 train_time:216182ms step_avg:245.94ms torch.cuda.memory_allocated()=369551872
16:30:37.065: step:890/1000 train_loss:4.9837 train_time:216440ms step_avg:245.95ms torch.cuda.memory_allocated()=369551872
16:30:37.323: step:891/1000 train_loss:4.8965 train_time:216698ms step_avg:245.97ms torch.cuda.memory_allocated()=369551872
16:30:37.574: step:892/1000 train_loss:4.9637 train_time:216949ms step_avg:245.97ms torch.cuda.memory_allocated()=369551872
16:30:37.848: step:893/1000 train_loss:5.0425 train_time:217223ms step_avg:246.01ms torch.cuda.memory_allocated()=369551872
16:30:38.096: step:894/1000 train_loss:5.0298 train_time:217471ms step_avg:246.01ms torch.cuda.memory_allocated()=369551872
16:30:38.347: step:895/1000 train_loss:4.9638 train_time:217722ms step_avg:246.01ms torch.cuda.memory_allocated()=369551872
16:30:38.604: step:896/1000 train_loss:4.7616 train_time:217979ms step_avg:246.03ms torch.cuda.memory_allocated()=369551872
16:30:38.860: step:897/1000 train_loss:5.0459 train_time:218234ms step_avg:246.04ms torch.cuda.memory_allocated()=369551872
16:30:39.107: step:898/1000 train_loss:5.0217 train_time:218482ms step_avg:246.04ms torch.cuda.memory_allocated()=369551872
16:30:39.352: step:899/1000 train_loss:5.0400 train_time:218727ms step_avg:246.04ms torch.cuda.memory_allocated()=369551872
16:30:39.613: step:900/1000 train_loss:5.0389 train_time:218987ms step_avg:246.05ms torch.cuda.memory_allocated()=369551872
16:30:39.875: step:901/1000 train_loss:5.1026 train_time:219250ms step_avg:246.07ms torch.cuda.memory_allocated()=369551872
16:30:40.134: step:902/1000 train_loss:4.7012 train_time:219508ms step_avg:246.09ms torch.cuda.memory_allocated()=369551872
16:30:40.387: step:903/1000 train_loss:4.8701 train_time:219762ms step_avg:246.09ms torch.cuda.memory_allocated()=369551872
16:30:40.634: step:904/1000 train_loss:5.0495 train_time:220008ms step_avg:246.09ms torch.cuda.memory_allocated()=369551872
16:30:40.882: step:905/1000 train_loss:5.0120 train_time:220256ms step_avg:246.10ms torch.cuda.memory_allocated()=369551872
16:30:41.130: step:906/1000 train_loss:4.9038 train_time:220505ms step_avg:246.10ms torch.cuda.memory_allocated()=369551872
16:30:41.385: step:907/1000 train_loss:5.0304 train_time:220759ms step_avg:246.11ms torch.cuda.memory_allocated()=369551872
16:30:41.643: step:908/1000 train_loss:5.1099 train_time:221018ms step_avg:246.12ms torch.cuda.memory_allocated()=369551872
16:30:41.898: step:909/1000 train_loss:4.7217 train_time:221272ms step_avg:246.13ms torch.cuda.memory_allocated()=369551872
16:30:42.149: step:910/1000 train_loss:5.1629 train_time:221524ms step_avg:246.14ms torch.cuda.memory_allocated()=369551872
16:30:42.402: step:911/1000 train_loss:5.3600 train_time:221777ms step_avg:246.15ms torch.cuda.memory_allocated()=369551872
16:30:42.679: step:912/1000 train_loss:5.1291 train_time:222054ms step_avg:246.18ms torch.cuda.memory_allocated()=369551872
16:30:42.952: step:913/1000 train_loss:5.0295 train_time:222326ms step_avg:246.21ms torch.cuda.memory_allocated()=369551872
16:30:43.199: step:914/1000 train_loss:4.9818 train_time:222574ms step_avg:246.21ms torch.cuda.memory_allocated()=369551872
16:30:43.449: step:915/1000 train_loss:4.7497 train_time:222823ms step_avg:246.21ms torch.cuda.memory_allocated()=369551872
16:30:43.708: step:916/1000 train_loss:5.6174 train_time:223083ms step_avg:246.23ms torch.cuda.memory_allocated()=369551872
16:30:43.965: step:917/1000 train_loss:5.4126 train_time:223339ms step_avg:246.24ms torch.cuda.memory_allocated()=369551872
16:30:44.218: step:918/1000 train_loss:5.1582 train_time:223593ms step_avg:246.25ms torch.cuda.memory_allocated()=369551872
16:30:44.474: step:919/1000 train_loss:5.2835 train_time:223849ms step_avg:246.26ms torch.cuda.memory_allocated()=369551872
16:30:44.736: step:920/1000 train_loss:4.7906 train_time:224110ms step_avg:246.28ms torch.cuda.memory_allocated()=369551872
16:30:44.990: step:921/1000 train_loss:5.0046 train_time:224365ms step_avg:246.28ms torch.cuda.memory_allocated()=369551872
16:30:45.243: step:922/1000 train_loss:4.9123 train_time:224618ms step_avg:246.29ms torch.cuda.memory_allocated()=369551872
16:30:45.500: step:923/1000 train_loss:5.0573 train_time:224875ms step_avg:246.30ms torch.cuda.memory_allocated()=369551872
16:30:45.748: step:924/1000 train_loss:5.0095 train_time:225122ms step_avg:246.30ms torch.cuda.memory_allocated()=369551872
16:30:46.007: step:925/1000 train_loss:4.7030 train_time:225382ms step_avg:246.32ms torch.cuda.memory_allocated()=369551872
16:30:46.264: step:926/1000 train_loss:5.1192 train_time:225639ms step_avg:246.33ms torch.cuda.memory_allocated()=369551872
16:30:46.523: step:927/1000 train_loss:5.0201 train_time:225897ms step_avg:246.34ms torch.cuda.memory_allocated()=369551872
16:30:46.775: step:928/1000 train_loss:4.7894 train_time:226150ms step_avg:246.35ms torch.cuda.memory_allocated()=369551872
16:30:47.027: step:929/1000 train_loss:4.6888 train_time:226402ms step_avg:246.36ms torch.cuda.memory_allocated()=369551872
16:30:47.280: step:930/1000 train_loss:5.3267 train_time:226655ms step_avg:246.36ms torch.cuda.memory_allocated()=369551872
16:30:47.532: step:931/1000 train_loss:4.9615 train_time:226906ms step_avg:246.37ms torch.cuda.memory_allocated()=369551872
16:30:47.795: step:932/1000 train_loss:4.5733 train_time:227170ms step_avg:246.39ms torch.cuda.memory_allocated()=369551872
16:30:48.050: step:933/1000 train_loss:4.7049 train_time:227425ms step_avg:246.40ms torch.cuda.memory_allocated()=369551872
16:30:48.311: step:934/1000 train_loss:4.7526 train_time:227686ms step_avg:246.41ms torch.cuda.memory_allocated()=369551872
16:30:48.566: step:935/1000 train_loss:4.9002 train_time:227941ms step_avg:246.42ms torch.cuda.memory_allocated()=369551872
16:30:48.814: step:936/1000 train_loss:4.9128 train_time:228189ms step_avg:246.42ms torch.cuda.memory_allocated()=369551872
16:30:49.062: step:937/1000 train_loss:4.9047 train_time:228437ms step_avg:246.43ms torch.cuda.memory_allocated()=369551872
16:30:49.324: step:938/1000 train_loss:4.7995 train_time:228699ms step_avg:246.44ms torch.cuda.memory_allocated()=369551872
16:30:49.578: step:939/1000 train_loss:5.2285 train_time:228953ms step_avg:246.45ms torch.cuda.memory_allocated()=369551872
16:30:49.845: step:940/1000 train_loss:4.9261 train_time:229220ms step_avg:246.47ms torch.cuda.memory_allocated()=369551872
16:30:50.101: step:941/1000 train_loss:5.0998 train_time:229476ms step_avg:246.48ms torch.cuda.memory_allocated()=369551872
16:30:50.364: step:942/1000 train_loss:4.8932 train_time:229739ms step_avg:246.50ms torch.cuda.memory_allocated()=369551872
16:30:50.617: step:943/1000 train_loss:4.7334 train_time:229992ms step_avg:246.51ms torch.cuda.memory_allocated()=369551872
16:30:50.877: step:944/1000 train_loss:4.6416 train_time:230252ms step_avg:246.52ms torch.cuda.memory_allocated()=369551872
16:30:51.130: step:945/1000 train_loss:5.2403 train_time:230505ms step_avg:246.53ms torch.cuda.memory_allocated()=369551872
16:30:51.383: step:946/1000 train_loss:4.9872 train_time:230758ms step_avg:246.54ms torch.cuda.memory_allocated()=369551872
16:30:51.630: step:947/1000 train_loss:5.1065 train_time:231005ms step_avg:246.54ms torch.cuda.memory_allocated()=369551872
16:30:51.876: step:948/1000 train_loss:4.9982 train_time:231251ms step_avg:246.54ms torch.cuda.memory_allocated()=369551872
16:30:52.143: step:949/1000 train_loss:4.9192 train_time:231518ms step_avg:246.56ms torch.cuda.memory_allocated()=369551872
16:30:52.426: step:950/1000 train_loss:4.9333 train_time:231801ms step_avg:246.60ms torch.cuda.memory_allocated()=369551872
16:30:52.684: step:951/1000 train_loss:4.8694 train_time:232059ms step_avg:246.61ms torch.cuda.memory_allocated()=369551872
16:30:52.940: step:952/1000 train_loss:5.0284 train_time:232315ms step_avg:246.62ms torch.cuda.memory_allocated()=369551872
16:30:53.200: step:953/1000 train_loss:5.1401 train_time:232574ms step_avg:246.63ms torch.cuda.memory_allocated()=369551872
16:30:53.454: step:954/1000 train_loss:4.9020 train_time:232829ms step_avg:246.64ms torch.cuda.memory_allocated()=369551872
16:30:53.731: step:955/1000 train_loss:4.7470 train_time:233106ms step_avg:246.67ms torch.cuda.memory_allocated()=369551872
16:30:53.985: step:956/1000 train_loss:4.6927 train_time:233360ms step_avg:246.68ms torch.cuda.memory_allocated()=369551872
16:30:54.240: step:957/1000 train_loss:4.9614 train_time:233615ms step_avg:246.69ms torch.cuda.memory_allocated()=369551872
16:30:54.522: step:958/1000 train_loss:5.5851 train_time:233897ms step_avg:246.73ms torch.cuda.memory_allocated()=369551872
16:30:54.778: step:959/1000 train_loss:4.9274 train_time:234152ms step_avg:246.74ms torch.cuda.memory_allocated()=369551872
16:30:55.031: step:960/1000 train_loss:5.0092 train_time:234406ms step_avg:246.74ms torch.cuda.memory_allocated()=369551872
16:30:55.292: step:961/1000 train_loss:4.7325 train_time:234666ms step_avg:246.76ms torch.cuda.memory_allocated()=369551872
16:30:55.552: step:962/1000 train_loss:4.6743 train_time:234926ms step_avg:246.77ms torch.cuda.memory_allocated()=369551872
16:30:55.807: step:963/1000 train_loss:5.0148 train_time:235182ms step_avg:246.78ms torch.cuda.memory_allocated()=369551872
16:30:56.060: step:964/1000 train_loss:4.9649 train_time:235434ms step_avg:246.79ms torch.cuda.memory_allocated()=369551872
16:30:56.318: step:965/1000 train_loss:4.9504 train_time:235693ms step_avg:246.80ms torch.cuda.memory_allocated()=369551872
16:30:56.568: step:966/1000 train_loss:4.9736 train_time:235942ms step_avg:246.80ms torch.cuda.memory_allocated()=369551872
16:30:56.819: step:967/1000 train_loss:4.9925 train_time:236194ms step_avg:246.81ms torch.cuda.memory_allocated()=369551872
16:30:57.068: step:968/1000 train_loss:4.9283 train_time:236443ms step_avg:246.81ms torch.cuda.memory_allocated()=369551872
16:30:57.326: step:969/1000 train_loss:4.7874 train_time:236701ms step_avg:246.82ms torch.cuda.memory_allocated()=369551872
16:30:57.576: step:970/1000 train_loss:4.8235 train_time:236951ms step_avg:246.82ms torch.cuda.memory_allocated()=369551872
16:30:57.829: step:971/1000 train_loss:4.9985 train_time:237204ms step_avg:246.83ms torch.cuda.memory_allocated()=369551872
16:30:58.082: step:972/1000 train_loss:4.8749 train_time:237456ms step_avg:246.84ms torch.cuda.memory_allocated()=369551872
16:30:58.336: step:973/1000 train_loss:4.9348 train_time:237711ms step_avg:246.84ms torch.cuda.memory_allocated()=369551872
16:30:58.585: step:974/1000 train_loss:4.8133 train_time:237960ms step_avg:246.85ms torch.cuda.memory_allocated()=369551872
16:30:58.831: step:975/1000 train_loss:4.9649 train_time:238206ms step_avg:246.85ms torch.cuda.memory_allocated()=369551872
16:30:59.076: step:976/1000 train_loss:5.0124 train_time:238451ms step_avg:246.84ms torch.cuda.memory_allocated()=369551872
16:30:59.341: step:977/1000 train_loss:5.0442 train_time:238716ms step_avg:246.86ms torch.cuda.memory_allocated()=369551872
16:30:59.597: step:978/1000 train_loss:5.2726 train_time:238972ms step_avg:246.87ms torch.cuda.memory_allocated()=369551872
16:30:59.879: step:979/1000 train_loss:5.5298 train_time:239253ms step_avg:246.91ms torch.cuda.memory_allocated()=369551872
16:31:00.166: step:980/1000 train_loss:5.7238 train_time:239541ms step_avg:246.95ms torch.cuda.memory_allocated()=369551872
16:31:00.434: step:981/1000 train_loss:5.1997 train_time:239808ms step_avg:246.97ms torch.cuda.memory_allocated()=369551872
16:31:00.683: step:982/1000 train_loss:5.2004 train_time:240058ms step_avg:246.97ms torch.cuda.memory_allocated()=369551872
16:31:00.944: step:983/1000 train_loss:5.0560 train_time:240319ms step_avg:246.99ms torch.cuda.memory_allocated()=369551872
16:31:01.196: step:984/1000 train_loss:4.9576 train_time:240570ms step_avg:246.99ms torch.cuda.memory_allocated()=369551872
16:31:01.452: step:985/1000 train_loss:4.8789 train_time:240827ms step_avg:247.00ms torch.cuda.memory_allocated()=369551872
16:31:01.698: step:986/1000 train_loss:4.8674 train_time:241073ms step_avg:247.00ms torch.cuda.memory_allocated()=369551872
16:31:01.948: step:987/1000 train_loss:4.7667 train_time:241323ms step_avg:247.00ms torch.cuda.memory_allocated()=369551872
16:31:02.201: step:988/1000 train_loss:5.1294 train_time:241575ms step_avg:247.01ms torch.cuda.memory_allocated()=369551872
16:31:02.460: step:989/1000 train_loss:4.7804 train_time:241835ms step_avg:247.02ms torch.cuda.memory_allocated()=369551872
16:31:02.721: step:990/1000 train_loss:4.9643 train_time:242096ms step_avg:247.04ms torch.cuda.memory_allocated()=369551872
16:31:02.988: step:991/1000 train_loss:4.7549 train_time:242362ms step_avg:247.06ms torch.cuda.memory_allocated()=369551872
16:31:03.241: step:992/1000 train_loss:4.8129 train_time:242615ms step_avg:247.06ms torch.cuda.memory_allocated()=369551872
16:31:03.488: step:993/1000 train_loss:4.9044 train_time:242863ms step_avg:247.06ms torch.cuda.memory_allocated()=369551872
16:31:03.737: step:994/1000 train_loss:4.7961 train_time:243112ms step_avg:247.07ms torch.cuda.memory_allocated()=369551872
16:31:03.004: step:995/1000 train_loss:4.8923 train_time:243378ms step_avg:247.08ms torch.cuda.memory_allocated()=369551872
16:31:04.279: step:996/1000 train_loss:4.2892 train_time:243654ms step_avg:247.11ms torch.cuda.memory_allocated()=369551872
16:31:04.540: step:997/1000 train_loss:4.6352 train_time:243915ms step_avg:247.13ms torch.cuda.memory_allocated()=369551872
16:31:04.797: step:998/1000 train_loss:4.6594 train_time:244172ms step_avg:247.14ms torch.cuda.memory_allocated()=369551872
16:31:05.043: step:999/1000 train_loss:4.8102 train_time:244418ms step_avg:247.14ms torch.cuda.memory_allocated()=369551872
16:31:05.286: step:1000/1000 train_loss:4.7827 train_time:244661ms step_avg:247.13ms torch.cuda.memory_allocated()=369551872
16:31:08.143: step:1000/1000 val_loss:5.0411 train_time:244661ms step_avg:247.13ms
16:32:04.224: Renamed logs/459123d4-3385-4cee-a7bb-6a3dd7c8bdc1.txt -> logs/20250202_MoEUT_Inits.txt
16:32:04.226: peak memory allocated: 6688 MiB reserved: 10684 MiB
