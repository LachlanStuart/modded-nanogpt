13:59:47.367: from collections import defaultdict
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import atexit

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.profiler import profile, record_function, ProfilerActivity
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
# torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)
import triton
import triton.language as tl

try:
    from lovely_tensors import monkey_patch
    monkey_patch()
except ImportError:
    pass


# -----------------------------------------------------------------------------
#region  Custom operators : FP8 matmul by @YouJiacheng
@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        # x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        x_f8 = x.mul(x_s).to(torch.float8_e5m2)
        # w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e5m2)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    # return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)
    return x @ w.t(), x.to(torch.float8_e5m2), w.to(torch.float8_e5m2)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)
#endregion
# -----------------------------------------------------------------------------
#region Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()
#endregion
# -----------------------------------------------------------------------------
#region PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve
#endregion
# -----------------------------------------------------------------------------
#region The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

def create_block_masks(input_seq: Tensor, sliding_window_num_blocks: Tensor):
    BLOCK_SIZE = 128
    docs = (input_seq == 50256).cumsum(0)

    def document_causal(b, h, q_idx, kv_idx):
        causal_mask = q_idx >= kv_idx
        document_mask = docs[q_idx] == docs[kv_idx]
        return causal_mask & document_mask

    def dense_to_ordered(dense_mask: Tensor):
        num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
        indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
        return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

    # manual block mask creation by @YouJiacheng
    assert len(input_seq) % BLOCK_SIZE == 0
    NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
    block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
    any_causal_bm = block_idx[:, None] >= block_idx
    all_causal_bm = block_idx[:, None] > block_idx
    docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
    docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
    any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
    all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
    any_bm = any_causal_bm & any_document_bm
    all_bm = all_causal_bm & all_document_bm
    partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
    full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
    def build_bm(sw_num_blocks: Tensor) -> BlockMask:
        return BlockMask.from_kv_blocks(
            torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
            partial_kv_indices,
            torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
            full_kv_indices,
            BLOCK_SIZE=BLOCK_SIZE,
            mask_mod=document_causal,
        )
    # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
    return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        # self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977


    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i, block in enumerate(self.blocks[:self.num_encoder_layers]):
            x = block(x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i, block in enumerate(self.blocks[self.num_encoder_layers:]):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = block(x, ve_dec[i], x0, block_masks[i])

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

#endregion
# -----------------------------------------------------------------------------
#region MoEUT Triton kernels
# From https://github.com/RobertCsordas/moeut/blob/master/moeut/cvmm.py

@dataclass
class CVMMSel:
    raw_sel: torch.Tensor
    sel: torch.Tensor
    sel_index: torch.Tensor
    out_index: torch.Tensor | None = None
    reduction_weight: torch.Tensor | None = None

    def clone(self) -> 'CVMMSel':
        return CVMMSel(self.raw_sel, self.sel, self.sel_index, self.out_index, self.reduction_weight)


def cvmm_prepare_sel(sel: torch.Tensor, n_experts: int) -> CVMMSel:
    fsel = sel.flatten()
    ssel, sel_index = fsel.sort()
    return CVMMSel(sel, ssel.view_as(sel), sel_index, None)


def dtype_to_type_id(dtype: torch.dtype):
    if dtype == torch.float32:
        return 0
    elif dtype == torch.float16:
        return 1
    elif dtype == torch.bfloat16:
        return 2

    raise ValueError("Unknown dtype")


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
    ],
    key=['M', 'N', 'K', 'dtype_id', 'allow_tf32']
)
@triton.jit
def cvmm_kernel(
    # Pointers to matrices
    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,
    # Matrix dimensions
    M, N, K,
    # The stride variables represent how much to increase the ptr by when moving by 1
    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`
    # by to get the element one row down (A has M rows).
    stride_am, stride_ak,
    stride_bo, stride_bk, stride_bn,
    stride_cm, stride_cn,
    stride_index, stride_sel, stride_out_index,
    out_index_is_none: tl.constexpr,
    dtype_id: tl.constexpr, allow_tf32: tl.constexpr,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr
):
    """Kernel for computing the matmul C = A x B.
    A has shape (M, K), B has shape (K, N) and C has shape (M, N)
    """
    # -----------------------------------------------------------
    # Map program ids `pid` to the block of C it should compute.
    # This is done in a grouped ordering to promote L2 data reuse.
    # See above `L2 Cache Optimizations` section for details.
    pid = tl.program_id(axis=0)

    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_n = (pid % num_pid_in_group) // group_size_m

    pid_m = first_pid_m + (pid % group_size_m)

    sel_first = tl.load(sel_ptr + pid_m * BLOCK_SIZE_M * stride_sel)
    sel_last = tl.load(sel_ptr + (min((pid_m + 1) * BLOCK_SIZE_M, M) - 1) * stride_sel)
    sel_all = tl.load(sel_ptr + stride_sel * ((pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M))

    for matrix_id in range(sel_first, sel_last + 1):
        # ----------------------------------------------------------
        # Create pointers for the first blocks of A and B.
        # We will advance this pointer as we move in the K direction
        # and accumulate
        # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
        # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
        # See above `Pointer Arithmetics` section for details
        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N

        remap_offs_am = tl.load(index_ptr + stride_index * offs_am)

        # Create offset pointers
        offs_k = tl.arange(0, BLOCK_SIZE_K)
        a_ptrs = a_ptr + (remap_offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
        b_ptrs = b_ptr + matrix_id * stride_bo + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

        # -----------------------------------------------------------
        # Iterate to compute a block of the C matrix.
        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
        # of fp32 values for higher accuracy.
        # `accumulator` will be converted back to fp16 after the loop.
        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
            # Load the next block of A and B, generate a mask by checking the K dimension.
            # If it is out of bounds, set it to 0.
            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
            # We accumulate along the K dimension.

            # Triton was unhappy with passing dtypes as vars.
            if dtype_id == 1:
                a = a.to(tl.float16)
                b = b.to(tl.float16)
            elif dtype_id == 2:
                a = a.to(tl.bfloat16)
                b = b.to(tl.bfloat16)

            accumulator += tl.dot(a, b, allow_tf32=allow_tf32)

            # Advance the ptrs to the next K block.
            a_ptrs += BLOCK_SIZE_K * stride_ak
            b_ptrs += BLOCK_SIZE_K * stride_bk


        if dtype_id == 1:
            c = accumulator.to(tl.float16)
        elif dtype_id == 2:
            c = accumulator.to(tl.bfloat16)
        else:
            c = accumulator

        # -----------------------------------------------------------
        # Write back the block of the output matrix C with masks.
        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)

        if out_index_is_none:
            remap_offs_cm = remap_offs_am
        else:
            remap_offs_cm = tl.load(out_index_ptr + stride_out_index * offs_am)

        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
        c_ptrs = c_ptr + stride_cm * remap_offs_cm[:, None] + stride_cn * offs_cn[None, :]
        c_mask = ((offs_cm[:, None] < M) & (sel_all[:, None] == matrix_id)) & (offs_cn[None, :] < N)
        tl.store(c_ptrs, c, mask=c_mask)


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 128}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 4}, num_stages=4, num_warps=4),

        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        # triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 128}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),

        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 16}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 16}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
    ],
    key=['M', 'N', 'K', 'out_dtype_id', 'allow_tf32', 'dtype_id'], reset_to_zero = ['c_ptr']
)
@triton.jit
def cvmm_backward_kernel3(
    # Pointers to matrices
    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,
    # Matrix dimensions
    M, N, K,
    # The stride variables represent how much to increase the ptr by when moving by 1
    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`
    # by to get the element one row down (A has M rows).
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_co, stride_cm, stride_cn,
    stride_index, stride_sel, stride_out_index,
    out_index_is_none: tl.constexpr,
    out_dtype_id: tl.constexpr, allow_tf32: tl.constexpr, dtype_id: tl.constexpr,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr, K_BLOCKS: tl.constexpr
):
    """Kernel for computing the matmul C = A x B.
    A has shape (M, K), B has shape (K, N) and C has shape (M, N)
    """
    # -----------------------------------------------------------
    # Map program ids `pid` to the block of C it should compute.
    # This is done in a grouped ordering to promote L2 data reuse.
    # See above `L2 Cache Optimizations` section for details.
    pid = tl.program_id(axis=0)
    k_block_id = tl.program_id(axis=1)

    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # ----------------------------------------------------------
    # Create pointers for the first blocks of A and B.
    # We will advance this pointer as we move in the K direction
    # and accumulate
    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
    # See above `Pointer Arithmetics` section for details
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N

    # -----------------------------------------------------------
    # Iterate to compute a block of the C matrix.
    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
    # of fp32 values for higher accuracy.
    # `accumulator` will be converted back to fp16 after the loop.

    a_ptrs_this = a_ptr + offs_am[:, None] * stride_am
    b_ptrs_this = b_ptr + offs_bn[None, :] * stride_bn

    # Kactual = end_i - start_i
    # Nblocks = (Kactual + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K

    # WORK_PER_WORKER = (Nblocks + K_BLOCKS - 1) // K_BLOCKS
    # WORK_PER_WORKER = WORK_PER_WORKER if WORK_PER_WORKER > MIN_WORK_SIZE else MIN_WORK_SIZE


    # # Kloop_start = (Kactual + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K

    # first_block_k = k_block_id * WORK_PER_WORKER
    # last_block_k = min((k_block_id+1) * WORK_PER_WORKER, Nblocks)

    block_start_index = k_block_id * BLOCK_SIZE_K * K_BLOCKS
    block_end_index = min(block_start_index + BLOCK_SIZE_K * K_BLOCKS, K) - 1

    first_mat = tl.load(sel_ptr + stride_sel * block_start_index)
    last_mat = tl.load(sel_ptr + stride_sel * block_end_index)


    for matrix_index in range(first_mat, last_mat + 1):
        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

        start_i = block_start_index
        end_i = block_end_index + 1
        while start_i < end_i:
            middle = (start_i + end_i) // 2
            middle_matrix = tl.load(sel_ptr + middle * stride_sel)
            if middle_matrix < matrix_index:
                start_i = middle + 1
            else:
                end_i = middle


        # # Continue binary search: find the first matrix that is > matrix_index
        start_i2 = start_i
        end_i = block_end_index + 1
        while start_i2 < end_i:
            middle = (start_i2 + end_i) // 2
            middle_matrix = tl.load(sel_ptr + middle * stride_sel)
            if middle_matrix <= matrix_index:
                start_i2 = middle + 1
            else:
                end_i = middle

        end_i = start_i2

        count = end_i - start_i

        block_mem_indices_f_base = start_i  + tl.arange(0, BLOCK_SIZE_K)

        if count > 0:
            for k in range((count + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K):
                # block_mem_indices = (k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)) % K
                block_mem_indices_f = block_mem_indices_f_base + k * BLOCK_SIZE_K
                block_mem_indices = block_mem_indices_f % K
                a_index = tl.load(index_ptr + stride_index * block_mem_indices)
                if out_index_is_none:
                    b_index = a_index
                else:
                    b_index = tl.load(out_index_ptr + stride_out_index * block_mem_indices)
                sel_ok = block_mem_indices_f < end_i

                a_ptrs = a_ptrs_this + a_index[None, :] * stride_ak
                b_ptrs = b_ptrs_this + b_index[:, None] * stride_bk

                # Load the next block of A and B, generate a mask by checking the K dimension.
                # If it is out of bounds, set it to 0.
                a = tl.load(a_ptrs, mask=sel_ok[None, :], other=0.0)
                b = tl.load(b_ptrs, mask=sel_ok[:, None], other=0.0)

                if dtype_id == 1:
                    a = a.to(tl.float16)
                    b = b.to(tl.float16)
                elif dtype_id == 2:
                    a = a.to(tl.bfloat16)
                    b = b.to(tl.bfloat16)

                # We accumulate along the K dimension.
                accumulator += tl.dot(a, b, allow_tf32=allow_tf32)

            if out_dtype_id == 1:
                c = accumulator.to(tl.float16)
            elif out_dtype_id == 2:
                c = accumulator.to(tl.bfloat16)
            else:
                c = accumulator

            # -----------------------------------------------------------
            # Write back the block of the output matrix C with masks.
            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
            c_ptrs = c_ptr + stride_co * matrix_index + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
            # tl.store(c_ptrs, c, mask=c_mask)
            tl.atomic_add(c_ptrs, c, mask=c_mask)


torch.library.define("mylib::cvmm_triton", "(Tensor x, Tensor sel_index, Tensor sel, Tensor keys, ScalarType out_dtype, Tensor out_index) -> Tensor")
@torch.library.impl("mylib::cvmm_triton", "default")
def cvmm_triton(
    x: torch.Tensor,
    sel_index: torch.Tensor,
    sel: torch.Tensor,
    keys: torch.Tensor,
    out_dtype: torch.dtype,
    out_index: torch.Tensor
):
    x = x.flatten(end_dim=-2)
    assert x.shape[-1] == keys.shape[1]

    sel_shape = sel.shape
    sel = sel.flatten()

    M = sel.shape[0]
    O, K, N = keys.shape
    # Allocates output.
    out = torch.empty((M, N), device=x.device, dtype=out_dtype)
    # out = torch.zeros((M, N), device=x.device, dtype=out_dtype)
    # 1D launch kernel where each block gets its own program.

    # expected_m_per_matrix = int(math.ceil(M / O * 1.5))
    # expected_m_per_matrix = M

    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )

    out_index_is_none = False
    if out_index.numel() == 1 and out_index == -1:
        out_index_is_none = True

    # FIXME: This has illegal memory access for non-float32 x/keys?
    cvmm_kernel[grid](
        x, keys, out, sel_index, sel, out_index,
        M, N, K,
        x.stride(0), x.stride(1),
        keys.stride(0), keys.stride(1), keys.stride(2),
        out.stride(0), out.stride(1),
        sel_index.stride(0), sel.stride(0), 0 if out_index_is_none else out_index.stride(0),
        out_index_is_none=out_index_is_none,
        dtype_id = dtype_to_type_id(out.dtype), allow_tf32=False, #torch.backends.cuda.matmul.allow_tf32
    )

    return out.view(*sel_shape, N)


@torch.library.register_fake("mylib::cvmm_triton", cvmm_triton)
def cvmm_triton_abstract(x, sel_idx, sel, keys, out_dtype, out_index):
    sel_shape = sel.shape
    sel = sel.flatten()
    M = sel.shape[0]
    O, K, N = keys.shape
    out = torch.empty((M, N), device=x.device, dtype=out_dtype)
    sel_shape = sel.shape
    return out.view(*sel_shape, N)


# torch.library.define("mylib::cvmm_triton_backward", "(Tensor x, Tensor sel_index, Tensor sel, Tensor grads, int n_experts, ScalarType key_dtype, bool op_float16, Tensor out_index) -> Tensor")

# @torch.library.impl("mylib::cvmm_triton_backward", "default")
def cvmm_triton_backward(
    x: torch.Tensor,
    sel_index: torch.Tensor,
    sel: torch.Tensor,
    grads: torch.Tensor,
    n_experts: int,
    key_dtype: torch.dtype,
    op_dtype: torch.dtype,
    out_index: torch.Tensor
):
    x = x.flatten(end_dim=-2)
    x = x.transpose(0, 1)
    grads = grads.flatten(end_dim=-2)
    sel = sel.flatten()
    M, _ = x.shape
    K, N = grads.shape
    # FIX: out must be atomic_add'able, which excludes bfloat16. Cast to key_dtype after. Maybe this could be f16
    out = torch.zeros((n_experts, M, N), device=x.device, dtype=torch.float32)
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), triton.cdiv(K, META['BLOCK_SIZE_K'] * META['K_BLOCKS'])
    )
    out_index_is_none = False
    if out_index.numel() == 1 and out_index == -1:
        out_index_is_none = True

    cvmm_backward_kernel3[grid](
        x, grads, out, sel_index, sel, out_index,
        M, N, K,
        x.stride(0), x.stride(1),
        grads.stride(0), grads.stride(1),
        out.stride(0), out.stride(1), out.stride(2),
        sel_index.stride(0), sel.stride(0), 0 if out_index_is_none else out_index.stride(0),
        out_index_is_none=out_index_is_none,
        out_dtype_id=dtype_to_type_id(out.dtype),
        dtype_id=dtype_to_type_id(op_dtype),
        allow_tf32=False #torch.backends.cuda.matmul.allow_tf32
    )
    return out.to(dtype=key_dtype)


class CVMM(torch.autograd.Function):
    warned = False

    @staticmethod
    def forward(
        ctx,
        x: torch.Tensor,
        sel_index: torch.Tensor,
        sel: torch.Tensor,
        keys: torch.Tensor,
        out_index: torch.Tensor | None = None,
        reduction_weight: torch.Tensor | None = None
    ):
        ctx.save_for_backward(x, keys, sel, sel_index, out_index, reduction_weight)

        # out_type = get_dtype()
        out_type = x.dtype
        # out_type = torch.float32
        if out_index is None:
            out_index = torch.tensor(-1).cuda()
        res = torch.ops.mylib.cvmm_triton(x, sel_index, sel, keys, out_type, out_index)

        if reduction_weight is not None:
            res = res.view(*reduction_weight.shape, res.shape[-1])
            res = (reduction_weight.unsqueeze(-2).type_as(res) @ res).squeeze(-2)

        ctx.op_type = out_type
        ctx.keys_type = keys.dtype
        ctx.dtype = out_type
        return res.type_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        x, keys, sel, sel_index, out_index, reduction_weight = ctx.saved_tensors
        keys_dt = keys

        # Backward for weight
        if reduction_weight is not None:
            # Project back the grads with he reduction weight, so the grad for the weight matrix is ok
            grad_output_w = reduction_weight.unsqueeze(-1).type_as(grad_output) @ grad_output.unsqueeze(-2)
        else:
            grad_output_w  = grad_output

        out_index_is_none = False
        if out_index is None:
            out_index_is_none = True
            out_index = torch.tensor(-1).cuda()

        grad_w = cvmm_triton_backward(
            x,
            sel_index,
            sel,
            grad_output_w,
            keys_dt.shape[0],
            ctx.keys_type,
            ctx.dtype,
            out_index=out_index
        )

        # Backward for input and reduction weight
        grad_w_off = None

        bw_index = sel_index if out_index_is_none else out_index
        bw_index_out = torch.tensor(-1).cuda()
        if reduction_weight is not None:
            # Hack the output indices to emulate repeats
            bw_index_out = bw_index
            bw_index = bw_index // reduction_weight.shape[-1]

        grad_x_full = torch.ops.mylib.cvmm_triton(
            grad_output,
            bw_index,
            sel,
            keys_dt.transpose(1,2),
            ctx.op_type,
            bw_index_out
        )

        grad_x_full = grad_x_full.view(*x.shape[:-1], -1, x.shape[-1])
        if reduction_weight is not None:
            # grad_x_full is the unscaled grad. For the input, we have to scale it, for the reduction wegiht,
            # we have to compute dot products with the input.
            grad_x = (reduction_weight.view(*grad_x_full.shape[:-1]).unsqueeze(-2).type_as(grad_x_full) @ grad_x_full).squeeze(-2)
            grad_w_off = (grad_x_full.type_as(reduction_weight) @ x.unsqueeze(-1).type_as(reduction_weight)).squeeze(-1).view_as(reduction_weight)
        elif grad_x_full.shape[-2] != 1:
            grad_x = grad_x_full.sum(-2)
        else:
            grad_x = grad_x_full

        grad_x = grad_x.view_as(x)

        return grad_x, None, None, grad_w, None, grad_w_off


def cvmm(x: torch.Tensor, sel: torch.Tensor | CVMMSel, keys: torch.Tensor):
    if not isinstance(sel, CVMMSel):
        sel = cvmm_prepare_sel(sel, keys.shape[0])
    assert x.dtype == keys.dtype, f"{x.dtype=} != {keys.dtype=}"

    return CVMM.apply(x, sel.sel_index, sel.sel, keys, sel.out_index, sel.reduction_weight)


def cvmm_prepare_sel2(sel: torch.Tensor, w: torch.Tensor | None = None) -> CVMMSel:
    # Has multiple selections for each batch element
    n_per_batch = sel.shape[-1]

    # indices = torch.arange(sel.nelement() // n_per_batch, device=sel.device, dtype=torch.int32)
    # indices = indices.repeat_interleave(n_per_batch).flatten()

    fsel = sel.flatten()
    ssel, sel_index = fsel.sort()

    # in_index = indices[sel_index]
    in_index = sel_index // n_per_batch

    return CVMMSel(sel, ssel.view_as(sel), in_index, sel_index, w)

#endregion
# -----------------------------------------------------------------------------
#region MoEUT
def log_mean(x: torch.Tensor, dim: int = 0):
    return x.logsumexp(dim) - torch.log(torch.tensor(x.shape[dim]))


def entropy_l(l: torch.Tensor) -> torch.Tensor:
    return - (l * l.exp()).sum(-1)


def entropy_reg(sel: torch.Tensor, dim: int) -> torch.Tensor:
    sel = F.log_softmax(sel, dim=-1)
    sel = log_mean(sel, dim)
    return - entropy_l(sel).mean()


class SigmaMoE(torch.nn.Module):
    def __init__(self, dmodel: int, n_experts: int, expert_size: int, k: int,
                 activation=F.relu,
                 v_dim: int | None = None,
                 expert_dropout: float = 0.0):

        super().__init__()
        self.k_dim = dmodel
        self.v_dim = v_dim if v_dim is not None else dmodel
        self.n_experts = n_experts
        self.expert_size = expert_size
        self.size = self.n_experts * self.expert_size
        self.k_vec_dim = self.k_dim
        self.num_heads = k
        self.activation = activation
        self.expert_dropout = expert_dropout

        self.sel_hist = []

        self.keys = torch.nn.Parameter(torch.empty(self.n_experts, self.k_vec_dim, self.expert_size))
        self.values = torch.nn.Parameter(torch.empty(self.n_experts, self.expert_size, self.v_dim))
        self.expert_sel = torch.nn.Parameter(torch.empty(self.n_experts, self.k_vec_dim))

    @torch.no_grad
    def reset_parameters(self, std_scale: float):
        torch.nn.init.normal_(self.expert_sel, 0, std_scale / (self.k_dim) ** 0.5)
        torch.nn.init.normal_(self.keys, 0, std_scale / (self.k_dim) ** 0.5)
        torch.nn.init.normal_(self.values, 0, std_scale / (self.n_experts * self.expert_size) ** 0.5)

        # self.renorm_keep_std(self.expert_sel, dim=1)
        std = self.expert_sel.std()
        self.expert_sel.div_(self.expert_sel.norm(dim=1, keepdim=True))
        self.expert_sel.mul_(std / self.expert_sel.std())

    def get_reg_loss(self) -> torch.Tensor:
        if not self.sel_hist:
            return 0

        # Average over time and layers.
        loss = entropy_reg(torch.stack(self.sel_hist, dim=-2).flatten(-3, -2), -2)
        self.sel_hist = []
        return loss

    def forward(self, input: torch.Tensor, sel_input: torch.Tensor | None = None) -> tuple[torch.Tensor, torch.Tensor]:
        # Selection score calculation
        sel = F.linear(sel_input if sel_input is not None else input, self.expert_sel, None)
        if self.training:
            self.sel_hist.append(sel)

        # Selection activation and topk
        sel = F.sigmoid(sel)

        if self.training and self.expert_dropout > 0:
            mask = torch.rand_like(sel) < self.expert_dropout
            sel = sel.masked_fill(mask, float("-inf"))

        sel_val, sel_index = sel.topk(self.num_heads, dim=-1, sorted=False)

        # Preprocess the selection indices. They will be needed for both layers and save some time
        sel_indices = cvmm_prepare_sel2(sel_index.int())

        # "Up-projection" layer for each head
        scores = cvmm(input, sel_indices, self.keys)
        scores = self.activation(scores)

        # Down projection layer for each head
        sel_indices = sel_indices.clone()
        sel_indices.reduction_weight = sel_val
        sel_indices.sel_index = sel_indices.out_index
        sel_indices.out_index = None

        out = cvmm(scores, sel_indices, self.values)

        res = out.view(*input.shape[:-1], self.v_dim)
        return res


class SwitchHeadRoPE(torch.nn.Module):
    def __init__(self, state_size: int, num_heads: int, n_experts: int, max_seq_len: int,
                 head_dim: int | None = None, expert_dropout: float = 0.0, moe_k: int = 2
                 ):

        super().__init__()

        self.input_size = state_size
        self.output_size = state_size
        self.pe_size = self.input_size
        self.expert_dropout = expert_dropout
        self.moe_k = moe_k
        self.attention_to_visualize = []
        self.selections_to_visualize = {}
        self.n_experts = n_experts

        self.sel_hist = []

        self.num_heads = num_heads
        self.head_dim = head_dim or (state_size // num_heads)
        self.rotary = Rotary(self.head_dim, max_seq_len)

        self.q = torch.nn.Linear(self.input_size, self.head_dim * self.num_heads, bias=False)
        self.k = torch.nn.Linear(self.input_size, self.head_dim * self.num_heads, bias=False)

        if self.n_experts > 1:
            self.v = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size, self.head_dim))
            self.o = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.head_dim, self.output_size))
            self.sel_v = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size))
        else:
            self.v = torch.nn.Parameter(torch.empty(self.num_heads * self.head_dim, self.input_size))
            self.o = torch.nn.Parameter(torch.empty(self.output_size, self.num_heads * self.head_dim))

        self.sel_o = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size))

        self.register_buffer("scale", torch.full([1], 1.0 / (self.head_dim) ** 0.5), persistent=False)

    @torch.no_grad
    def reset_parameters(self, std_scale: float):
        def renorm_rows(x: torch.Tensor):
            std_t = x.std(dim=-1, keepdim=True)
            x.div_(x.norm(dim=-1, keepdim=True))
            x.mul_(std_t / x.std())

        if self.n_experts > 1:
            torch.nn.init.normal_(self.sel_v, 0, std_scale / (self.input_size) ** 0.5)
            renorm_rows(self.sel_v)

        torch.nn.init.normal_(self.sel_o, 0, std_scale / (self.input_size) ** 0.5)
        renorm_rows(self.sel_o)

        torch.nn.init.normal_(self.k.weight, 0, std_scale / (self.input_size) ** 0.5)
        torch.nn.init.normal_(self.q.weight, 0, std_scale / (self.input_size) ** 0.5)
        torch.nn.init.normal_(self.v, 0, std_scale / (self.input_size) ** 0.5)
        torch.nn.init.normal_(self.o, 0, std_scale / (self.num_heads * self.head_dim) ** 0.5)


    def project_to_torch_order(self, x: torch.Tensor):
        return x.view(*x.shape[:-1], self.num_heads, -1).transpose(-2, -3)


    def get_sel(self, t: torch.Tensor, w: torch.Tensor) -> tuple[CVMMSel, torch.Tensor]:
        sel = F.linear(t, w).float()
        sel = sel_raw = sel.view(*sel.shape[:-1], self.num_heads, -1)
        sel = sel.sigmoid()

        with torch.no_grad():
            if self.expert_dropout > 0 and self.training:
                mask = torch.rand_like(sel) < self.expert_dropout
                sel2 = sel.masked_fill(mask, float('-inf'))
            else:
                sel2 = sel
            _, sel_index = sel2.topk(self.moe_k, dim=-1, sorted=False)
        sel_val = torch.gather(sel, -1, sel_index)

        sel_index_shifted = (torch.arange(self.num_heads, device=sel_index.device, dtype=sel_index.dtype) * self.n_experts).unsqueeze(-1) + sel_index
        return cvmm_prepare_sel2(sel_index_shifted.flatten(-2,-1), sel_val), sel_raw

    def get_reg_loss(self) -> torch.Tensor:
        loss = 0
        if self.sel_hist:
            for i in range(len(self.sel_hist[0])):
                loss = loss + entropy_reg(torch.stack([l[i] for l in self.sel_hist], dim=-3).flatten(-4,-3), -3)
        self.sel_hist = []
        return loss

    def forward(self, q_src: torch.Tensor, k_src: torch.Tensor, v_src: torch.Tensor, ve: torch.Tensor | None, block_mask: BlockMask) -> torch.Tensor:
        # *src: [batch_size, out_len, c]
        B, T = q_src.size(0), q_src.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"

        pos_offset = q_src.shape[1] - k_src.shape[1]
        assert pos_offset >= 0

        scale = self.scale.sqrt()

        q = self.q(q_src)
        k = self.k(k_src)
        q = q * scale.type_as(q)
        k = k * scale.type_as(k)

        if self.n_experts > 1:
            v_sel, v_sel_r = self.get_sel(k_src, self.sel_v)
            o_sel, o_sel_r = self.get_sel(q_src, self.sel_o)
            if self.training:
                self.sel_hist.append((o_sel_r, v_sel_r))

            v = cvmm(v_src, v_sel, self.v).transpose(-2, -3)
        else:
            o_gate = F.sigmoid(F.linear(q_src, self.sel_o))
            # v = self.project_to_torch_order(F.linear(v_src, self.v))
            # return x.view(*x.shape[:-1], self.num_heads, -1).transpose(-2, -3)
            v = F.linear(v_src, self.v).view(B, T, self.num_heads, self.head_dim).transpose(1,2)

        q = q.view(B, T, self.num_heads, self.head_dim)
        k = k.view(B, T, self.num_heads, self.head_dim)
        v = v.type_as(q)

        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q).transpose(1,2), self.rotary(k).transpose(1,2)
        # if ve is not None:
        #     v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        # else: # skip mid-layers token value embeddings by @YouJiacheng
        #     v = self.lambdas[0] * v

        res = flex_attention(q, k, v, block_mask=block_mask, scale=0.12)
        res = res.transpose(1, 2)

        if self.n_experts > 1:
            # The output selection indices are calculated from the current state and are also used for projecting "q".
            # But that projection needs to create multiple copies for the different heads. Here we already have the
            # heads, but we have to create copies for the top-k elements. We can calculate that from the reduction
            # weight. We also want to compute not only the weighted average between the top-k elements, but also
            # of the different heads. So reshape the reduction weight accordingly.
            o_sel.sel_index = o_sel.out_index // o_sel.reduction_weight.shape[-1]
            o_sel.reduction_weight = o_sel.reduction_weight.flatten(-2)
            out = cvmm(res, o_sel, self.o)
        else:
            res = res * o_gate[..., None]
            out = F.linear(res.flatten(-2), self.o)

        return out


class MoEUTLayer(torch.nn.Module):
    def __init__(self, d_model: int, num_heads: int, ff_expert_size: int, ff_n_experts: int,
                 att_n_experts: int, max_seq_len: int, head_dim: int | None = None, att_k: int = 2,
                 ff_k: int = 8, ff_expert_dropout: float = 0.0, att_expert_dropout: float = 0.0):

        super().__init__()
        self.attention = SwitchHeadRoPE(
            d_model, num_heads, att_n_experts, max_seq_len=max_seq_len, head_dim=head_dim, moe_k=att_k,
            expert_dropout=att_expert_dropout)
        self.ffn = SigmaMoE(d_model, ff_n_experts, ff_expert_size, k=ff_k, expert_dropout=ff_expert_dropout)
        self.ln1 = torch.nn.LayerNorm(d_model)
        self.ln2 = torch.nn.LayerNorm(d_model)

    def forward(self, x: torch.Tensor, ve: torch.Tensor | None, block_mask: BlockMask) -> torch.Tensor:
        xnorm = self.ln1(x)
        x = x + self.attention(xnorm, xnorm, x, ve, block_mask)
        upd = self.ffn(x, self.ln2(x))
        return x + upd


class MoEUT(torch.nn.Module):
    def __init__(self, d_model: int, n_layers: int, num_heads: int, ff_expert_size: int, ff_n_experts: int,
                 att_n_experts: int, max_seq_len: int, head_dim: int | None = None, att_k: int = 2,
                 ff_k: int = 8, ff_expert_dropout: float = 0.0, att_expert_dropout: float = 0.0,
                 entropy_reg: float = 0.01, att_entropy_reg: float = 0.001,
                 group_size: int = 2):
        super().__init__()

        self.entropy_reg = entropy_reg
        self.att_entropy_reg = att_entropy_reg

        self.n_repeats = n_layers // group_size
        self.layers = torch.nn.ModuleList([
            MoEUTLayer(d_model, num_heads, ff_expert_size, ff_n_experts, att_n_experts,
                       max_seq_len, head_dim, att_k, ff_k,
                       ff_expert_dropout, att_expert_dropout)
            for _ in range(group_size)
        ])

        self.reset_parameters()

    def forward(self, x: torch.Tensor, block_mask: BlockMask) -> tuple[torch.Tensor, torch.Tensor]:
        # Run the model
        for r in range(self.n_repeats):
            for layer in self.layers:
                x = layer(x, ve=None, block_mask=block_mask)

        # Collect regularizaiton losses. Must be at the end because it is across the layers.
        reg_loss = torch.zeros(1, device=x.device, dtype=torch.float32)
        for layer in self.modules():
            if isinstance(layer, SigmaMoE):
                reg_loss = reg_loss + self.entropy_reg * layer.get_reg_loss()
            elif isinstance(layer, SwitchHeadRoPE):
                reg_loss = reg_loss + self.att_entropy_reg * layer.get_reg_loss()

        return x, reg_loss

    @torch.no_grad
    def reset_parameters(self):
        scale = (2 / (self.n_repeats * len(self.layers))) ** 0.5
        for layer in self.modules():
            if isinstance(layer, (SwitchHeadRoPE, SigmaMoE)):
                layer.reset_parameters(scale)
            elif isinstance(layer, torch.nn.LayerNorm):
                torch.nn.init.ones_(layer.weight)
                torch.nn.init.zeros_(layer.bias)


class MoEUTWrapper(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.moeut = MoEUT(
            d_model=model_dim,
            n_layers=num_layers,
            num_heads=num_heads,
            ff_expert_size=128,
            ff_n_experts=64,  # FIXME: Arbitrary decision
            att_n_experts=10, # !! From MoEUT paper, but they also used really weird numbers like d_head=41 so I don't trust their judgement
            max_seq_len=max_seq_len,
            head_dim=None,
            att_k=2,
            ff_k=8,
            ff_expert_dropout=0.0,
            att_expert_dropout=0.0,
            entropy_reg=0.01,
            att_entropy_reg=0.001,
            group_size=2,
        )
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977


    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, _ = create_block_masks(input_seq, sliding_window_num_blocks)

        x = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x, reg_loss = self.moeut(x, long_bm)
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        if self.training:
            loss = loss + reg_loss
        return loss


#endregion
# -----------------------------------------------------------------------------
#region Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets
#endregion
# -----------------------------------------------------------------------------
#region utils, hyperparams
def print0(s, console=True):
    if master_process:
        timestamp = time.strftime("%H:%M:%S.") + f"{time.time() % 1:.3f}"[2:]
        s = f"{timestamp}: {s}"
        if console:
            print(s)
        if logfile:
            with open(logfile, "a") as f:
                print(s, file=f)

def log_mem():
    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )

@dataclass(frozen=True, kw_only=True)
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations: int = 1770 # number of iterations to run
    cooldown_frac: float = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len: int = 48*1024 # FlexAttention sequence length
    val_seq_len: int = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint: bool = False
    dev: bool = False

TEST_HPARAMS = Hyperparameters(
    train_files = "data/fineweb1B/fineweb_train_*.bin",
    val_files = "data/fineweb1B/fineweb_val_*.bin",
    val_tokens = 1048576,
    num_iterations = 1000, #770,
    cooldown_frac = 0.4,
    val_loss_every = 125,
    seq_len = 16*1024,
    val_seq_len = 4*16*1024,
    save_checkpoint = False,
)
DEV_HPARAMS = Hyperparameters(
    train_files = "data/fineweb1B/fineweb_train_*.bin",
    val_files = "data/fineweb1B/fineweb_val_*.bin",
    val_tokens = 1024,
    num_iterations = 10, #770,
    cooldown_frac = 0.4,
    val_loss_every = 125,
    seq_len = 512,
    val_seq_len = 512,
    save_checkpoint = False,
    dev=True,
)

#endregion
# -----------------------------------------------------------------------------
#region main()
master_process = None
logfile = None
def main(args = TEST_HPARAMS):
# def main(args = DEV_HPARAMS):
    global master_process, logfile
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    atexit.register(dist.destroy_process_group)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    if master_process and not args.dev:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)


    # begin by printing this file (the Python code)
    print0(code, console=False)
    print0("="*100, console=False)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}", console=False)
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}", console=False)
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi(), console=False)
    print0("="*100, console=False)
    atexit.register(log_mem)

    torch.random.manual_seed(0)
    torch.cuda.synchronize()
    print0("Init data")
    # load data
    train_batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

    torch.cuda.synchronize()
    print0("Init model")
    # REF: model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    model: nn.Module = MoEUTWrapper(vocab_size=50257, num_layers=12, num_heads=3, model_dim=384, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    model.bfloat16()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()

    # count parameters
    n_params_by_dtype = defaultdict(lambda: 0)
    for name, param in model.named_parameters():
        dist.broadcast(param.detach(), 0)
        n_params_by_dtype[param.dtype] += param.numel()
    for dt, n_params in n_params_by_dtype.items():
        print0(f"{dt}: {n_params/1024/1024:.3f}Mi params")
    print0(f"total: {sum(n_params_by_dtype.values())/1024/1024:.3f}Mi params")


    torch.cuda.synchronize()
    print0("Init optimizers")
    # collect the parameters to optimize
    hidden_matrix_params = [p for n, p in model.named_parameters() if p.ndim >= 2 and "embed" not in n and "lm_head" not in n]
    embed_params = [p for n, p in model.named_parameters() if "embed" in n]
    scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    lr_mod = (args.seq_len/Hyperparameters().seq_len) ** 0.5  # Correct LR based on difference in batch size vs 4
    print(f"{lr_mod=}")
    adam_params = [dict(params=head_params, lr=0.008*lr_mod), dict(params=embed_params, lr=0.6*lr_mod), dict(params=scalar_params, lr=0.04*lr_mod)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05*lr_mod, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(step: int):
        t = 1 - step / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    if not args.dev:
        model: nn.Module = torch.compile(model) #, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    print0("Starting train loop")
    train_steps = args.num_iterations
    prof = None
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # if step == 5:
        #     prof = profile(record_shapes=True, profile_memory=True, with_stack=True)
        #     prof.__enter__()
        #     prof.start()
        # if prof is not None:
        #     if step == 9:
        #         prof.__exit__(None, None, None)
        #         prof.export_chrome_trace("trace.json")
        #         prof = None
        #     else:
        #         prof.step()

        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_batch_size = world_size * args.val_seq_len
            assert args.val_tokens % val_batch_size == 0
            val_steps = args.val_tokens // val_batch_size
            val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for i in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION -----------------
        inputs, targets = next(train_loader)
        train_losses = []
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            loss = model(input_seq, target_seq, sw_num_blks(window_size))
            loss.backward()
            dist.all_reduce(loss, op=dist.ReduceOp.AVG)
            train_losses.append(loss.item())
            del loss
        train_loss = sum(train_losses or [torch.nan]) / max(len(train_losses), 1)
        for param in model.parameters():
            if param.grad is not None:
                dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        del param
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)

        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_loss:{train_loss:.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms {torch.cuda.memory_allocated()=}", console=True)

    if master_process and logfile is not None:
        new_logfile = input("Name run? ")
        if new_logfile:
            old_logfile = logfile
            logfile = f"logs/{new_logfile}.txt"
            os.rename(old_logfile, logfile)
            print0("Renamed {old_logfile} -> {logfile}")
    else:
        print(logfile)
#endregion
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    main()

13:59:47.367: ====================================================================================================
13:59:47.367: Running Python 3.12.7 (main, Oct 16 2024, 04:37:19) [Clang 18.1.8 ]
13:59:47.368: Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
13:59:47.453: Sun Feb  2 13:59:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090 Ti     On  |   00000000:2D:00.0 Off |                  Off |
|  0%   41C    P2             93W /  450W |     882MiB /  24564MiB |      6%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A        26      G   /Xwayland                                   N/A      |
|    0   N/A  N/A    248516      C   /python3.12                                 N/A      |
+-----------------------------------------------------------------------------------------+

13:59:47.453: ====================================================================================================
13:59:47.453: Init data
13:59:47.454: Init model
13:59:47.909: torch.bfloat16: 55.108Mi params
13:59:47.909: total: 55.108Mi params
13:59:47.910: Init optimizers
13:59:47.922: Starting train loop
14:02:28.441: step:0/1000 val_loss:10.8258 train_time:0ms step_avg:nanms
14:03:30.141: step:1/1000 train_loss:10.7336 train_time:61700ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:30.496: step:2/1000 train_loss:10.7345 train_time:62055ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:30.729: step:3/1000 train_loss:9.8277 train_time:62288ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:30.963: step:4/1000 train_loss:8.8574 train_time:62522ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:31.199: step:5/1000 train_loss:8.0422 train_time:62757ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:31.434: step:6/1000 train_loss:7.6413 train_time:62993ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:31.667: step:7/1000 train_loss:7.6815 train_time:63226ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:31.900: step:8/1000 train_loss:7.4711 train_time:63459ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:32.133: step:9/1000 train_loss:7.3313 train_time:63692ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:32.368: step:10/1000 train_loss:7.7543 train_time:63927ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:32.602: step:11/1000 train_loss:7.3694 train_time:234ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:32.836: step:12/1000 train_loss:7.0067 train_time:468ms step_avg:nanms torch.cuda.memory_allocated()=369581568
14:03:33.071: step:13/1000 train_loss:6.3576 train_time:703ms step_avg:234.47ms torch.cuda.memory_allocated()=369581568
14:03:33.305: step:14/1000 train_loss:7.0733 train_time:937ms step_avg:234.35ms torch.cuda.memory_allocated()=369581568
14:03:33.540: step:15/1000 train_loss:7.4664 train_time:1172ms step_avg:234.35ms torch.cuda.memory_allocated()=369581568
14:03:33.774: step:16/1000 train_loss:8.1205 train_time:1406ms step_avg:234.33ms torch.cuda.memory_allocated()=369581568
14:03:34.008: step:17/1000 train_loss:7.6622 train_time:1640ms step_avg:234.34ms torch.cuda.memory_allocated()=369581568
14:03:34.242: step:18/1000 train_loss:7.8589 train_time:1874ms step_avg:234.24ms torch.cuda.memory_allocated()=369581568
14:03:34.476: step:19/1000 train_loss:7.2646 train_time:2108ms step_avg:234.18ms torch.cuda.memory_allocated()=369581568
14:03:34.709: step:20/1000 train_loss:7.2887 train_time:2342ms step_avg:234.16ms torch.cuda.memory_allocated()=369581568
14:03:34.943: step:21/1000 train_loss:7.1954 train_time:2575ms step_avg:234.12ms torch.cuda.memory_allocated()=369581568
14:03:35.177: step:22/1000 train_loss:7.1868 train_time:2809ms step_avg:234.07ms torch.cuda.memory_allocated()=369581568
14:03:35.409: step:23/1000 train_loss:6.9996 train_time:3042ms step_avg:233.97ms torch.cuda.memory_allocated()=369581568
14:03:35.644: step:24/1000 train_loss:6.8269 train_time:3276ms step_avg:234.02ms torch.cuda.memory_allocated()=369581568
14:03:35.877: step:25/1000 train_loss:6.8718 train_time:3509ms step_avg:233.95ms torch.cuda.memory_allocated()=369581568
14:03:36.111: step:26/1000 train_loss:6.8432 train_time:3743ms step_avg:233.92ms torch.cuda.memory_allocated()=369581568
14:03:36.347: step:27/1000 train_loss:7.0131 train_time:3979ms step_avg:234.05ms torch.cuda.memory_allocated()=369581568
14:03:36.579: step:28/1000 train_loss:7.1814 train_time:4211ms step_avg:233.97ms torch.cuda.memory_allocated()=369581568
14:03:36.813: step:29/1000 train_loss:7.2197 train_time:4445ms step_avg:233.93ms torch.cuda.memory_allocated()=369581568
14:03:37.049: step:30/1000 train_loss:6.8532 train_time:4681ms step_avg:234.04ms torch.cuda.memory_allocated()=369581568
14:03:37.282: step:31/1000 train_loss:6.9333 train_time:4914ms step_avg:234.01ms torch.cuda.memory_allocated()=369581568
14:03:37.516: step:32/1000 train_loss:6.6741 train_time:5148ms step_avg:233.99ms torch.cuda.memory_allocated()=369581568
14:03:37.749: step:33/1000 train_loss:6.8452 train_time:5381ms step_avg:233.96ms torch.cuda.memory_allocated()=369581568
14:03:37.983: step:34/1000 train_loss:6.5798 train_time:5615ms step_avg:233.95ms torch.cuda.memory_allocated()=369581568
14:03:38.217: step:35/1000 train_loss:7.0133 train_time:5849ms step_avg:233.97ms torch.cuda.memory_allocated()=369581568
14:03:38.450: step:36/1000 train_loss:6.7428 train_time:6082ms step_avg:233.94ms torch.cuda.memory_allocated()=369581568
14:03:38.685: step:37/1000 train_loss:6.7626 train_time:6317ms step_avg:233.97ms torch.cuda.memory_allocated()=369581568
14:03:38.919: step:38/1000 train_loss:6.6137 train_time:6551ms step_avg:233.98ms torch.cuda.memory_allocated()=369581568
14:03:39.153: step:39/1000 train_loss:6.8331 train_time:6785ms step_avg:233.95ms torch.cuda.memory_allocated()=369581568
14:03:39.386: step:40/1000 train_loss:6.5531 train_time:7018ms step_avg:233.95ms torch.cuda.memory_allocated()=369581568
14:03:39.620: step:41/1000 train_loss:6.5993 train_time:7252ms step_avg:233.93ms torch.cuda.memory_allocated()=369581568
14:03:39.854: step:42/1000 train_loss:6.4711 train_time:7486ms step_avg:233.95ms torch.cuda.memory_allocated()=369581568
14:03:40.088: step:43/1000 train_loss:6.6237 train_time:7720ms step_avg:233.94ms torch.cuda.memory_allocated()=369581568
14:03:40.320: step:44/1000 train_loss:6.3024 train_time:7953ms step_avg:233.90ms torch.cuda.memory_allocated()=369581568
14:03:40.553: step:45/1000 train_loss:7.3094 train_time:8185ms step_avg:233.85ms torch.cuda.memory_allocated()=369581568
14:03:40.785: step:46/1000 train_loss:6.5005 train_time:8417ms step_avg:233.81ms torch.cuda.memory_allocated()=369581568
14:03:41.020: step:47/1000 train_loss:6.5937 train_time:8653ms step_avg:233.85ms torch.cuda.memory_allocated()=369581568
14:03:41.255: step:48/1000 train_loss:6.9119 train_time:8887ms step_avg:233.87ms torch.cuda.memory_allocated()=369581568
14:03:41.490: step:49/1000 train_loss:6.6104 train_time:9122ms step_avg:233.89ms torch.cuda.memory_allocated()=369581568
14:03:41.723: step:50/1000 train_loss:6.6614 train_time:9355ms step_avg:233.86ms torch.cuda.memory_allocated()=369581568
14:03:41.957: step:51/1000 train_loss:6.4906 train_time:9589ms step_avg:233.88ms torch.cuda.memory_allocated()=369581568
14:03:42.191: step:52/1000 train_loss:6.7765 train_time:9823ms step_avg:233.87ms torch.cuda.memory_allocated()=369581568
14:03:42.426: step:53/1000 train_loss:6.1021 train_time:10058ms step_avg:233.91ms torch.cuda.memory_allocated()=369581568
14:03:42.660: step:54/1000 train_loss:6.4704 train_time:10292ms step_avg:233.90ms torch.cuda.memory_allocated()=369581568
14:03:42.893: step:55/1000 train_loss:7.0085 train_time:10525ms step_avg:233.89ms torch.cuda.memory_allocated()=369581568
14:03:43.127: step:56/1000 train_loss:6.3377 train_time:10759ms step_avg:233.89ms torch.cuda.memory_allocated()=369581568
14:03:43.360: step:57/1000 train_loss:6.6005 train_time:10992ms step_avg:233.87ms torch.cuda.memory_allocated()=369581568
14:03:43.592: step:58/1000 train_loss:6.5326 train_time:11224ms step_avg:233.84ms torch.cuda.memory_allocated()=369581568
14:03:43.826: step:59/1000 train_loss:6.2594 train_time:11458ms step_avg:233.84ms torch.cuda.memory_allocated()=369581568
14:03:44.060: step:60/1000 train_loss:6.6514 train_time:11692ms step_avg:233.83ms torch.cuda.memory_allocated()=369581568
14:03:44.293: step:61/1000 train_loss:6.8572 train_time:11925ms step_avg:233.83ms torch.cuda.memory_allocated()=369581568
14:03:44.527: step:62/1000 train_loss:6.7323 train_time:12159ms step_avg:233.84ms torch.cuda.memory_allocated()=369581568
14:03:44.760: step:63/1000 train_loss:6.6313 train_time:12392ms step_avg:233.82ms torch.cuda.memory_allocated()=369581568
14:03:44.993: step:64/1000 train_loss:6.5584 train_time:12625ms step_avg:233.80ms torch.cuda.memory_allocated()=369581568
14:03:45.227: step:65/1000 train_loss:6.6852 train_time:12859ms step_avg:233.79ms torch.cuda.memory_allocated()=369581568
14:03:45.460: step:66/1000 train_loss:6.3191 train_time:13092ms step_avg:233.79ms torch.cuda.memory_allocated()=369581568
14:03:45.693: step:67/1000 train_loss:6.7863 train_time:13325ms step_avg:233.78ms torch.cuda.memory_allocated()=369581568
14:03:45.926: step:68/1000 train_loss:6.3809 train_time:13559ms step_avg:233.77ms torch.cuda.memory_allocated()=369581568
14:03:46.161: step:69/1000 train_loss:6.2271 train_time:13793ms step_avg:233.78ms torch.cuda.memory_allocated()=369581568
14:03:46.397: step:70/1000 train_loss:6.3480 train_time:14029ms step_avg:233.81ms torch.cuda.memory_allocated()=369581568
14:03:46.629: step:71/1000 train_loss:6.6285 train_time:14261ms step_avg:233.79ms torch.cuda.memory_allocated()=369581568
14:03:46.863: step:72/1000 train_loss:6.4659 train_time:14495ms step_avg:233.80ms torch.cuda.memory_allocated()=369581568
14:03:47.096: step:73/1000 train_loss:6.3697 train_time:14728ms step_avg:233.78ms torch.cuda.memory_allocated()=369581568
14:03:47.330: step:74/1000 train_loss:6.5143 train_time:14962ms step_avg:233.79ms torch.cuda.memory_allocated()=369581568
14:03:47.563: step:75/1000 train_loss:6.1170 train_time:15195ms step_avg:233.77ms torch.cuda.memory_allocated()=369581568
14:03:47.802: step:76/1000 train_loss:6.3407 train_time:15434ms step_avg:233.85ms torch.cuda.memory_allocated()=369581568
14:03:48.043: step:77/1000 train_loss:6.2695 train_time:15675ms step_avg:233.95ms torch.cuda.memory_allocated()=369581568
14:03:48.282: step:78/1000 train_loss:6.1040 train_time:15914ms step_avg:234.03ms torch.cuda.memory_allocated()=369581568
14:03:48.523: step:79/1000 train_loss:6.3816 train_time:16155ms step_avg:234.14ms torch.cuda.memory_allocated()=369581568
14:03:48.763: step:80/1000 train_loss:6.1246 train_time:16395ms step_avg:234.21ms torch.cuda.memory_allocated()=369581568
14:03:48.003: step:81/1000 train_loss:6.0978 train_time:16635ms step_avg:234.30ms torch.cuda.memory_allocated()=369581568
14:03:49.242: step:82/1000 train_loss:6.1934 train_time:16874ms step_avg:234.36ms torch.cuda.memory_allocated()=369581568
14:03:49.483: step:83/1000 train_loss:6.4825 train_time:17115ms step_avg:234.45ms torch.cuda.memory_allocated()=369581568
14:03:49.723: step:84/1000 train_loss:6.8204 train_time:17355ms step_avg:234.52ms torch.cuda.memory_allocated()=369581568
14:03:49.964: step:85/1000 train_loss:6.1752 train_time:17596ms step_avg:234.61ms torch.cuda.memory_allocated()=369581568
14:03:50.204: step:86/1000 train_loss:6.4395 train_time:17836ms step_avg:234.68ms torch.cuda.memory_allocated()=369581568
14:03:50.442: step:87/1000 train_loss:6.3708 train_time:18074ms step_avg:234.73ms torch.cuda.memory_allocated()=369581568
14:03:50.683: step:88/1000 train_loss:6.2588 train_time:18315ms step_avg:234.81ms torch.cuda.memory_allocated()=369581568
14:03:50.924: step:89/1000 train_loss:6.2049 train_time:18556ms step_avg:234.89ms torch.cuda.memory_allocated()=369581568
14:03:51.163: step:90/1000 train_loss:6.3335 train_time:18795ms step_avg:234.94ms torch.cuda.memory_allocated()=369581568
14:03:51.406: step:91/1000 train_loss:6.1944 train_time:19039ms step_avg:235.04ms torch.cuda.memory_allocated()=369581568
14:03:51.648: step:92/1000 train_loss:6.4828 train_time:19280ms step_avg:235.12ms torch.cuda.memory_allocated()=369581568
14:03:51.888: step:93/1000 train_loss:7.0080 train_time:19520ms step_avg:235.18ms torch.cuda.memory_allocated()=369581568
14:03:52.128: step:94/1000 train_loss:6.1483 train_time:19760ms step_avg:235.24ms torch.cuda.memory_allocated()=369581568
14:03:52.368: step:95/1000 train_loss:6.3074 train_time:20000ms step_avg:235.30ms torch.cuda.memory_allocated()=369581568
14:03:52.607: step:96/1000 train_loss:5.9448 train_time:20239ms step_avg:235.34ms torch.cuda.memory_allocated()=369581568
14:03:52.847: step:97/1000 train_loss:6.0227 train_time:20480ms step_avg:235.40ms torch.cuda.memory_allocated()=369581568
14:03:53.087: step:98/1000 train_loss:6.3060 train_time:20719ms step_avg:235.45ms torch.cuda.memory_allocated()=369581568
14:03:53.328: step:99/1000 train_loss:5.7397 train_time:20960ms step_avg:235.50ms torch.cuda.memory_allocated()=369581568
14:03:53.567: step:100/1000 train_loss:6.2130 train_time:21199ms step_avg:235.55ms torch.cuda.memory_allocated()=369581568
14:03:53.806: step:101/1000 train_loss:6.1196 train_time:21438ms step_avg:235.58ms torch.cuda.memory_allocated()=369581568
14:03:54.046: step:102/1000 train_loss:5.8958 train_time:21678ms step_avg:235.63ms torch.cuda.memory_allocated()=369581568
14:03:54.287: step:103/1000 train_loss:6.0125 train_time:21919ms step_avg:235.69ms torch.cuda.memory_allocated()=369581568
14:03:54.527: step:104/1000 train_loss:6.0234 train_time:22159ms step_avg:235.74ms torch.cuda.memory_allocated()=369581568
14:03:54.766: step:105/1000 train_loss:6.0787 train_time:22398ms step_avg:235.77ms torch.cuda.memory_allocated()=369581568
14:03:55.008: step:106/1000 train_loss:6.3807 train_time:22640ms step_avg:235.83ms torch.cuda.memory_allocated()=369581568
14:03:55.248: step:107/1000 train_loss:6.1237 train_time:22880ms step_avg:235.88ms torch.cuda.memory_allocated()=369581568
14:03:55.488: step:108/1000 train_loss:6.1814 train_time:23120ms step_avg:235.92ms torch.cuda.memory_allocated()=369581568
14:03:55.728: step:109/1000 train_loss:6.1492 train_time:23360ms step_avg:235.96ms torch.cuda.memory_allocated()=369581568
14:03:55.969: step:110/1000 train_loss:5.7263 train_time:23601ms step_avg:236.01ms torch.cuda.memory_allocated()=369581568
14:03:56.208: step:111/1000 train_loss:6.0352 train_time:23841ms step_avg:236.04ms torch.cuda.memory_allocated()=369581568
14:03:56.448: step:112/1000 train_loss:6.1976 train_time:24080ms step_avg:236.08ms torch.cuda.memory_allocated()=369581568
14:03:56.689: step:113/1000 train_loss:5.9052 train_time:24321ms step_avg:236.12ms torch.cuda.memory_allocated()=369581568
14:03:56.929: step:114/1000 train_loss:5.9224 train_time:24561ms step_avg:236.16ms torch.cuda.memory_allocated()=369581568
14:03:57.169: step:115/1000 train_loss:5.8590 train_time:24801ms step_avg:236.20ms torch.cuda.memory_allocated()=369581568
14:03:57.412: step:116/1000 train_loss:6.0596 train_time:25044ms step_avg:236.26ms torch.cuda.memory_allocated()=369581568
14:03:57.653: step:117/1000 train_loss:6.1320 train_time:25285ms step_avg:236.31ms torch.cuda.memory_allocated()=369581568
14:03:57.892: step:118/1000 train_loss:5.9493 train_time:25525ms step_avg:236.34ms torch.cuda.memory_allocated()=369581568
14:03:58.134: step:119/1000 train_loss:5.8175 train_time:25766ms step_avg:236.38ms torch.cuda.memory_allocated()=369581568
14:03:58.374: step:120/1000 train_loss:5.6695 train_time:26006ms step_avg:236.42ms torch.cuda.memory_allocated()=369581568
14:03:58.615: step:121/1000 train_loss:6.4760 train_time:26247ms step_avg:236.46ms torch.cuda.memory_allocated()=369581568
14:03:58.855: step:122/1000 train_loss:6.3037 train_time:26487ms step_avg:236.49ms torch.cuda.memory_allocated()=369581568
14:03:59.094: step:123/1000 train_loss:6.0884 train_time:26726ms step_avg:236.52ms torch.cuda.memory_allocated()=369581568
14:03:59.334: step:124/1000 train_loss:6.0362 train_time:26966ms step_avg:236.54ms torch.cuda.memory_allocated()=369581568
14:03:59.575: step:125/1000 train_loss:6.0760 train_time:27207ms step_avg:236.59ms torch.cuda.memory_allocated()=369581568
14:04:02.505: step:125/1000 val_loss:6.1935 train_time:27208ms step_avg:236.59ms
14:04:02.747: step:126/1000 train_loss:6.4814 train_time:27449ms step_avg:236.63ms torch.cuda.memory_allocated()=369581568
14:04:02.995: step:127/1000 train_loss:6.2635 train_time:27697ms step_avg:236.73ms torch.cuda.memory_allocated()=369581568
14:04:03.244: step:128/1000 train_loss:6.0052 train_time:27946ms step_avg:236.83ms torch.cuda.memory_allocated()=369581568
14:04:03.486: step:129/1000 train_loss:5.7932 train_time:28188ms step_avg:236.87ms torch.cuda.memory_allocated()=369581568
14:04:03.726: step:130/1000 train_loss:6.2697 train_time:28428ms step_avg:236.90ms torch.cuda.memory_allocated()=369581568
14:04:03.967: step:131/1000 train_loss:5.9866 train_time:28669ms step_avg:236.93ms torch.cuda.memory_allocated()=369581568
14:04:04.207: step:132/1000 train_loss:6.0707 train_time:28909ms step_avg:236.96ms torch.cuda.memory_allocated()=369581568
14:04:04.447: step:133/1000 train_loss:6.0587 train_time:29149ms step_avg:236.98ms torch.cuda.memory_allocated()=369581568
14:04:04.686: step:134/1000 train_loss:5.9731 train_time:29388ms step_avg:237.00ms torch.cuda.memory_allocated()=369581568
14:04:04.927: step:135/1000 train_loss:6.4913 train_time:29628ms step_avg:237.03ms torch.cuda.memory_allocated()=369581568
14:04:05.165: step:136/1000 train_loss:6.0909 train_time:29867ms step_avg:237.04ms torch.cuda.memory_allocated()=369581568
14:04:05.404: step:137/1000 train_loss:5.9445 train_time:30105ms step_avg:237.05ms torch.cuda.memory_allocated()=369581568
14:04:05.644: step:138/1000 train_loss:5.9049 train_time:30345ms step_avg:237.07ms torch.cuda.memory_allocated()=369581568
14:04:05.883: step:139/1000 train_loss:6.2325 train_time:30585ms step_avg:237.09ms torch.cuda.memory_allocated()=369581568
14:04:06.124: step:140/1000 train_loss:5.6724 train_time:30825ms step_avg:237.12ms torch.cuda.memory_allocated()=369581568
14:04:06.366: step:141/1000 train_loss:5.9906 train_time:31068ms step_avg:237.16ms torch.cuda.memory_allocated()=369581568
14:04:06.608: step:142/1000 train_loss:5.9061 train_time:31310ms step_avg:237.19ms torch.cuda.memory_allocated()=369581568
14:04:06.850: step:143/1000 train_loss:5.9862 train_time:31552ms step_avg:237.23ms torch.cuda.memory_allocated()=369581568
14:04:07.091: step:144/1000 train_loss:6.4602 train_time:31792ms step_avg:237.26ms torch.cuda.memory_allocated()=369581568
14:04:07.330: step:145/1000 train_loss:5.9170 train_time:32032ms step_avg:237.27ms torch.cuda.memory_allocated()=369581568
14:04:07.570: step:146/1000 train_loss:6.0801 train_time:32272ms step_avg:237.29ms torch.cuda.memory_allocated()=369581568
14:04:07.810: step:147/1000 train_loss:5.9474 train_time:32511ms step_avg:237.31ms torch.cuda.memory_allocated()=369581568
14:04:08.050: step:148/1000 train_loss:5.6291 train_time:32751ms step_avg:237.33ms torch.cuda.memory_allocated()=369581568
14:04:08.289: step:149/1000 train_loss:5.6343 train_time:32991ms step_avg:237.35ms torch.cuda.memory_allocated()=369581568
14:04:08.534: step:150/1000 train_loss:5.6798 train_time:33236ms step_avg:237.40ms torch.cuda.memory_allocated()=369581568
14:04:08.779: step:151/1000 train_loss:5.7747 train_time:33480ms step_avg:237.45ms torch.cuda.memory_allocated()=369581568
14:04:09.024: step:152/1000 train_loss:6.0045 train_time:33726ms step_avg:237.51ms torch.cuda.memory_allocated()=369581568
14:04:09.268: step:153/1000 train_loss:5.8737 train_time:33970ms step_avg:237.55ms torch.cuda.memory_allocated()=369581568
14:04:09.515: step:154/1000 train_loss:5.9085 train_time:34217ms step_avg:237.62ms torch.cuda.memory_allocated()=369581568
14:04:09.761: step:155/1000 train_loss:5.6917 train_time:34463ms step_avg:237.67ms torch.cuda.memory_allocated()=369581568
14:04:10.006: step:156/1000 train_loss:6.0163 train_time:34707ms step_avg:237.72ms torch.cuda.memory_allocated()=369581568
14:04:10.250: step:157/1000 train_loss:5.9392 train_time:34951ms step_avg:237.76ms torch.cuda.memory_allocated()=369581568
14:04:10.495: step:158/1000 train_loss:5.9174 train_time:35197ms step_avg:237.82ms torch.cuda.memory_allocated()=369581568
14:04:10.739: step:159/1000 train_loss:5.7678 train_time:35441ms step_avg:237.86ms torch.cuda.memory_allocated()=369581568
14:04:10.984: step:160/1000 train_loss:5.7759 train_time:35686ms step_avg:237.91ms torch.cuda.memory_allocated()=369581568
14:04:11.229: step:161/1000 train_loss:5.6575 train_time:35931ms step_avg:237.95ms torch.cuda.memory_allocated()=369581568
14:04:11.473: step:162/1000 train_loss:5.8285 train_time:36175ms step_avg:237.99ms torch.cuda.memory_allocated()=369581568
14:04:11.719: step:163/1000 train_loss:5.7427 train_time:36420ms step_avg:238.04ms torch.cuda.memory_allocated()=369581568
14:04:11.965: step:164/1000 train_loss:5.6165 train_time:36667ms step_avg:238.10ms torch.cuda.memory_allocated()=369581568
14:04:12.210: step:165/1000 train_loss:5.7602 train_time:36912ms step_avg:238.14ms torch.cuda.memory_allocated()=369581568
14:04:12.455: step:166/1000 train_loss:5.6349 train_time:37157ms step_avg:238.18ms torch.cuda.memory_allocated()=369581568
14:04:12.701: step:167/1000 train_loss:5.8972 train_time:37402ms step_avg:238.23ms torch.cuda.memory_allocated()=369581568
14:04:12.947: step:168/1000 train_loss:5.9739 train_time:37648ms step_avg:238.28ms torch.cuda.memory_allocated()=369581568
14:04:13.192: step:169/1000 train_loss:5.6379 train_time:37893ms step_avg:238.32ms torch.cuda.memory_allocated()=369581568
14:04:13.435: step:170/1000 train_loss:5.6717 train_time:38137ms step_avg:238.36ms torch.cuda.memory_allocated()=369581568
14:04:13.680: step:171/1000 train_loss:5.7752 train_time:38382ms step_avg:238.40ms torch.cuda.memory_allocated()=369581568
14:04:13.925: step:172/1000 train_loss:5.7319 train_time:38627ms step_avg:238.44ms torch.cuda.memory_allocated()=369581568
14:04:14.169: step:173/1000 train_loss:5.5751 train_time:38870ms step_avg:238.47ms torch.cuda.memory_allocated()=369581568
14:04:14.414: step:174/1000 train_loss:5.9136 train_time:39116ms step_avg:238.51ms torch.cuda.memory_allocated()=369581568
14:04:14.658: step:175/1000 train_loss:5.8366 train_time:39360ms step_avg:238.55ms torch.cuda.memory_allocated()=369581568
14:04:14.902: step:176/1000 train_loss:5.8200 train_time:39604ms step_avg:238.58ms torch.cuda.memory_allocated()=369581568
14:04:15.146: step:177/1000 train_loss:5.7122 train_time:39848ms step_avg:238.61ms torch.cuda.memory_allocated()=369581568
14:04:15.392: step:178/1000 train_loss:5.8328 train_time:40093ms step_avg:238.65ms torch.cuda.memory_allocated()=369581568
14:04:15.637: step:179/1000 train_loss:5.7028 train_time:40338ms step_avg:238.69ms torch.cuda.memory_allocated()=369581568
14:04:15.883: step:180/1000 train_loss:5.5949 train_time:40584ms step_avg:238.73ms torch.cuda.memory_allocated()=369581568
14:04:16.127: step:181/1000 train_loss:5.7529 train_time:40829ms step_avg:238.77ms torch.cuda.memory_allocated()=369581568
14:04:16.375: step:182/1000 train_loss:5.5310 train_time:41076ms step_avg:238.82ms torch.cuda.memory_allocated()=369581568
14:04:16.620: step:183/1000 train_loss:5.6947 train_time:41321ms step_avg:238.85ms torch.cuda.memory_allocated()=369581568
14:04:16.865: step:184/1000 train_loss:5.7744 train_time:41567ms step_avg:238.89ms torch.cuda.memory_allocated()=369581568
14:04:17.110: step:185/1000 train_loss:5.6178 train_time:41812ms step_avg:238.93ms torch.cuda.memory_allocated()=369581568
14:04:17.355: step:186/1000 train_loss:5.8255 train_time:42057ms step_avg:238.96ms torch.cuda.memory_allocated()=369581568
14:04:17.599: step:187/1000 train_loss:5.8419 train_time:42301ms step_avg:238.99ms torch.cuda.memory_allocated()=369581568
14:04:17.844: step:188/1000 train_loss:5.9765 train_time:42546ms step_avg:239.02ms torch.cuda.memory_allocated()=369581568
14:04:18.090: step:189/1000 train_loss:5.8452 train_time:42792ms step_avg:239.06ms torch.cuda.memory_allocated()=369581568
14:04:18.336: step:190/1000 train_loss:5.8537 train_time:43037ms step_avg:239.10ms torch.cuda.memory_allocated()=369581568
14:04:18.581: step:191/1000 train_loss:5.8382 train_time:43283ms step_avg:239.13ms torch.cuda.memory_allocated()=369581568
14:04:18.826: step:192/1000 train_loss:5.9812 train_time:43528ms step_avg:239.16ms torch.cuda.memory_allocated()=369581568
14:04:19.074: step:193/1000 train_loss:5.8140 train_time:43776ms step_avg:239.21ms torch.cuda.memory_allocated()=369581568
14:04:19.319: step:194/1000 train_loss:5.7157 train_time:44021ms step_avg:239.24ms torch.cuda.memory_allocated()=369581568
14:04:19.565: step:195/1000 train_loss:6.2386 train_time:44267ms step_avg:239.28ms torch.cuda.memory_allocated()=369581568
14:04:19.811: step:196/1000 train_loss:6.1142 train_time:44512ms step_avg:239.31ms torch.cuda.memory_allocated()=369581568
14:04:20.057: step:197/1000 train_loss:5.6486 train_time:44758ms step_avg:239.35ms torch.cuda.memory_allocated()=369581568
14:04:20.302: step:198/1000 train_loss:5.7185 train_time:45004ms step_avg:239.38ms torch.cuda.memory_allocated()=369581568
14:04:20.548: step:199/1000 train_loss:5.7305 train_time:45250ms step_avg:239.42ms torch.cuda.memory_allocated()=369581568
14:04:20.793: step:200/1000 train_loss:5.7457 train_time:45495ms step_avg:239.45ms torch.cuda.memory_allocated()=369581568
14:04:21.039: step:201/1000 train_loss:5.6192 train_time:45741ms step_avg:239.48ms torch.cuda.memory_allocated()=369581568
14:04:21.287: step:202/1000 train_loss:5.8774 train_time:45989ms step_avg:239.52ms torch.cuda.memory_allocated()=369581568
14:04:21.533: step:203/1000 train_loss:5.9349 train_time:46235ms step_avg:239.56ms torch.cuda.memory_allocated()=369581568
14:04:21.778: step:204/1000 train_loss:5.4398 train_time:46479ms step_avg:239.58ms torch.cuda.memory_allocated()=369581568
14:04:22.023: step:205/1000 train_loss:6.1973 train_time:46725ms step_avg:239.61ms torch.cuda.memory_allocated()=369581568
14:04:22.270: step:206/1000 train_loss:6.1224 train_time:46972ms step_avg:239.65ms torch.cuda.memory_allocated()=369581568
14:04:22.515: step:207/1000 train_loss:5.9246 train_time:47217ms step_avg:239.68ms torch.cuda.memory_allocated()=369581568
14:04:22.761: step:208/1000 train_loss:5.8998 train_time:47463ms step_avg:239.71ms torch.cuda.memory_allocated()=369581568
14:04:23.007: step:209/1000 train_loss:5.7013 train_time:47709ms step_avg:239.74ms torch.cuda.memory_allocated()=369581568
14:04:23.252: step:210/1000 train_loss:5.9758 train_time:47953ms step_avg:239.77ms torch.cuda.memory_allocated()=369581568
14:04:23.496: step:211/1000 train_loss:6.0124 train_time:48198ms step_avg:239.79ms torch.cuda.memory_allocated()=369581568
14:04:23.743: step:212/1000 train_loss:5.9671 train_time:48444ms step_avg:239.82ms torch.cuda.memory_allocated()=369581568
14:04:23.989: step:213/1000 train_loss:5.8181 train_time:48691ms step_avg:239.86ms torch.cuda.memory_allocated()=369581568
14:04:24.234: step:214/1000 train_loss:5.6832 train_time:48936ms step_avg:239.88ms torch.cuda.memory_allocated()=369581568
14:04:24.479: step:215/1000 train_loss:5.8505 train_time:49181ms step_avg:239.91ms torch.cuda.memory_allocated()=369581568
14:04:24.726: step:216/1000 train_loss:5.9338 train_time:49428ms step_avg:239.94ms torch.cuda.memory_allocated()=369581568
14:04:24.973: step:217/1000 train_loss:5.6276 train_time:49675ms step_avg:239.97ms torch.cuda.memory_allocated()=369581568
14:04:25.219: step:218/1000 train_loss:5.7307 train_time:49921ms step_avg:240.00ms torch.cuda.memory_allocated()=369581568
14:04:25.464: step:219/1000 train_loss:5.7593 train_time:50166ms step_avg:240.03ms torch.cuda.memory_allocated()=369581568
14:04:25.710: step:220/1000 train_loss:5.7449 train_time:50412ms step_avg:240.06ms torch.cuda.memory_allocated()=369581568
14:04:25.955: step:221/1000 train_loss:5.9478 train_time:50657ms step_avg:240.08ms torch.cuda.memory_allocated()=369581568
14:04:26.201: step:222/1000 train_loss:5.8196 train_time:50903ms step_avg:240.11ms torch.cuda.memory_allocated()=369581568
14:04:26.446: step:223/1000 train_loss:5.7662 train_time:51148ms step_avg:240.13ms torch.cuda.memory_allocated()=369581568
14:04:26.695: step:224/1000 train_loss:5.6026 train_time:51396ms step_avg:240.17ms torch.cuda.memory_allocated()=369581568
14:04:26.945: step:225/1000 train_loss:5.5442 train_time:51647ms step_avg:240.22ms torch.cuda.memory_allocated()=369581568
14:04:27.195: step:226/1000 train_loss:6.3312 train_time:51897ms step_avg:240.26ms torch.cuda.memory_allocated()=369581568
14:04:27.445: step:227/1000 train_loss:5.6232 train_time:52147ms step_avg:240.31ms torch.cuda.memory_allocated()=369581568
14:04:27.694: step:228/1000 train_loss:5.6160 train_time:52395ms step_avg:240.35ms torch.cuda.memory_allocated()=369581568
14:04:27.943: step:229/1000 train_loss:5.6400 train_time:52645ms step_avg:240.39ms torch.cuda.memory_allocated()=369581568
14:04:28.193: step:230/1000 train_loss:5.6913 train_time:52895ms step_avg:240.43ms torch.cuda.memory_allocated()=369581568
14:04:28.441: step:231/1000 train_loss:5.8635 train_time:53142ms step_avg:240.46ms torch.cuda.memory_allocated()=369581568
14:04:28.689: step:232/1000 train_loss:5.5123 train_time:53391ms step_avg:240.50ms torch.cuda.memory_allocated()=369581568
14:04:28.941: step:233/1000 train_loss:5.8720 train_time:53643ms step_avg:240.55ms torch.cuda.memory_allocated()=369581568
14:04:29.191: step:234/1000 train_loss:5.4708 train_time:53893ms step_avg:240.59ms torch.cuda.memory_allocated()=369581568
14:04:29.439: step:235/1000 train_loss:5.7859 train_time:54141ms step_avg:240.63ms torch.cuda.memory_allocated()=369581568
14:04:29.688: step:236/1000 train_loss:5.5542 train_time:54390ms step_avg:240.66ms torch.cuda.memory_allocated()=369581568
14:04:29.937: step:237/1000 train_loss:5.4109 train_time:54639ms step_avg:240.70ms torch.cuda.memory_allocated()=369581568
14:04:30.186: step:238/1000 train_loss:5.5925 train_time:54888ms step_avg:240.74ms torch.cuda.memory_allocated()=369581568
14:04:30.435: step:239/1000 train_loss:5.6858 train_time:55136ms step_avg:240.77ms torch.cuda.memory_allocated()=369581568
14:04:30.683: step:240/1000 train_loss:5.4818 train_time:55384ms step_avg:240.80ms torch.cuda.memory_allocated()=369581568
14:04:30.933: step:241/1000 train_loss:4.7144 train_time:55635ms step_avg:240.84ms torch.cuda.memory_allocated()=369581568
14:04:31.183: step:242/1000 train_loss:5.6533 train_time:55885ms step_avg:240.88ms torch.cuda.memory_allocated()=369581568
14:04:31.432: step:243/1000 train_loss:5.8364 train_time:56134ms step_avg:240.92ms torch.cuda.memory_allocated()=369581568
14:04:31.681: step:244/1000 train_loss:5.5506 train_time:56383ms step_avg:240.95ms torch.cuda.memory_allocated()=369581568
14:04:31.931: step:245/1000 train_loss:5.5423 train_time:56632ms step_avg:240.99ms torch.cuda.memory_allocated()=369581568
14:04:32.182: step:246/1000 train_loss:5.6258 train_time:56883ms step_avg:241.03ms torch.cuda.memory_allocated()=369581568
14:04:32.436: step:247/1000 train_loss:5.7559 train_time:57137ms step_avg:241.09ms torch.cuda.memory_allocated()=369581568
14:04:32.685: step:248/1000 train_loss:5.5666 train_time:57387ms step_avg:241.12ms torch.cuda.memory_allocated()=369581568
14:04:32.935: step:249/1000 train_loss:6.0701 train_time:57637ms step_avg:241.16ms torch.cuda.memory_allocated()=369581568
14:04:33.184: step:250/1000 train_loss:5.6736 train_time:57886ms step_avg:241.19ms torch.cuda.memory_allocated()=369581568
14:04:35.849: step:250/1000 val_loss:5.8240 train_time:57886ms step_avg:241.19ms
14:04:36.099: step:251/1000 train_loss:5.4259 train_time:58135ms step_avg:241.22ms torch.cuda.memory_allocated()=369581568
14:04:36.348: step:252/1000 train_loss:5.7835 train_time:58385ms step_avg:241.26ms torch.cuda.memory_allocated()=369581568
14:04:36.599: step:253/1000 train_loss:5.3886 train_time:58635ms step_avg:241.30ms torch.cuda.memory_allocated()=369581568
14:04:36.850: step:254/1000 train_loss:5.2132 train_time:58886ms step_avg:241.34ms torch.cuda.memory_allocated()=369581568
14:04:37.099: step:255/1000 train_loss:5.4167 train_time:59135ms step_avg:241.37ms torch.cuda.memory_allocated()=369581568
14:04:37.351: step:256/1000 train_loss:5.6986 train_time:59387ms step_avg:241.41ms torch.cuda.memory_allocated()=369581568
14:04:37.599: step:257/1000 train_loss:5.8803 train_time:59635ms step_avg:241.44ms torch.cuda.memory_allocated()=369581568
14:04:37.849: step:258/1000 train_loss:5.6682 train_time:59885ms step_avg:241.47ms torch.cuda.memory_allocated()=369581568
14:04:38.098: step:259/1000 train_loss:5.7142 train_time:60135ms step_avg:241.50ms torch.cuda.memory_allocated()=369581568
14:04:38.347: step:260/1000 train_loss:5.7492 train_time:60383ms step_avg:241.53ms torch.cuda.memory_allocated()=369581568
14:04:38.596: step:261/1000 train_loss:5.9058 train_time:60632ms step_avg:241.56ms torch.cuda.memory_allocated()=369581568
14:04:38.845: step:262/1000 train_loss:5.5741 train_time:60881ms step_avg:241.59ms torch.cuda.memory_allocated()=369581568
14:04:39.095: step:263/1000 train_loss:5.6887 train_time:61131ms step_avg:241.62ms torch.cuda.memory_allocated()=369581568
14:04:39.345: step:264/1000 train_loss:5.6257 train_time:61381ms step_avg:241.66ms torch.cuda.memory_allocated()=369581568
14:04:39.596: step:265/1000 train_loss:5.7544 train_time:61632ms step_avg:241.70ms torch.cuda.memory_allocated()=369581568
14:04:39.844: step:266/1000 train_loss:5.5352 train_time:61880ms step_avg:241.72ms torch.cuda.memory_allocated()=369581568
14:04:40.092: step:267/1000 train_loss:5.7321 train_time:62129ms step_avg:241.75ms torch.cuda.memory_allocated()=369581568
14:04:40.342: step:268/1000 train_loss:5.6780 train_time:62378ms step_avg:241.78ms torch.cuda.memory_allocated()=369581568
14:04:40.591: step:269/1000 train_loss:5.7359 train_time:62628ms step_avg:241.81ms torch.cuda.memory_allocated()=369581568
14:04:40.841: step:270/1000 train_loss:5.8147 train_time:62877ms step_avg:241.84ms torch.cuda.memory_allocated()=369581568
14:04:41.091: step:271/1000 train_loss:5.7665 train_time:63127ms step_avg:241.87ms torch.cuda.memory_allocated()=369581568
14:04:41.340: step:272/1000 train_loss:5.9685 train_time:63376ms step_avg:241.89ms torch.cuda.memory_allocated()=369581568
14:04:41.588: step:273/1000 train_loss:5.7800 train_time:63624ms step_avg:241.92ms torch.cuda.memory_allocated()=369581568
14:04:41.837: step:274/1000 train_loss:5.7020 train_time:63873ms step_avg:241.94ms torch.cuda.memory_allocated()=369581568
14:04:42.085: step:275/1000 train_loss:5.7220 train_time:64122ms step_avg:241.97ms torch.cuda.memory_allocated()=369581568
14:04:42.334: step:276/1000 train_loss:5.9888 train_time:64370ms step_avg:241.99ms torch.cuda.memory_allocated()=369581568
14:04:42.582: step:277/1000 train_loss:5.5571 train_time:64618ms step_avg:242.02ms torch.cuda.memory_allocated()=369581568
14:04:42.832: step:278/1000 train_loss:5.5010 train_time:64868ms step_avg:242.05ms torch.cuda.memory_allocated()=369581568
14:04:43.082: step:279/1000 train_loss:5.6522 train_time:65118ms step_avg:242.07ms torch.cuda.memory_allocated()=369581568
14:04:43.330: step:280/1000 train_loss:5.6960 train_time:65366ms step_avg:242.10ms torch.cuda.memory_allocated()=369581568
14:04:43.577: step:281/1000 train_loss:5.4680 train_time:65613ms step_avg:242.11ms torch.cuda.memory_allocated()=369581568
14:04:43.825: step:282/1000 train_loss:5.5283 train_time:65861ms step_avg:242.14ms torch.cuda.memory_allocated()=369581568
14:04:44.075: step:283/1000 train_loss:5.4075 train_time:66111ms step_avg:242.17ms torch.cuda.memory_allocated()=369581568
14:04:44.324: step:284/1000 train_loss:5.6202 train_time:66361ms step_avg:242.19ms torch.cuda.memory_allocated()=369581568
14:04:44.574: step:285/1000 train_loss:5.8669 train_time:66610ms step_avg:242.22ms torch.cuda.memory_allocated()=369581568
14:04:44.823: step:286/1000 train_loss:6.7396 train_time:66859ms step_avg:242.24ms torch.cuda.memory_allocated()=369581568
14:04:45.071: step:287/1000 train_loss:6.2233 train_time:67107ms step_avg:242.26ms torch.cuda.memory_allocated()=369581568
14:04:45.321: step:288/1000 train_loss:5.8975 train_time:67357ms step_avg:242.29ms torch.cuda.memory_allocated()=369581568
14:04:45.571: step:289/1000 train_loss:5.9143 train_time:67607ms step_avg:242.32ms torch.cuda.memory_allocated()=369581568
14:04:45.820: step:290/1000 train_loss:5.6376 train_time:67856ms step_avg:242.34ms torch.cuda.memory_allocated()=369581568
14:04:46.068: step:291/1000 train_loss:5.1915 train_time:68104ms step_avg:242.36ms torch.cuda.memory_allocated()=369581568
14:04:46.319: step:292/1000 train_loss:5.8822 train_time:68355ms step_avg:242.39ms torch.cuda.memory_allocated()=369581568
14:04:46.567: step:293/1000 train_loss:5.3506 train_time:68603ms step_avg:242.41ms torch.cuda.memory_allocated()=369581568
14:04:46.815: step:294/1000 train_loss:5.2917 train_time:68851ms step_avg:242.43ms torch.cuda.memory_allocated()=369581568
14:04:47.064: step:295/1000 train_loss:5.5803 train_time:69100ms step_avg:242.46ms torch.cuda.memory_allocated()=369581568
14:04:47.312: step:296/1000 train_loss:5.5597 train_time:69348ms step_avg:242.48ms torch.cuda.memory_allocated()=369581568
14:04:47.561: step:297/1000 train_loss:5.5118 train_time:69597ms step_avg:242.50ms torch.cuda.memory_allocated()=369581568
14:04:47.812: step:298/1000 train_loss:5.3342 train_time:69848ms step_avg:242.53ms torch.cuda.memory_allocated()=369581568
14:04:48.066: step:299/1000 train_loss:5.7951 train_time:70102ms step_avg:242.57ms torch.cuda.memory_allocated()=369581568
14:04:48.318: step:300/1000 train_loss:5.6699 train_time:70354ms step_avg:242.60ms torch.cuda.memory_allocated()=369581568
14:04:48.569: step:301/1000 train_loss:5.5103 train_time:70605ms step_avg:242.63ms torch.cuda.memory_allocated()=369581568
14:04:48.822: step:302/1000 train_loss:5.1395 train_time:70859ms step_avg:242.67ms torch.cuda.memory_allocated()=369581568
14:04:49.076: step:303/1000 train_loss:5.4357 train_time:71112ms step_avg:242.70ms torch.cuda.memory_allocated()=369581568
14:04:49.329: step:304/1000 train_loss:5.6700 train_time:71365ms step_avg:242.74ms torch.cuda.memory_allocated()=369581568
14:04:49.581: step:305/1000 train_loss:5.4844 train_time:71618ms step_avg:242.77ms torch.cuda.memory_allocated()=369581568
14:04:49.834: step:306/1000 train_loss:5.7502 train_time:71870ms step_avg:242.81ms torch.cuda.memory_allocated()=369581568
14:04:50.086: step:307/1000 train_loss:5.5243 train_time:72122ms step_avg:242.84ms torch.cuda.memory_allocated()=369581568
14:04:50.338: step:308/1000 train_loss:5.6571 train_time:72374ms step_avg:242.87ms torch.cuda.memory_allocated()=369581568
14:04:50.590: step:309/1000 train_loss:5.4361 train_time:72626ms step_avg:242.90ms torch.cuda.memory_allocated()=369581568
14:04:50.844: step:310/1000 train_loss:5.5520 train_time:72880ms step_avg:242.93ms torch.cuda.memory_allocated()=369581568
14:04:51.095: step:311/1000 train_loss:5.4465 train_time:73131ms step_avg:242.96ms torch.cuda.memory_allocated()=369581568
14:04:51.348: step:312/1000 train_loss:5.5224 train_time:73385ms step_avg:243.00ms torch.cuda.memory_allocated()=369581568
14:04:51.600: step:313/1000 train_loss:5.3208 train_time:73637ms step_avg:243.03ms torch.cuda.memory_allocated()=369581568
14:04:51.854: step:314/1000 train_loss:5.4989 train_time:73890ms step_avg:243.06ms torch.cuda.memory_allocated()=369581568
14:04:52.107: step:315/1000 train_loss:5.6889 train_time:74143ms step_avg:243.09ms torch.cuda.memory_allocated()=369581568
14:04:52.360: step:316/1000 train_loss:5.6149 train_time:74396ms step_avg:243.12ms torch.cuda.memory_allocated()=369581568
14:04:52.611: step:317/1000 train_loss:5.5017 train_time:74648ms step_avg:243.15ms torch.cuda.memory_allocated()=369581568
14:04:52.863: step:318/1000 train_loss:5.4955 train_time:74899ms step_avg:243.18ms torch.cuda.memory_allocated()=369581568
14:04:53.115: step:319/1000 train_loss:5.6111 train_time:75151ms step_avg:243.21ms torch.cuda.memory_allocated()=369581568
14:04:53.366: step:320/1000 train_loss:5.4727 train_time:75403ms step_avg:243.23ms torch.cuda.memory_allocated()=369581568
14:04:53.620: step:321/1000 train_loss:5.4139 train_time:75657ms step_avg:243.27ms torch.cuda.memory_allocated()=369581568
14:04:53.872: step:322/1000 train_loss:5.5728 train_time:75908ms step_avg:243.30ms torch.cuda.memory_allocated()=369581568
14:04:54.123: step:323/1000 train_loss:5.7077 train_time:76160ms step_avg:243.32ms torch.cuda.memory_allocated()=369581568
14:04:54.378: step:324/1000 train_loss:5.6260 train_time:76414ms step_avg:243.36ms torch.cuda.memory_allocated()=369581568
14:04:54.629: step:325/1000 train_loss:5.4831 train_time:76666ms step_avg:243.38ms torch.cuda.memory_allocated()=369581568
14:04:54.883: step:326/1000 train_loss:5.5353 train_time:76919ms step_avg:243.41ms torch.cuda.memory_allocated()=369581568
14:04:55.135: step:327/1000 train_loss:5.5843 train_time:77171ms step_avg:243.44ms torch.cuda.memory_allocated()=369581568
14:04:55.386: step:328/1000 train_loss:5.5078 train_time:77422ms step_avg:243.47ms torch.cuda.memory_allocated()=369581568
14:04:55.639: step:329/1000 train_loss:5.5263 train_time:77675ms step_avg:243.49ms torch.cuda.memory_allocated()=369581568
14:04:55.892: step:330/1000 train_loss:5.3997 train_time:77928ms step_avg:243.53ms torch.cuda.memory_allocated()=369581568
14:04:56.144: step:331/1000 train_loss:5.4337 train_time:78180ms step_avg:243.55ms torch.cuda.memory_allocated()=369581568
14:04:56.398: step:332/1000 train_loss:5.8262 train_time:78434ms step_avg:243.58ms torch.cuda.memory_allocated()=369581568
14:04:56.650: step:333/1000 train_loss:5.4066 train_time:78686ms step_avg:243.61ms torch.cuda.memory_allocated()=369581568
14:04:56.904: step:334/1000 train_loss:5.2845 train_time:78940ms step_avg:243.64ms torch.cuda.memory_allocated()=369581568
14:04:57.156: step:335/1000 train_loss:5.6639 train_time:79192ms step_avg:243.67ms torch.cuda.memory_allocated()=369581568
14:04:57.414: step:336/1000 train_loss:5.3854 train_time:79450ms step_avg:243.71ms torch.cuda.memory_allocated()=369581568
14:04:57.667: step:337/1000 train_loss:5.5799 train_time:79703ms step_avg:243.74ms torch.cuda.memory_allocated()=369581568
14:04:57.920: step:338/1000 train_loss:5.5005 train_time:79956ms step_avg:243.77ms torch.cuda.memory_allocated()=369581568
14:04:58.172: step:339/1000 train_loss:5.5991 train_time:80209ms step_avg:243.80ms torch.cuda.memory_allocated()=369581568
14:04:58.424: step:340/1000 train_loss:5.4657 train_time:80460ms step_avg:243.82ms torch.cuda.memory_allocated()=369581568
14:04:58.675: step:341/1000 train_loss:5.3241 train_time:80711ms step_avg:243.84ms torch.cuda.memory_allocated()=369581568
14:04:58.928: step:342/1000 train_loss:5.7271 train_time:80964ms step_avg:243.87ms torch.cuda.memory_allocated()=369581568
14:04:59.178: step:343/1000 train_loss:5.4123 train_time:81214ms step_avg:243.89ms torch.cuda.memory_allocated()=369581568
14:04:59.430: step:344/1000 train_loss:5.6176 train_time:81466ms step_avg:243.91ms torch.cuda.memory_allocated()=369581568
14:04:59.682: step:345/1000 train_loss:5.5629 train_time:81718ms step_avg:243.93ms torch.cuda.memory_allocated()=369581568
14:04:59.934: step:346/1000 train_loss:5.2981 train_time:81970ms step_avg:243.96ms torch.cuda.memory_allocated()=369581568
14:05:00.186: step:347/1000 train_loss:5.4477 train_time:82223ms step_avg:243.98ms torch.cuda.memory_allocated()=369581568
14:05:00.440: step:348/1000 train_loss:5.4126 train_time:82476ms step_avg:244.01ms torch.cuda.memory_allocated()=369581568
14:05:00.692: step:349/1000 train_loss:6.2544 train_time:82728ms step_avg:244.04ms torch.cuda.memory_allocated()=369581568
14:05:00.943: step:350/1000 train_loss:5.4397 train_time:82979ms step_avg:244.06ms torch.cuda.memory_allocated()=369581568
14:05:01.196: step:351/1000 train_loss:5.5754 train_time:83233ms step_avg:244.08ms torch.cuda.memory_allocated()=369581568
14:05:01.449: step:352/1000 train_loss:5.3947 train_time:83485ms step_avg:244.11ms torch.cuda.memory_allocated()=369581568
14:05:01.702: step:353/1000 train_loss:5.4864 train_time:83738ms step_avg:244.13ms torch.cuda.memory_allocated()=369581568
14:05:01.955: step:354/1000 train_loss:5.6543 train_time:83991ms step_avg:244.16ms torch.cuda.memory_allocated()=369581568
14:05:02.208: step:355/1000 train_loss:5.4291 train_time:84244ms step_avg:244.19ms torch.cuda.memory_allocated()=369581568
14:05:02.461: step:356/1000 train_loss:5.4736 train_time:84498ms step_avg:244.21ms torch.cuda.memory_allocated()=369581568
14:05:02.714: step:357/1000 train_loss:5.5177 train_time:84750ms step_avg:244.24ms torch.cuda.memory_allocated()=369581568
14:05:02.965: step:358/1000 train_loss:5.1946 train_time:85001ms step_avg:244.26ms torch.cuda.memory_allocated()=369581568
14:05:03.217: step:359/1000 train_loss:5.6747 train_time:85254ms step_avg:244.28ms torch.cuda.memory_allocated()=369581568
14:05:03.470: step:360/1000 train_loss:5.5064 train_time:85507ms step_avg:244.30ms torch.cuda.memory_allocated()=369581568
14:05:03.724: step:361/1000 train_loss:5.5850 train_time:85761ms step_avg:244.33ms torch.cuda.memory_allocated()=369581568
14:05:03.977: step:362/1000 train_loss:5.6651 train_time:86013ms step_avg:244.36ms torch.cuda.memory_allocated()=369581568
14:05:04.228: step:363/1000 train_loss:5.4852 train_time:86265ms step_avg:244.38ms torch.cuda.memory_allocated()=369581568
14:05:04.482: step:364/1000 train_loss:5.2994 train_time:86519ms step_avg:244.40ms torch.cuda.memory_allocated()=369581568
14:05:04.735: step:365/1000 train_loss:5.4140 train_time:86771ms step_avg:244.43ms torch.cuda.memory_allocated()=369581568
14:05:04.986: step:366/1000 train_loss:5.5149 train_time:87022ms step_avg:244.44ms torch.cuda.memory_allocated()=369581568
14:05:05.238: step:367/1000 train_loss:5.1800 train_time:87275ms step_avg:244.47ms torch.cuda.memory_allocated()=369581568
14:05:05.491: step:368/1000 train_loss:5.8400 train_time:87527ms step_avg:244.49ms torch.cuda.memory_allocated()=369581568
14:05:05.743: step:369/1000 train_loss:5.4235 train_time:87779ms step_avg:244.51ms torch.cuda.memory_allocated()=369581568
14:05:05.995: step:370/1000 train_loss:5.3037 train_time:88031ms step_avg:244.53ms torch.cuda.memory_allocated()=369581568
14:05:06.247: step:371/1000 train_loss:5.5802 train_time:88283ms step_avg:244.55ms torch.cuda.memory_allocated()=369581568
14:05:06.503: step:372/1000 train_loss:5.2859 train_time:88540ms step_avg:244.58ms torch.cuda.memory_allocated()=369581568
14:05:06.756: step:373/1000 train_loss:5.5720 train_time:88792ms step_avg:244.61ms torch.cuda.memory_allocated()=369581568
14:05:07.012: step:374/1000 train_loss:5.5788 train_time:89048ms step_avg:244.64ms torch.cuda.memory_allocated()=369581568
14:05:07.268: step:375/1000 train_loss:5.7560 train_time:89305ms step_avg:244.67ms torch.cuda.memory_allocated()=369581568
14:05:09.986: step:375/1000 val_loss:5.6061 train_time:89305ms step_avg:244.67ms
14:05:10.244: step:376/1000 train_loss:5.7592 train_time:89563ms step_avg:244.71ms torch.cuda.memory_allocated()=369581568
14:05:10.507: step:377/1000 train_loss:5.4415 train_time:89826ms step_avg:244.76ms torch.cuda.memory_allocated()=369581568
14:05:10.761: step:378/1000 train_loss:5.6601 train_time:90080ms step_avg:244.78ms torch.cuda.memory_allocated()=369581568
14:05:11.016: step:379/1000 train_loss:5.5624 train_time:90334ms step_avg:244.81ms torch.cuda.memory_allocated()=369581568
14:05:11.273: step:380/1000 train_loss:5.3383 train_time:90592ms step_avg:244.84ms torch.cuda.memory_allocated()=369581568
14:05:11.532: step:381/1000 train_loss:5.2742 train_time:90850ms step_avg:244.88ms torch.cuda.memory_allocated()=369581568
14:05:11.788: step:382/1000 train_loss:5.5930 train_time:91107ms step_avg:244.91ms torch.cuda.memory_allocated()=369581568
14:05:12.044: step:383/1000 train_loss:5.4952 train_time:91363ms step_avg:244.94ms torch.cuda.memory_allocated()=369581568
14:05:12.305: step:384/1000 train_loss:5.4158 train_time:91623ms step_avg:244.98ms torch.cuda.memory_allocated()=369581568
14:05:12.561: step:385/1000 train_loss:5.3651 train_time:91880ms step_avg:245.01ms torch.cuda.memory_allocated()=369581568
14:05:12.813: step:386/1000 train_loss:5.3377 train_time:92131ms step_avg:245.03ms torch.cuda.memory_allocated()=369581568
14:05:13.068: step:387/1000 train_loss:5.4682 train_time:92387ms step_avg:245.06ms torch.cuda.memory_allocated()=369581568
14:05:13.325: step:388/1000 train_loss:5.4391 train_time:92643ms step_avg:245.09ms torch.cuda.memory_allocated()=369581568
14:05:13.578: step:389/1000 train_loss:5.3403 train_time:92897ms step_avg:245.11ms torch.cuda.memory_allocated()=369581568
14:05:13.835: step:390/1000 train_loss:5.5717 train_time:93154ms step_avg:245.14ms torch.cuda.memory_allocated()=369581568
14:05:14.090: step:391/1000 train_loss:5.4178 train_time:93409ms step_avg:245.17ms torch.cuda.memory_allocated()=369581568
14:05:14.346: step:392/1000 train_loss:5.4753 train_time:93665ms step_avg:245.20ms torch.cuda.memory_allocated()=369581568
14:05:14.602: step:393/1000 train_loss:5.5535 train_time:93921ms step_avg:245.22ms torch.cuda.memory_allocated()=369581568
14:05:14.861: step:394/1000 train_loss:5.7389 train_time:94179ms step_avg:245.26ms torch.cuda.memory_allocated()=369581568
14:05:15.116: step:395/1000 train_loss:5.5831 train_time:94435ms step_avg:245.29ms torch.cuda.memory_allocated()=369581568
14:05:15.369: step:396/1000 train_loss:5.4453 train_time:94688ms step_avg:245.30ms torch.cuda.memory_allocated()=369581568
14:05:15.628: step:397/1000 train_loss:5.5884 train_time:94947ms step_avg:245.34ms torch.cuda.memory_allocated()=369581568
14:05:15.890: step:398/1000 train_loss:5.3027 train_time:95209ms step_avg:245.38ms torch.cuda.memory_allocated()=369581568
14:05:16.145: step:399/1000 train_loss:5.6869 train_time:95463ms step_avg:245.41ms torch.cuda.memory_allocated()=369581568
14:05:16.401: step:400/1000 train_loss:5.4045 train_time:95719ms step_avg:245.43ms torch.cuda.memory_allocated()=369581568
14:05:16.655: step:401/1000 train_loss:5.4256 train_time:95974ms step_avg:245.46ms torch.cuda.memory_allocated()=369581568
14:05:16.909: step:402/1000 train_loss:5.4793 train_time:96228ms step_avg:245.48ms torch.cuda.memory_allocated()=369581568
14:05:17.162: step:403/1000 train_loss:5.5411 train_time:96481ms step_avg:245.50ms torch.cuda.memory_allocated()=369581568
14:05:17.421: step:404/1000 train_loss:5.3869 train_time:96740ms step_avg:245.53ms torch.cuda.memory_allocated()=369581568
14:05:17.678: step:405/1000 train_loss:5.3708 train_time:96996ms step_avg:245.56ms torch.cuda.memory_allocated()=369581568
14:05:17.934: step:406/1000 train_loss:5.2725 train_time:97253ms step_avg:245.59ms torch.cuda.memory_allocated()=369581568
14:05:18.189: step:407/1000 train_loss:5.5268 train_time:97507ms step_avg:245.61ms torch.cuda.memory_allocated()=369581568
14:05:18.444: step:408/1000 train_loss:5.2475 train_time:97763ms step_avg:245.63ms torch.cuda.memory_allocated()=369581568
14:05:18.698: step:409/1000 train_loss:5.6628 train_time:98017ms step_avg:245.66ms torch.cuda.memory_allocated()=369581568
14:05:18.957: step:410/1000 train_loss:5.4536 train_time:98276ms step_avg:245.69ms torch.cuda.memory_allocated()=369581568
14:05:19.211: step:411/1000 train_loss:5.4107 train_time:98530ms step_avg:245.71ms torch.cuda.memory_allocated()=369581568
14:05:19.469: step:412/1000 train_loss:5.6562 train_time:98788ms step_avg:245.74ms torch.cuda.memory_allocated()=369581568
14:05:19.725: step:413/1000 train_loss:5.4868 train_time:99043ms step_avg:245.77ms torch.cuda.memory_allocated()=369581568
14:05:19.978: step:414/1000 train_loss:5.2342 train_time:99297ms step_avg:245.78ms torch.cuda.memory_allocated()=369581568
14:05:20.236: step:415/1000 train_loss:5.4210 train_time:99555ms step_avg:245.81ms torch.cuda.memory_allocated()=369581568
14:05:20.847: step:416/1000 train_loss:5.3751 train_time:100165ms step_avg:246.71ms torch.cuda.memory_allocated()=369581568
14:05:21.103: step:417/1000 train_loss:5.2691 train_time:100422ms step_avg:246.74ms torch.cuda.memory_allocated()=369581568
14:05:21.361: step:418/1000 train_loss:5.3190 train_time:100679ms step_avg:246.76ms torch.cuda.memory_allocated()=369581568
14:05:21.616: step:419/1000 train_loss:5.5177 train_time:100935ms step_avg:246.78ms torch.cuda.memory_allocated()=369581568
14:05:21.875: step:420/1000 train_loss:5.4143 train_time:101194ms step_avg:246.81ms torch.cuda.memory_allocated()=369581568
14:05:22.129: step:421/1000 train_loss:5.5056 train_time:101448ms step_avg:246.83ms torch.cuda.memory_allocated()=369581568
14:05:22.383: step:422/1000 train_loss:5.4863 train_time:101702ms step_avg:246.85ms torch.cuda.memory_allocated()=369581568
14:05:22.640: step:423/1000 train_loss:5.3773 train_time:101959ms step_avg:246.87ms torch.cuda.memory_allocated()=369581568
14:05:22.893: step:424/1000 train_loss:5.3814 train_time:102212ms step_avg:246.89ms torch.cuda.memory_allocated()=369581568
14:05:23.150: step:425/1000 train_loss:5.0020 train_time:102468ms step_avg:246.91ms torch.cuda.memory_allocated()=369581568
14:05:23.406: step:426/1000 train_loss:5.1874 train_time:102725ms step_avg:246.93ms torch.cuda.memory_allocated()=369581568
14:05:23.664: step:427/1000 train_loss:5.2367 train_time:102983ms step_avg:246.96ms torch.cuda.memory_allocated()=369581568
14:05:23.925: step:428/1000 train_loss:5.2340 train_time:103244ms step_avg:246.99ms torch.cuda.memory_allocated()=369581568
14:05:24.181: step:429/1000 train_loss:5.6556 train_time:103499ms step_avg:247.02ms torch.cuda.memory_allocated()=369581568
14:05:24.437: step:430/1000 train_loss:5.9420 train_time:103756ms step_avg:247.04ms torch.cuda.memory_allocated()=369581568
14:05:24.695: step:431/1000 train_loss:5.4313 train_time:104013ms step_avg:247.06ms torch.cuda.memory_allocated()=369581568
14:05:24.950: step:432/1000 train_loss:5.3123 train_time:104269ms step_avg:247.08ms torch.cuda.memory_allocated()=369581568
14:05:25.205: step:433/1000 train_loss:5.4075 train_time:104524ms step_avg:247.10ms torch.cuda.memory_allocated()=369581568
14:05:25.462: step:434/1000 train_loss:5.6014 train_time:104781ms step_avg:247.13ms torch.cuda.memory_allocated()=369581568
14:05:25.719: step:435/1000 train_loss:5.2429 train_time:105037ms step_avg:247.15ms torch.cuda.memory_allocated()=369581568
14:05:25.974: step:436/1000 train_loss:5.1895 train_time:105293ms step_avg:247.17ms torch.cuda.memory_allocated()=369581568
14:05:26.230: step:437/1000 train_loss:5.1877 train_time:105549ms step_avg:247.19ms torch.cuda.memory_allocated()=369581568
14:05:26.483: step:438/1000 train_loss:5.2389 train_time:105802ms step_avg:247.20ms torch.cuda.memory_allocated()=369581568
14:05:26.738: step:439/1000 train_loss:4.9857 train_time:106057ms step_avg:247.22ms torch.cuda.memory_allocated()=369581568
14:05:26.995: step:440/1000 train_loss:5.0463 train_time:106314ms step_avg:247.24ms torch.cuda.memory_allocated()=369581568
14:05:27.252: step:441/1000 train_loss:5.3536 train_time:106571ms step_avg:247.26ms torch.cuda.memory_allocated()=369581568
14:05:27.509: step:442/1000 train_loss:5.2367 train_time:106828ms step_avg:247.29ms torch.cuda.memory_allocated()=369581568
14:05:27.764: step:443/1000 train_loss:5.3753 train_time:107082ms step_avg:247.30ms torch.cuda.memory_allocated()=369581568
14:05:28.024: step:444/1000 train_loss:5.1892 train_time:107343ms step_avg:247.33ms torch.cuda.memory_allocated()=369581568
14:05:28.283: step:445/1000 train_loss:5.4326 train_time:107602ms step_avg:247.36ms torch.cuda.memory_allocated()=369581568
14:05:28.541: step:446/1000 train_loss:5.3553 train_time:107860ms step_avg:247.38ms torch.cuda.memory_allocated()=369581568
14:05:28.797: step:447/1000 train_loss:5.3965 train_time:108116ms step_avg:247.40ms torch.cuda.memory_allocated()=369581568
14:05:29.058: step:448/1000 train_loss:5.3995 train_time:108377ms step_avg:247.44ms torch.cuda.memory_allocated()=369581568
14:05:29.317: step:449/1000 train_loss:5.2428 train_time:108635ms step_avg:247.46ms torch.cuda.memory_allocated()=369581568
14:05:29.576: step:450/1000 train_loss:5.3679 train_time:108894ms step_avg:247.49ms torch.cuda.memory_allocated()=369581568
14:05:29.838: step:451/1000 train_loss:5.3556 train_time:109157ms step_avg:247.52ms torch.cuda.memory_allocated()=369581568
14:05:30.100: step:452/1000 train_loss:5.3875 train_time:109419ms step_avg:247.55ms torch.cuda.memory_allocated()=369581568
14:05:30.358: step:453/1000 train_loss:5.2093 train_time:109676ms step_avg:247.58ms torch.cuda.memory_allocated()=369581568
14:05:30.615: step:454/1000 train_loss:5.4006 train_time:109934ms step_avg:247.60ms torch.cuda.memory_allocated()=369581568
14:05:30.877: step:455/1000 train_loss:5.4045 train_time:110195ms step_avg:247.63ms torch.cuda.memory_allocated()=369581568
14:05:31.137: step:456/1000 train_loss:5.5251 train_time:110456ms step_avg:247.66ms torch.cuda.memory_allocated()=369581568
14:05:31.398: step:457/1000 train_loss:5.3676 train_time:110717ms step_avg:247.69ms torch.cuda.memory_allocated()=369581568
14:05:31.654: step:458/1000 train_loss:5.2789 train_time:110973ms step_avg:247.71ms torch.cuda.memory_allocated()=369581568
14:05:31.914: step:459/1000 train_loss:5.2637 train_time:111232ms step_avg:247.73ms torch.cuda.memory_allocated()=369581568
14:05:32.174: step:460/1000 train_loss:5.3584 train_time:111493ms step_avg:247.76ms torch.cuda.memory_allocated()=369581568
14:05:32.432: step:461/1000 train_loss:5.1995 train_time:111751ms step_avg:247.78ms torch.cuda.memory_allocated()=369581568
14:05:32.694: step:462/1000 train_loss:5.2825 train_time:112013ms step_avg:247.82ms torch.cuda.memory_allocated()=369581568
14:05:32.955: step:463/1000 train_loss:5.5867 train_time:112273ms step_avg:247.84ms torch.cuda.memory_allocated()=369581568
14:05:33.213: step:464/1000 train_loss:5.5234 train_time:112532ms step_avg:247.87ms torch.cuda.memory_allocated()=369581568
14:05:33.472: step:465/1000 train_loss:5.3775 train_time:112790ms step_avg:247.89ms torch.cuda.memory_allocated()=369581568
14:05:33.729: step:466/1000 train_loss:5.4951 train_time:113048ms step_avg:247.91ms torch.cuda.memory_allocated()=369581568
14:05:33.988: step:467/1000 train_loss:5.3124 train_time:113307ms step_avg:247.94ms torch.cuda.memory_allocated()=369581568
14:05:34.245: step:468/1000 train_loss:5.2010 train_time:113564ms step_avg:247.96ms torch.cuda.memory_allocated()=369581568
14:05:34.504: step:469/1000 train_loss:5.4232 train_time:113823ms step_avg:247.98ms torch.cuda.memory_allocated()=369581568
14:05:34.762: step:470/1000 train_loss:5.1679 train_time:114081ms step_avg:248.00ms torch.cuda.memory_allocated()=369581568
14:05:35.018: step:471/1000 train_loss:5.2608 train_time:114337ms step_avg:248.02ms torch.cuda.memory_allocated()=369581568
14:05:35.275: step:472/1000 train_loss:5.4284 train_time:114594ms step_avg:248.04ms torch.cuda.memory_allocated()=369581568
14:05:35.530: step:473/1000 train_loss:5.2743 train_time:114849ms step_avg:248.05ms torch.cuda.memory_allocated()=369581568
14:05:35.791: step:474/1000 train_loss:5.5280 train_time:115110ms step_avg:248.08ms torch.cuda.memory_allocated()=369581568
14:05:36.053: step:475/1000 train_loss:5.5961 train_time:115372ms step_avg:248.11ms torch.cuda.memory_allocated()=369581568
14:05:36.312: step:476/1000 train_loss:5.5027 train_time:115631ms step_avg:248.13ms torch.cuda.memory_allocated()=369581568
14:05:36.572: step:477/1000 train_loss:5.1323 train_time:115890ms step_avg:248.16ms torch.cuda.memory_allocated()=369581568
14:05:36.830: step:478/1000 train_loss:5.3134 train_time:116149ms step_avg:248.18ms torch.cuda.memory_allocated()=369581568
14:05:37.090: step:479/1000 train_loss:5.7045 train_time:116409ms step_avg:248.21ms torch.cuda.memory_allocated()=369581568
14:05:37.349: step:480/1000 train_loss:5.1485 train_time:116668ms step_avg:248.23ms torch.cuda.memory_allocated()=369581568
14:05:37.607: step:481/1000 train_loss:5.5604 train_time:116925ms step_avg:248.25ms torch.cuda.memory_allocated()=369581568
14:05:37.867: step:482/1000 train_loss:5.2557 train_time:117186ms step_avg:248.28ms torch.cuda.memory_allocated()=369581568
14:05:38.124: step:483/1000 train_loss:5.1735 train_time:117443ms step_avg:248.29ms torch.cuda.memory_allocated()=369581568
14:05:38.383: step:484/1000 train_loss:5.3754 train_time:117702ms step_avg:248.32ms torch.cuda.memory_allocated()=369581568
14:05:38.639: step:485/1000 train_loss:5.4136 train_time:117958ms step_avg:248.33ms torch.cuda.memory_allocated()=369581568
14:05:38.901: step:486/1000 train_loss:5.3546 train_time:118220ms step_avg:248.36ms torch.cuda.memory_allocated()=369581568
14:05:39.162: step:487/1000 train_loss:5.3847 train_time:118481ms step_avg:248.39ms torch.cuda.memory_allocated()=369581568
14:05:39.421: step:488/1000 train_loss:5.2366 train_time:118740ms step_avg:248.41ms torch.cuda.memory_allocated()=369581568
14:05:39.678: step:489/1000 train_loss:5.1818 train_time:118997ms step_avg:248.43ms torch.cuda.memory_allocated()=369581568
14:05:39.939: step:490/1000 train_loss:5.3826 train_time:119257ms step_avg:248.45ms torch.cuda.memory_allocated()=369581568
14:05:40.195: step:491/1000 train_loss:5.0642 train_time:119514ms step_avg:248.47ms torch.cuda.memory_allocated()=369581568
14:05:40.455: step:492/1000 train_loss:5.1386 train_time:119774ms step_avg:248.49ms torch.cuda.memory_allocated()=369581568
14:05:40.715: step:493/1000 train_loss:5.0302 train_time:120033ms step_avg:248.52ms torch.cuda.memory_allocated()=369581568
14:05:40.975: step:494/1000 train_loss:5.7085 train_time:120293ms step_avg:248.54ms torch.cuda.memory_allocated()=369581568
14:05:41.239: step:495/1000 train_loss:4.9430 train_time:120558ms step_avg:248.57ms torch.cuda.memory_allocated()=369581568
14:05:41.496: step:496/1000 train_loss:5.0754 train_time:120815ms step_avg:248.59ms torch.cuda.memory_allocated()=369581568
14:05:41.753: step:497/1000 train_loss:5.0830 train_time:121072ms step_avg:248.61ms torch.cuda.memory_allocated()=369581568
14:05:42.009: step:498/1000 train_loss:5.1567 train_time:121328ms step_avg:248.62ms torch.cuda.memory_allocated()=369581568
14:05:42.273: step:499/1000 train_loss:5.3433 train_time:121592ms step_avg:248.65ms torch.cuda.memory_allocated()=369581568
14:05:42.531: step:500/1000 train_loss:5.0202 train_time:121850ms step_avg:248.67ms torch.cuda.memory_allocated()=369581568
14:05:45.258: step:500/1000 val_loss:5.4102 train_time:121851ms step_avg:248.67ms
14:05:45.520: step:501/1000 train_loss:4.8498 train_time:122112ms step_avg:248.70ms torch.cuda.memory_allocated()=369581568
14:05:45.779: step:502/1000 train_loss:5.3144 train_time:122371ms step_avg:248.72ms torch.cuda.memory_allocated()=369581568
14:05:46.036: step:503/1000 train_loss:5.2284 train_time:122628ms step_avg:248.74ms torch.cuda.memory_allocated()=369581568
14:05:46.292: step:504/1000 train_loss:5.2500 train_time:122884ms step_avg:248.75ms torch.cuda.memory_allocated()=369581568
14:05:46.550: step:505/1000 train_loss:5.4665 train_time:123142ms step_avg:248.77ms torch.cuda.memory_allocated()=369581568
14:05:46.808: step:506/1000 train_loss:5.3954 train_time:123400ms step_avg:248.79ms torch.cuda.memory_allocated()=369581568
14:05:47.073: step:507/1000 train_loss:5.4487 train_time:123665ms step_avg:248.82ms torch.cuda.memory_allocated()=369581568
14:05:47.329: step:508/1000 train_loss:5.2379 train_time:123921ms step_avg:248.84ms torch.cuda.memory_allocated()=369581568
14:05:47.586: step:509/1000 train_loss:5.1652 train_time:124178ms step_avg:248.85ms torch.cuda.memory_allocated()=369581568
14:05:47.850: step:510/1000 train_loss:4.9652 train_time:124442ms step_avg:248.88ms torch.cuda.memory_allocated()=369581568
14:05:48.109: step:511/1000 train_loss:5.4687 train_time:124701ms step_avg:248.90ms torch.cuda.memory_allocated()=369581568
14:05:48.368: step:512/1000 train_loss:5.2822 train_time:124960ms step_avg:248.92ms torch.cuda.memory_allocated()=369581568
14:05:48.630: step:513/1000 train_loss:5.2872 train_time:125222ms step_avg:248.95ms torch.cuda.memory_allocated()=369581568
14:05:48.889: step:514/1000 train_loss:5.5630 train_time:125481ms step_avg:248.97ms torch.cuda.memory_allocated()=369581568
14:05:49.148: step:515/1000 train_loss:5.3274 train_time:125740ms step_avg:248.99ms torch.cuda.memory_allocated()=369581568
14:05:49.409: step:516/1000 train_loss:5.3891 train_time:126001ms step_avg:249.01ms torch.cuda.memory_allocated()=369581568
14:05:49.666: step:517/1000 train_loss:5.1572 train_time:126258ms step_avg:249.03ms torch.cuda.memory_allocated()=369581568
14:05:49.922: step:518/1000 train_loss:5.1279 train_time:126514ms step_avg:249.04ms torch.cuda.memory_allocated()=369581568
14:05:50.182: step:519/1000 train_loss:5.3878 train_time:126774ms step_avg:249.06ms torch.cuda.memory_allocated()=369581568
14:05:50.443: step:520/1000 train_loss:5.2460 train_time:127035ms step_avg:249.09ms torch.cuda.memory_allocated()=369581568
14:05:50.706: step:521/1000 train_loss:4.6894 train_time:127298ms step_avg:249.12ms torch.cuda.memory_allocated()=369581568
14:05:50.971: step:522/1000 train_loss:5.0957 train_time:127563ms step_avg:249.15ms torch.cuda.memory_allocated()=369581568
14:05:51.236: step:523/1000 train_loss:5.1850 train_time:127828ms step_avg:249.18ms torch.cuda.memory_allocated()=369581568
14:05:51.501: step:524/1000 train_loss:5.4386 train_time:128093ms step_avg:249.21ms torch.cuda.memory_allocated()=369581568
14:05:51.759: step:525/1000 train_loss:5.0291 train_time:128351ms step_avg:249.22ms torch.cuda.memory_allocated()=369581568
14:05:52.016: step:526/1000 train_loss:5.1575 train_time:128608ms step_avg:249.24ms torch.cuda.memory_allocated()=369581568
14:05:52.275: step:527/1000 train_loss:5.4629 train_time:128867ms step_avg:249.26ms torch.cuda.memory_allocated()=369581568
14:05:52.543: step:528/1000 train_loss:5.2224 train_time:129135ms step_avg:249.30ms torch.cuda.memory_allocated()=369581568
14:05:52.803: step:529/1000 train_loss:5.0092 train_time:129395ms step_avg:249.32ms torch.cuda.memory_allocated()=369581568
14:05:53.060: step:530/1000 train_loss:5.2052 train_time:129652ms step_avg:249.33ms torch.cuda.memory_allocated()=369581568
14:05:53.321: step:531/1000 train_loss:5.0279 train_time:129913ms step_avg:249.35ms torch.cuda.memory_allocated()=369581568
14:05:53.583: step:532/1000 train_loss:5.2747 train_time:130175ms step_avg:249.38ms torch.cuda.memory_allocated()=369581568
14:05:53.844: step:533/1000 train_loss:5.3731 train_time:130436ms step_avg:249.40ms torch.cuda.memory_allocated()=369581568
14:05:54.100: step:534/1000 train_loss:5.3365 train_time:130692ms step_avg:249.41ms torch.cuda.memory_allocated()=369581568
14:05:54.361: step:535/1000 train_loss:5.5770 train_time:130953ms step_avg:249.43ms torch.cuda.memory_allocated()=369581568
14:05:54.619: step:536/1000 train_loss:5.4518 train_time:131211ms step_avg:249.45ms torch.cuda.memory_allocated()=369581568
14:05:54.886: step:537/1000 train_loss:4.9934 train_time:131478ms step_avg:249.48ms torch.cuda.memory_allocated()=369581568
14:05:55.145: step:538/1000 train_loss:5.0427 train_time:131737ms step_avg:249.50ms torch.cuda.memory_allocated()=369581568
14:05:55.407: step:539/1000 train_loss:5.7906 train_time:131999ms step_avg:249.53ms torch.cuda.memory_allocated()=369581568
14:05:55.672: step:540/1000 train_loss:5.3140 train_time:132264ms step_avg:249.55ms torch.cuda.memory_allocated()=369581568
14:05:55.931: step:541/1000 train_loss:5.0479 train_time:132523ms step_avg:249.57ms torch.cuda.memory_allocated()=369581568
14:05:56.190: step:542/1000 train_loss:5.1987 train_time:132782ms step_avg:249.59ms torch.cuda.memory_allocated()=369581568
14:05:56.453: step:543/1000 train_loss:5.8378 train_time:133045ms step_avg:249.61ms torch.cuda.memory_allocated()=369581568
14:05:56.716: step:544/1000 train_loss:5.0729 train_time:133308ms step_avg:249.64ms torch.cuda.memory_allocated()=369581568
14:05:56.974: step:545/1000 train_loss:5.2704 train_time:133566ms step_avg:249.66ms torch.cuda.memory_allocated()=369581568
14:05:57.233: step:546/1000 train_loss:5.3477 train_time:133825ms step_avg:249.67ms torch.cuda.memory_allocated()=369581568
14:05:57.499: step:547/1000 train_loss:5.1942 train_time:134091ms step_avg:249.70ms torch.cuda.memory_allocated()=369581568
14:05:57.759: step:548/1000 train_loss:4.9999 train_time:134351ms step_avg:249.72ms torch.cuda.memory_allocated()=369581568
14:05:58.023: step:549/1000 train_loss:5.3695 train_time:134615ms step_avg:249.75ms torch.cuda.memory_allocated()=369581568
14:05:58.283: step:550/1000 train_loss:4.9700 train_time:134875ms step_avg:249.77ms torch.cuda.memory_allocated()=369581568
14:05:58.545: step:551/1000 train_loss:5.4997 train_time:135137ms step_avg:249.79ms torch.cuda.memory_allocated()=369581568
14:05:58.805: step:552/1000 train_loss:5.2104 train_time:135397ms step_avg:249.81ms torch.cuda.memory_allocated()=369581568
14:05:59.076: step:553/1000 train_loss:5.4093 train_time:135668ms step_avg:249.85ms torch.cuda.memory_allocated()=369581568
14:05:59.346: step:554/1000 train_loss:5.0029 train_time:135938ms step_avg:249.89ms torch.cuda.memory_allocated()=369581568
14:05:59.608: step:555/1000 train_loss:5.3851 train_time:136200ms step_avg:249.91ms torch.cuda.memory_allocated()=369581568
14:05:59.870: step:556/1000 train_loss:5.2653 train_time:136462ms step_avg:249.93ms torch.cuda.memory_allocated()=369581568
14:06:00.130: step:557/1000 train_loss:5.3298 train_time:136722ms step_avg:249.95ms torch.cuda.memory_allocated()=369581568
14:06:00.395: step:558/1000 train_loss:5.3963 train_time:136987ms step_avg:249.98ms torch.cuda.memory_allocated()=369581568
14:06:00.660: step:559/1000 train_loss:5.6997 train_time:137251ms step_avg:250.00ms torch.cuda.memory_allocated()=369581568
14:06:00.917: step:560/1000 train_loss:5.1906 train_time:137509ms step_avg:250.02ms torch.cuda.memory_allocated()=369581568
14:06:01.178: step:561/1000 train_loss:5.3204 train_time:137770ms step_avg:250.04ms torch.cuda.memory_allocated()=369581568
14:06:01.441: step:562/1000 train_loss:5.4498 train_time:138033ms step_avg:250.06ms torch.cuda.memory_allocated()=369581568
14:06:01.714: step:563/1000 train_loss:5.8237 train_time:138306ms step_avg:250.10ms torch.cuda.memory_allocated()=369581568
14:06:01.985: step:564/1000 train_loss:5.6859 train_time:138577ms step_avg:250.14ms torch.cuda.memory_allocated()=369581568
14:06:02.244: step:565/1000 train_loss:5.1951 train_time:138836ms step_avg:250.16ms torch.cuda.memory_allocated()=369581568
14:06:02.506: step:566/1000 train_loss:5.3720 train_time:139098ms step_avg:250.18ms torch.cuda.memory_allocated()=369581568
14:06:02.766: step:567/1000 train_loss:5.0774 train_time:139358ms step_avg:250.19ms torch.cuda.memory_allocated()=369581568
14:06:03.027: step:568/1000 train_loss:4.9913 train_time:139619ms step_avg:250.21ms torch.cuda.memory_allocated()=369581568
14:06:03.287: step:569/1000 train_loss:5.5419 train_time:139879ms step_avg:250.23ms torch.cuda.memory_allocated()=369581568
14:06:03.546: step:570/1000 train_loss:5.1111 train_time:140138ms step_avg:250.25ms torch.cuda.memory_allocated()=369581568
14:06:03.803: step:571/1000 train_loss:5.1962 train_time:140395ms step_avg:250.26ms torch.cuda.memory_allocated()=369581568
14:06:04.067: step:572/1000 train_loss:5.2568 train_time:140658ms step_avg:250.28ms torch.cuda.memory_allocated()=369581568
14:06:04.329: step:573/1000 train_loss:5.2244 train_time:140921ms step_avg:250.30ms torch.cuda.memory_allocated()=369581568
14:06:04.585: step:574/1000 train_loss:5.1816 train_time:141177ms step_avg:250.31ms torch.cuda.memory_allocated()=369581568
14:06:04.842: step:575/1000 train_loss:5.2592 train_time:141434ms step_avg:250.32ms torch.cuda.memory_allocated()=369581568
14:06:05.101: step:576/1000 train_loss:5.3577 train_time:141693ms step_avg:250.34ms torch.cuda.memory_allocated()=369581568
14:06:05.361: step:577/1000 train_loss:5.1195 train_time:141953ms step_avg:250.36ms torch.cuda.memory_allocated()=369581568
14:06:05.619: step:578/1000 train_loss:5.5308 train_time:142211ms step_avg:250.37ms torch.cuda.memory_allocated()=369581568
14:06:05.876: step:579/1000 train_loss:5.2854 train_time:142468ms step_avg:250.38ms torch.cuda.memory_allocated()=369581568
14:06:06.141: step:580/1000 train_loss:4.9083 train_time:142733ms step_avg:250.41ms torch.cuda.memory_allocated()=369581568
14:06:06.410: step:581/1000 train_loss:5.2568 train_time:143002ms step_avg:250.44ms torch.cuda.memory_allocated()=369581568
14:06:06.670: step:582/1000 train_loss:5.4398 train_time:143262ms step_avg:250.46ms torch.cuda.memory_allocated()=369581568
14:06:06.934: step:583/1000 train_loss:5.0753 train_time:143526ms step_avg:250.48ms torch.cuda.memory_allocated()=369581568
14:06:07.194: step:584/1000 train_loss:5.2955 train_time:143786ms step_avg:250.50ms torch.cuda.memory_allocated()=369581568
14:06:07.453: step:585/1000 train_loss:5.3128 train_time:144045ms step_avg:250.51ms torch.cuda.memory_allocated()=369581568
14:06:07.722: step:586/1000 train_loss:5.4612 train_time:144314ms step_avg:250.55ms torch.cuda.memory_allocated()=369581568
14:06:07.985: step:587/1000 train_loss:5.2617 train_time:144577ms step_avg:250.57ms torch.cuda.memory_allocated()=369581568
14:06:08.246: step:588/1000 train_loss:5.2427 train_time:144838ms step_avg:250.59ms torch.cuda.memory_allocated()=369581568
14:06:08.507: step:589/1000 train_loss:5.5768 train_time:145099ms step_avg:250.60ms torch.cuda.memory_allocated()=369581568
14:06:08.778: step:590/1000 train_loss:6.3804 train_time:145370ms step_avg:250.64ms torch.cuda.memory_allocated()=369581568
14:06:09.036: step:591/1000 train_loss:5.3414 train_time:145628ms step_avg:250.65ms torch.cuda.memory_allocated()=369581568
14:06:09.290: step:592/1000 train_loss:5.1982 train_time:145882ms step_avg:250.66ms torch.cuda.memory_allocated()=369581568
14:06:09.552: step:593/1000 train_loss:4.8709 train_time:146144ms step_avg:250.68ms torch.cuda.memory_allocated()=369581568
14:06:09.817: step:594/1000 train_loss:5.1901 train_time:146409ms step_avg:250.70ms torch.cuda.memory_allocated()=369581568
14:06:10.079: step:595/1000 train_loss:5.3818 train_time:146671ms step_avg:250.72ms torch.cuda.memory_allocated()=369581568
14:06:10.345: step:596/1000 train_loss:5.0031 train_time:146937ms step_avg:250.75ms torch.cuda.memory_allocated()=369581568
14:06:10.616: step:597/1000 train_loss:5.1893 train_time:147208ms step_avg:250.78ms torch.cuda.memory_allocated()=369581568
14:06:10.888: step:598/1000 train_loss:5.2247 train_time:147480ms step_avg:250.82ms torch.cuda.memory_allocated()=369581568
14:06:11.147: step:599/1000 train_loss:5.1087 train_time:147739ms step_avg:250.83ms torch.cuda.memory_allocated()=369581568
14:06:11.422: step:600/1000 train_loss:6.0048 train_time:148014ms step_avg:250.87ms torch.cuda.memory_allocated()=369581568
14:06:11.681: step:601/1000 train_loss:4.7962 train_time:148273ms step_avg:250.89ms torch.cuda.memory_allocated()=369581568
14:06:11.946: step:602/1000 train_loss:5.0903 train_time:148537ms step_avg:250.91ms torch.cuda.memory_allocated()=369581568
14:06:12.204: step:603/1000 train_loss:5.2076 train_time:148796ms step_avg:250.92ms torch.cuda.memory_allocated()=369581568
14:06:12.464: step:604/1000 train_loss:5.3122 train_time:149056ms step_avg:250.94ms torch.cuda.memory_allocated()=369581568
14:06:12.725: step:605/1000 train_loss:5.1565 train_time:149317ms step_avg:250.95ms torch.cuda.memory_allocated()=369581568
14:06:12.991: step:606/1000 train_loss:5.2569 train_time:149583ms step_avg:250.98ms torch.cuda.memory_allocated()=369581568
14:06:13.250: step:607/1000 train_loss:5.2878 train_time:149842ms step_avg:250.99ms torch.cuda.memory_allocated()=369581568
14:06:13.518: step:608/1000 train_loss:5.4864 train_time:150110ms step_avg:251.02ms torch.cuda.memory_allocated()=369581568
14:06:13.779: step:609/1000 train_loss:5.1132 train_time:150371ms step_avg:251.04ms torch.cuda.memory_allocated()=369581568
14:06:14.042: step:610/1000 train_loss:5.0744 train_time:150634ms step_avg:251.06ms torch.cuda.memory_allocated()=369581568
14:06:14.310: step:611/1000 train_loss:4.6747 train_time:150902ms step_avg:251.09ms torch.cuda.memory_allocated()=369581568
14:06:14.573: step:612/1000 train_loss:4.8305 train_time:151165ms step_avg:251.10ms torch.cuda.memory_allocated()=369581568
14:06:14.836: step:613/1000 train_loss:4.9674 train_time:151428ms step_avg:251.12ms torch.cuda.memory_allocated()=369581568
14:06:15.101: step:614/1000 train_loss:5.0178 train_time:151693ms step_avg:251.15ms torch.cuda.memory_allocated()=369581568
14:06:15.366: step:615/1000 train_loss:4.9700 train_time:151958ms step_avg:251.17ms torch.cuda.memory_allocated()=369581568
14:06:15.623: step:616/1000 train_loss:5.0937 train_time:152215ms step_avg:251.18ms torch.cuda.memory_allocated()=369581568
14:06:15.887: step:617/1000 train_loss:5.0600 train_time:152479ms step_avg:251.20ms torch.cuda.memory_allocated()=369581568
14:06:16.145: step:618/1000 train_loss:5.2979 train_time:152736ms step_avg:251.21ms torch.cuda.memory_allocated()=369581568
14:06:16.408: step:619/1000 train_loss:5.2251 train_time:153000ms step_avg:251.23ms torch.cuda.memory_allocated()=369581568
14:06:16.670: step:620/1000 train_loss:5.1080 train_time:153262ms step_avg:251.25ms torch.cuda.memory_allocated()=369581568
14:06:16.930: step:621/1000 train_loss:5.1825 train_time:153522ms step_avg:251.26ms torch.cuda.memory_allocated()=369581568
14:06:17.194: step:622/1000 train_loss:5.0485 train_time:153786ms step_avg:251.28ms torch.cuda.memory_allocated()=369581568
14:06:17.461: step:623/1000 train_loss:5.0622 train_time:154053ms step_avg:251.31ms torch.cuda.memory_allocated()=369581568
14:06:17.719: step:624/1000 train_loss:5.2240 train_time:154311ms step_avg:251.32ms torch.cuda.memory_allocated()=369581568
14:06:17.978: step:625/1000 train_loss:5.1204 train_time:154570ms step_avg:251.33ms torch.cuda.memory_allocated()=369581568
14:06:20.742: step:625/1000 val_loss:5.2396 train_time:154570ms step_avg:251.33ms
14:06:20.000: step:626/1000 train_loss:5.2819 train_time:154828ms step_avg:251.34ms torch.cuda.memory_allocated()=369581568
14:06:21.263: step:627/1000 train_loss:5.2184 train_time:155091ms step_avg:251.36ms torch.cuda.memory_allocated()=369581568
14:06:21.530: step:628/1000 train_loss:5.4910 train_time:155358ms step_avg:251.39ms torch.cuda.memory_allocated()=369581568
14:06:21.795: step:629/1000 train_loss:5.0307 train_time:155623ms step_avg:251.41ms torch.cuda.memory_allocated()=369581568
14:06:22.060: step:630/1000 train_loss:5.1134 train_time:155888ms step_avg:251.43ms torch.cuda.memory_allocated()=369581568
14:06:22.324: step:631/1000 train_loss:5.2239 train_time:156152ms step_avg:251.45ms torch.cuda.memory_allocated()=369581568
14:06:22.590: step:632/1000 train_loss:4.9915 train_time:156418ms step_avg:251.48ms torch.cuda.memory_allocated()=369581568
14:06:22.857: step:633/1000 train_loss:5.1863 train_time:156685ms step_avg:251.50ms torch.cuda.memory_allocated()=369581568
14:06:23.128: step:634/1000 train_loss:4.8968 train_time:156956ms step_avg:251.53ms torch.cuda.memory_allocated()=369581568
14:06:23.388: step:635/1000 train_loss:5.1221 train_time:157216ms step_avg:251.54ms torch.cuda.memory_allocated()=369581568
14:06:23.656: step:636/1000 train_loss:5.2324 train_time:157484ms step_avg:251.57ms torch.cuda.memory_allocated()=369581568
14:06:23.914: step:637/1000 train_loss:5.2702 train_time:157742ms step_avg:251.58ms torch.cuda.memory_allocated()=369581568
14:06:24.175: step:638/1000 train_loss:5.1981 train_time:158003ms step_avg:251.60ms torch.cuda.memory_allocated()=369581568
14:06:24.431: step:639/1000 train_loss:5.0876 train_time:158258ms step_avg:251.60ms torch.cuda.memory_allocated()=369581568
14:06:24.686: step:640/1000 train_loss:4.9582 train_time:158514ms step_avg:251.61ms torch.cuda.memory_allocated()=369581568
14:06:24.947: step:641/1000 train_loss:4.9043 train_time:158775ms step_avg:251.62ms torch.cuda.memory_allocated()=369581568
14:06:25.210: step:642/1000 train_loss:5.2332 train_time:159038ms step_avg:251.64ms torch.cuda.memory_allocated()=369581568
14:06:25.476: step:643/1000 train_loss:5.1013 train_time:159304ms step_avg:251.66ms torch.cuda.memory_allocated()=369581568
14:06:25.742: step:644/1000 train_loss:5.2111 train_time:159570ms step_avg:251.69ms torch.cuda.memory_allocated()=369581568
14:06:25.002: step:645/1000 train_loss:5.2408 train_time:159830ms step_avg:251.70ms torch.cuda.memory_allocated()=369581568
14:06:26.269: step:646/1000 train_loss:5.0851 train_time:160097ms step_avg:251.72ms torch.cuda.memory_allocated()=369581568
14:06:26.537: step:647/1000 train_loss:5.1292 train_time:160365ms step_avg:251.75ms torch.cuda.memory_allocated()=369581568
14:06:26.800: step:648/1000 train_loss:5.0931 train_time:160628ms step_avg:251.77ms torch.cuda.memory_allocated()=369581568
14:06:27.060: step:649/1000 train_loss:5.1869 train_time:160888ms step_avg:251.78ms torch.cuda.memory_allocated()=369581568
14:06:27.322: step:650/1000 train_loss:5.0336 train_time:161150ms step_avg:251.80ms torch.cuda.memory_allocated()=369581568
14:06:27.587: step:651/1000 train_loss:4.9585 train_time:161415ms step_avg:251.82ms torch.cuda.memory_allocated()=369581568
14:06:27.855: step:652/1000 train_loss:4.9113 train_time:161683ms step_avg:251.84ms torch.cuda.memory_allocated()=369581568
14:06:28.117: step:653/1000 train_loss:4.8183 train_time:161945ms step_avg:251.86ms torch.cuda.memory_allocated()=369581568
14:06:28.380: step:654/1000 train_loss:4.9068 train_time:162208ms step_avg:251.88ms torch.cuda.memory_allocated()=369581568
14:06:28.642: step:655/1000 train_loss:4.9247 train_time:162470ms step_avg:251.89ms torch.cuda.memory_allocated()=369581568
14:06:28.904: step:656/1000 train_loss:5.1870 train_time:162732ms step_avg:251.91ms torch.cuda.memory_allocated()=369581568
14:06:29.162: step:657/1000 train_loss:5.1054 train_time:162990ms step_avg:251.92ms torch.cuda.memory_allocated()=369581568
14:06:29.424: step:658/1000 train_loss:5.0722 train_time:163252ms step_avg:251.93ms torch.cuda.memory_allocated()=369581568
14:06:29.690: step:659/1000 train_loss:5.4525 train_time:163518ms step_avg:251.95ms torch.cuda.memory_allocated()=369581568
14:06:29.954: step:660/1000 train_loss:5.4215 train_time:163782ms step_avg:251.97ms torch.cuda.memory_allocated()=369581568
14:06:30.212: step:661/1000 train_loss:5.0844 train_time:164040ms step_avg:251.98ms torch.cuda.memory_allocated()=369581568
14:06:30.474: step:662/1000 train_loss:4.9466 train_time:164302ms step_avg:252.00ms torch.cuda.memory_allocated()=369581568
14:06:30.736: step:663/1000 train_loss:5.0554 train_time:164564ms step_avg:252.01ms torch.cuda.memory_allocated()=369581568
14:06:30.997: step:664/1000 train_loss:5.0692 train_time:164825ms step_avg:252.03ms torch.cuda.memory_allocated()=369581568
14:06:31.253: step:665/1000 train_loss:5.2684 train_time:165081ms step_avg:252.03ms torch.cuda.memory_allocated()=369581568
14:06:31.514: step:666/1000 train_loss:5.4782 train_time:165342ms step_avg:252.05ms torch.cuda.memory_allocated()=369581568
14:06:31.777: step:667/1000 train_loss:4.9543 train_time:165605ms step_avg:252.06ms torch.cuda.memory_allocated()=369581568
14:06:32.037: step:668/1000 train_loss:5.2167 train_time:165865ms step_avg:252.07ms torch.cuda.memory_allocated()=369581568
14:06:32.307: step:669/1000 train_loss:5.1794 train_time:166135ms step_avg:252.10ms torch.cuda.memory_allocated()=369581568
14:06:32.565: step:670/1000 train_loss:5.0606 train_time:166393ms step_avg:252.11ms torch.cuda.memory_allocated()=369581568
14:06:32.827: step:671/1000 train_loss:5.1688 train_time:166655ms step_avg:252.13ms torch.cuda.memory_allocated()=369581568
14:06:33.084: step:672/1000 train_loss:5.0071 train_time:166912ms step_avg:252.13ms torch.cuda.memory_allocated()=369581568
14:06:33.347: step:673/1000 train_loss:5.3578 train_time:167175ms step_avg:252.15ms torch.cuda.memory_allocated()=369581568
14:06:33.626: step:674/1000 train_loss:5.5939 train_time:167454ms step_avg:252.19ms torch.cuda.memory_allocated()=369581568
14:06:33.889: step:675/1000 train_loss:5.0362 train_time:167716ms step_avg:252.21ms torch.cuda.memory_allocated()=369581568
14:06:34.154: step:676/1000 train_loss:5.0426 train_time:167982ms step_avg:252.22ms torch.cuda.memory_allocated()=369581568
14:06:34.428: step:677/1000 train_loss:5.1528 train_time:168256ms step_avg:252.26ms torch.cuda.memory_allocated()=369581568
14:06:34.708: step:678/1000 train_loss:5.3285 train_time:168536ms step_avg:252.30ms torch.cuda.memory_allocated()=369581568
14:06:34.968: step:679/1000 train_loss:5.1238 train_time:168796ms step_avg:252.31ms torch.cuda.memory_allocated()=369581568
14:06:35.249: step:680/1000 train_loss:5.0052 train_time:169077ms step_avg:252.35ms torch.cuda.memory_allocated()=369581568
14:06:35.529: step:681/1000 train_loss:4.8794 train_time:169357ms step_avg:252.40ms torch.cuda.memory_allocated()=369581568
14:06:35.805: step:682/1000 train_loss:4.9066 train_time:169633ms step_avg:252.43ms torch.cuda.memory_allocated()=369581568
14:06:36.068: step:683/1000 train_loss:5.0346 train_time:169896ms step_avg:252.45ms torch.cuda.memory_allocated()=369581568
14:06:36.336: step:684/1000 train_loss:5.1384 train_time:170164ms step_avg:252.47ms torch.cuda.memory_allocated()=369581568
14:06:36.605: step:685/1000 train_loss:5.4674 train_time:170433ms step_avg:252.49ms torch.cuda.memory_allocated()=369581568
14:06:36.880: step:686/1000 train_loss:5.0690 train_time:170708ms step_avg:252.53ms torch.cuda.memory_allocated()=369581568
14:06:37.143: step:687/1000 train_loss:4.9664 train_time:170971ms step_avg:252.54ms torch.cuda.memory_allocated()=369581568
14:06:37.411: step:688/1000 train_loss:5.7170 train_time:171239ms step_avg:252.56ms torch.cuda.memory_allocated()=369581568
14:06:37.670: step:689/1000 train_loss:4.7951 train_time:171498ms step_avg:252.57ms torch.cuda.memory_allocated()=369581568
14:06:37.934: step:690/1000 train_loss:4.9483 train_time:171762ms step_avg:252.59ms torch.cuda.memory_allocated()=369581568
14:06:38.201: step:691/1000 train_loss:5.0906 train_time:172029ms step_avg:252.61ms torch.cuda.memory_allocated()=369581568
14:06:38.464: step:692/1000 train_loss:4.8254 train_time:172292ms step_avg:252.63ms torch.cuda.memory_allocated()=369581568
14:06:38.733: step:693/1000 train_loss:4.9045 train_time:172561ms step_avg:252.65ms torch.cuda.memory_allocated()=369581568
14:06:39.009: step:694/1000 train_loss:5.3357 train_time:172837ms step_avg:252.69ms torch.cuda.memory_allocated()=369581568
14:06:39.268: step:695/1000 train_loss:4.9545 train_time:173096ms step_avg:252.69ms torch.cuda.memory_allocated()=369581568
14:06:39.534: step:696/1000 train_loss:5.0896 train_time:173362ms step_avg:252.71ms torch.cuda.memory_allocated()=369581568
14:06:39.798: step:697/1000 train_loss:5.1984 train_time:173626ms step_avg:252.73ms torch.cuda.memory_allocated()=369581568
14:06:40.060: step:698/1000 train_loss:4.8626 train_time:173888ms step_avg:252.74ms torch.cuda.memory_allocated()=369581568
14:06:40.323: step:699/1000 train_loss:5.1775 train_time:174151ms step_avg:252.76ms torch.cuda.memory_allocated()=369581568
14:06:40.598: step:700/1000 train_loss:6.0002 train_time:174426ms step_avg:252.79ms torch.cuda.memory_allocated()=369581568
14:06:40.867: step:701/1000 train_loss:4.7909 train_time:174695ms step_avg:252.81ms torch.cuda.memory_allocated()=369581568
14:06:41.136: step:702/1000 train_loss:4.8029 train_time:174964ms step_avg:252.84ms torch.cuda.memory_allocated()=369581568
14:06:41.401: step:703/1000 train_loss:5.0908 train_time:175228ms step_avg:252.85ms torch.cuda.memory_allocated()=369581568
14:06:41.664: step:704/1000 train_loss:5.1220 train_time:175492ms step_avg:252.87ms torch.cuda.memory_allocated()=369581568
14:06:41.926: step:705/1000 train_loss:5.1769 train_time:175754ms step_avg:252.88ms torch.cuda.memory_allocated()=369581568
14:06:42.182: step:706/1000 train_loss:4.9692 train_time:176010ms step_avg:252.89ms torch.cuda.memory_allocated()=369581568
14:06:42.445: step:707/1000 train_loss:5.2415 train_time:176273ms step_avg:252.90ms torch.cuda.memory_allocated()=369581568
14:06:42.709: step:708/1000 train_loss:4.9572 train_time:176537ms step_avg:252.92ms torch.cuda.memory_allocated()=369581568
14:06:42.972: step:709/1000 train_loss:4.9376 train_time:176800ms step_avg:252.93ms torch.cuda.memory_allocated()=369581568
14:06:43.236: step:710/1000 train_loss:5.0621 train_time:177063ms step_avg:252.95ms torch.cuda.memory_allocated()=369581568
14:06:43.511: step:711/1000 train_loss:5.1375 train_time:177339ms step_avg:252.98ms torch.cuda.memory_allocated()=369581568
14:06:43.772: step:712/1000 train_loss:5.0866 train_time:177600ms step_avg:252.99ms torch.cuda.memory_allocated()=369581568
14:06:44.039: step:713/1000 train_loss:5.4722 train_time:177867ms step_avg:253.01ms torch.cuda.memory_allocated()=369581568
14:06:44.304: step:714/1000 train_loss:4.9931 train_time:178131ms step_avg:253.03ms torch.cuda.memory_allocated()=369581568
14:06:44.577: step:715/1000 train_loss:4.7846 train_time:178405ms step_avg:253.06ms torch.cuda.memory_allocated()=369581568
14:06:44.840: step:716/1000 train_loss:4.9748 train_time:178668ms step_avg:253.07ms torch.cuda.memory_allocated()=369581568
14:06:45.106: step:717/1000 train_loss:4.8038 train_time:178934ms step_avg:253.09ms torch.cuda.memory_allocated()=369581568
14:06:45.369: step:718/1000 train_loss:4.9435 train_time:179196ms step_avg:253.10ms torch.cuda.memory_allocated()=369581568
14:06:45.638: step:719/1000 train_loss:4.9944 train_time:179466ms step_avg:253.12ms torch.cuda.memory_allocated()=369581568
14:06:45.902: step:720/1000 train_loss:4.8477 train_time:179729ms step_avg:253.14ms torch.cuda.memory_allocated()=369581568
14:06:46.164: step:721/1000 train_loss:5.0939 train_time:179992ms step_avg:253.15ms torch.cuda.memory_allocated()=369581568
14:06:46.425: step:722/1000 train_loss:5.1355 train_time:180253ms step_avg:253.16ms torch.cuda.memory_allocated()=369581568
14:06:46.689: step:723/1000 train_loss:4.7632 train_time:180517ms step_avg:253.18ms torch.cuda.memory_allocated()=369581568
14:06:46.949: step:724/1000 train_loss:4.8886 train_time:180777ms step_avg:253.19ms torch.cuda.memory_allocated()=369581568
14:06:47.222: step:725/1000 train_loss:5.1315 train_time:181050ms step_avg:253.22ms torch.cuda.memory_allocated()=369581568
14:06:47.485: step:726/1000 train_loss:5.0400 train_time:181313ms step_avg:253.23ms torch.cuda.memory_allocated()=369581568
14:06:47.744: step:727/1000 train_loss:4.9851 train_time:181572ms step_avg:253.24ms torch.cuda.memory_allocated()=369581568
14:06:47.004: step:728/1000 train_loss:5.0395 train_time:181832ms step_avg:253.25ms torch.cuda.memory_allocated()=369581568
14:06:48.265: step:729/1000 train_loss:4.7930 train_time:182093ms step_avg:253.26ms torch.cuda.memory_allocated()=369581568
14:06:48.525: step:730/1000 train_loss:5.0067 train_time:182353ms step_avg:253.27ms torch.cuda.memory_allocated()=369581568
14:06:48.787: step:731/1000 train_loss:4.7898 train_time:182615ms step_avg:253.28ms torch.cuda.memory_allocated()=369581568
14:06:49.049: step:732/1000 train_loss:4.7728 train_time:182877ms step_avg:253.29ms torch.cuda.memory_allocated()=369581568
14:06:49.320: step:733/1000 train_loss:4.5053 train_time:183148ms step_avg:253.32ms torch.cuda.memory_allocated()=369581568
14:06:49.583: step:734/1000 train_loss:4.6910 train_time:183411ms step_avg:253.33ms torch.cuda.memory_allocated()=369581568
14:06:49.847: step:735/1000 train_loss:4.9688 train_time:183675ms step_avg:253.34ms torch.cuda.memory_allocated()=369581568
14:06:50.125: step:736/1000 train_loss:4.9357 train_time:183953ms step_avg:253.38ms torch.cuda.memory_allocated()=369581568
14:06:50.397: step:737/1000 train_loss:5.7386 train_time:184225ms step_avg:253.40ms torch.cuda.memory_allocated()=369581568
14:06:50.662: step:738/1000 train_loss:5.1783 train_time:184490ms step_avg:253.42ms torch.cuda.memory_allocated()=369581568
14:06:50.924: step:739/1000 train_loss:4.9977 train_time:184752ms step_avg:253.43ms torch.cuda.memory_allocated()=369581568
14:06:51.188: step:740/1000 train_loss:4.8774 train_time:185015ms step_avg:253.45ms torch.cuda.memory_allocated()=369581568
14:06:51.453: step:741/1000 train_loss:4.8554 train_time:185281ms step_avg:253.46ms torch.cuda.memory_allocated()=369581568
14:06:51.718: step:742/1000 train_loss:4.9785 train_time:185546ms step_avg:253.48ms torch.cuda.memory_allocated()=369581568
14:06:51.985: step:743/1000 train_loss:4.9000 train_time:185813ms step_avg:253.50ms torch.cuda.memory_allocated()=369581568
14:06:52.256: step:744/1000 train_loss:4.8265 train_time:186084ms step_avg:253.52ms torch.cuda.memory_allocated()=369581568
14:06:52.553: step:745/1000 train_loss:4.8954 train_time:186381ms step_avg:253.58ms torch.cuda.memory_allocated()=369581568
14:06:52.843: step:746/1000 train_loss:4.9815 train_time:186671ms step_avg:253.63ms torch.cuda.memory_allocated()=369581568
14:06:53.138: step:747/1000 train_loss:4.9955 train_time:186966ms step_avg:253.69ms torch.cuda.memory_allocated()=369581568
14:06:53.441: step:748/1000 train_loss:4.7797 train_time:187269ms step_avg:253.75ms torch.cuda.memory_allocated()=369581568
14:06:53.712: step:749/1000 train_loss:4.9601 train_time:187539ms step_avg:253.77ms torch.cuda.memory_allocated()=369581568
14:06:53.986: step:750/1000 train_loss:4.9617 train_time:187814ms step_avg:253.80ms torch.cuda.memory_allocated()=369581568
14:06:56.784: step:750/1000 val_loss:5.0438 train_time:187815ms step_avg:253.80ms
14:06:57.044: step:751/1000 train_loss:4.9237 train_time:188075ms step_avg:253.81ms torch.cuda.memory_allocated()=369581568
14:06:57.318: step:752/1000 train_loss:5.0178 train_time:188349ms step_avg:253.84ms torch.cuda.memory_allocated()=369581568
14:06:57.596: step:753/1000 train_loss:4.8014 train_time:188627ms step_avg:253.87ms torch.cuda.memory_allocated()=369581568
14:06:57.860: step:754/1000 train_loss:5.2349 train_time:188890ms step_avg:253.88ms torch.cuda.memory_allocated()=369581568
14:06:58.119: step:755/1000 train_loss:4.9591 train_time:189149ms step_avg:253.89ms torch.cuda.memory_allocated()=369581568
14:06:58.380: step:756/1000 train_loss:5.0058 train_time:189411ms step_avg:253.90ms torch.cuda.memory_allocated()=369581568
14:06:58.644: step:757/1000 train_loss:4.9771 train_time:189674ms step_avg:253.91ms torch.cuda.memory_allocated()=369581568
14:06:58.907: step:758/1000 train_loss:5.1397 train_time:189937ms step_avg:253.93ms torch.cuda.memory_allocated()=369581568
14:06:59.170: step:759/1000 train_loss:4.6614 train_time:190200ms step_avg:253.94ms torch.cuda.memory_allocated()=369581568
14:06:59.434: step:760/1000 train_loss:4.9540 train_time:190464ms step_avg:253.95ms torch.cuda.memory_allocated()=369581568
14:06:59.697: step:761/1000 train_loss:4.7800 train_time:190728ms step_avg:253.96ms torch.cuda.memory_allocated()=369581568
14:06:59.961: step:762/1000 train_loss:4.7135 train_time:190991ms step_avg:253.98ms torch.cuda.memory_allocated()=369581568
14:07:00.231: step:763/1000 train_loss:4.7281 train_time:191262ms step_avg:254.00ms torch.cuda.memory_allocated()=369581568
14:07:00.499: step:764/1000 train_loss:4.8531 train_time:191530ms step_avg:254.02ms torch.cuda.memory_allocated()=369581568
14:07:00.800: step:765/1000 train_loss:4.7580 train_time:191830ms step_avg:254.08ms torch.cuda.memory_allocated()=369581568
14:07:01.085: step:766/1000 train_loss:5.0514 train_time:192116ms step_avg:254.12ms torch.cuda.memory_allocated()=369581568
14:07:01.353: step:767/1000 train_loss:4.9310 train_time:192384ms step_avg:254.14ms torch.cuda.memory_allocated()=369581568
14:07:01.621: step:768/1000 train_loss:5.2283 train_time:192652ms step_avg:254.16ms torch.cuda.memory_allocated()=369581568
14:07:01.883: step:769/1000 train_loss:4.7310 train_time:192913ms step_avg:254.17ms torch.cuda.memory_allocated()=369581568
14:07:02.145: step:770/1000 train_loss:5.0038 train_time:193175ms step_avg:254.18ms torch.cuda.memory_allocated()=369581568
14:07:02.414: step:771/1000 train_loss:4.8796 train_time:193444ms step_avg:254.20ms torch.cuda.memory_allocated()=369581568
14:07:02.729: step:772/1000 train_loss:4.8160 train_time:193760ms step_avg:254.28ms torch.cuda.memory_allocated()=369581568
14:07:03.008: step:773/1000 train_loss:5.0494 train_time:194039ms step_avg:254.31ms torch.cuda.memory_allocated()=369581568
14:07:03.282: step:774/1000 train_loss:4.9736 train_time:194313ms step_avg:254.34ms torch.cuda.memory_allocated()=369581568
14:07:03.544: step:775/1000 train_loss:4.7838 train_time:194575ms step_avg:254.35ms torch.cuda.memory_allocated()=369581568
14:07:03.807: step:776/1000 train_loss:4.8460 train_time:194837ms step_avg:254.36ms torch.cuda.memory_allocated()=369581568
14:07:04.071: step:777/1000 train_loss:4.7819 train_time:195102ms step_avg:254.37ms torch.cuda.memory_allocated()=369581568
14:07:04.332: step:778/1000 train_loss:4.7974 train_time:195363ms step_avg:254.38ms torch.cuda.memory_allocated()=369581568
14:07:04.608: step:779/1000 train_loss:4.3818 train_time:195638ms step_avg:254.41ms torch.cuda.memory_allocated()=369581568
14:07:04.872: step:780/1000 train_loss:4.8300 train_time:195902ms step_avg:254.42ms torch.cuda.memory_allocated()=369581568
14:07:05.131: step:781/1000 train_loss:4.9178 train_time:196161ms step_avg:254.42ms torch.cuda.memory_allocated()=369581568
14:07:05.389: step:782/1000 train_loss:4.7531 train_time:196420ms step_avg:254.43ms torch.cuda.memory_allocated()=369581568
14:07:05.660: step:783/1000 train_loss:5.2275 train_time:196690ms step_avg:254.45ms torch.cuda.memory_allocated()=369581568
14:07:05.935: step:784/1000 train_loss:4.8347 train_time:196965ms step_avg:254.48ms torch.cuda.memory_allocated()=369581568
14:07:06.211: step:785/1000 train_loss:4.9921 train_time:197242ms step_avg:254.51ms torch.cuda.memory_allocated()=369581568
14:07:06.476: step:786/1000 train_loss:4.7049 train_time:197506ms step_avg:254.52ms torch.cuda.memory_allocated()=369581568
14:07:06.753: step:787/1000 train_loss:4.6737 train_time:197783ms step_avg:254.55ms torch.cuda.memory_allocated()=369581568
14:07:07.025: step:788/1000 train_loss:4.8487 train_time:198055ms step_avg:254.57ms torch.cuda.memory_allocated()=369581568
14:07:07.290: step:789/1000 train_loss:4.7583 train_time:198320ms step_avg:254.58ms torch.cuda.memory_allocated()=369581568
14:07:07.557: step:790/1000 train_loss:4.6125 train_time:198587ms step_avg:254.60ms torch.cuda.memory_allocated()=369581568
14:07:07.824: step:791/1000 train_loss:5.0899 train_time:198854ms step_avg:254.61ms torch.cuda.memory_allocated()=369581568
14:07:08.085: step:792/1000 train_loss:4.8399 train_time:199115ms step_avg:254.62ms torch.cuda.memory_allocated()=369581568
14:07:08.347: step:793/1000 train_loss:4.9038 train_time:199377ms step_avg:254.63ms torch.cuda.memory_allocated()=369581568
14:07:08.607: step:794/1000 train_loss:4.9309 train_time:199638ms step_avg:254.64ms torch.cuda.memory_allocated()=369581568
14:07:08.868: step:795/1000 train_loss:4.9773 train_time:199898ms step_avg:254.65ms torch.cuda.memory_allocated()=369581568
14:07:09.134: step:796/1000 train_loss:4.8067 train_time:200164ms step_avg:254.66ms torch.cuda.memory_allocated()=369581568
14:07:09.404: step:797/1000 train_loss:5.2755 train_time:200434ms step_avg:254.68ms torch.cuda.memory_allocated()=369581568
14:07:09.667: step:798/1000 train_loss:4.8049 train_time:200698ms step_avg:254.69ms torch.cuda.memory_allocated()=369581568
14:07:09.932: step:799/1000 train_loss:5.0243 train_time:200963ms step_avg:254.71ms torch.cuda.memory_allocated()=369581568
14:07:10.193: step:800/1000 train_loss:5.0335 train_time:201224ms step_avg:254.71ms torch.cuda.memory_allocated()=369581568
14:07:10.455: step:801/1000 train_loss:5.1014 train_time:201485ms step_avg:254.72ms torch.cuda.memory_allocated()=369581568
14:07:10.727: step:802/1000 train_loss:4.5445 train_time:201758ms step_avg:254.74ms torch.cuda.memory_allocated()=369581568
14:07:10.998: step:803/1000 train_loss:4.6944 train_time:202029ms step_avg:254.76ms torch.cuda.memory_allocated()=369581568
14:07:11.284: step:804/1000 train_loss:4.9387 train_time:202315ms step_avg:254.80ms torch.cuda.memory_allocated()=369581568
14:07:11.563: step:805/1000 train_loss:5.3969 train_time:202593ms step_avg:254.83ms torch.cuda.memory_allocated()=369581568
14:07:11.828: step:806/1000 train_loss:5.0761 train_time:202859ms step_avg:254.85ms torch.cuda.memory_allocated()=369581568
14:07:12.093: step:807/1000 train_loss:5.0389 train_time:203124ms step_avg:254.86ms torch.cuda.memory_allocated()=369581568
14:07:12.356: step:808/1000 train_loss:4.8681 train_time:203387ms step_avg:254.87ms torch.cuda.memory_allocated()=369581568
14:07:12.617: step:809/1000 train_loss:4.6802 train_time:203648ms step_avg:254.88ms torch.cuda.memory_allocated()=369581568
14:07:12.889: step:810/1000 train_loss:4.5195 train_time:203920ms step_avg:254.90ms torch.cuda.memory_allocated()=369581568
14:07:13.150: step:811/1000 train_loss:5.0685 train_time:204180ms step_avg:254.91ms torch.cuda.memory_allocated()=369581568
14:07:13.414: step:812/1000 train_loss:5.0246 train_time:204445ms step_avg:254.92ms torch.cuda.memory_allocated()=369581568
14:07:13.671: step:813/1000 train_loss:4.8072 train_time:204702ms step_avg:254.92ms torch.cuda.memory_allocated()=369581568
14:07:13.928: step:814/1000 train_loss:5.0238 train_time:204959ms step_avg:254.92ms torch.cuda.memory_allocated()=369581568
14:07:14.191: step:815/1000 train_loss:4.9512 train_time:205221ms step_avg:254.93ms torch.cuda.memory_allocated()=369581568
14:07:14.450: step:816/1000 train_loss:4.7359 train_time:205481ms step_avg:254.94ms torch.cuda.memory_allocated()=369581568
14:07:14.720: step:817/1000 train_loss:4.8108 train_time:205751ms step_avg:254.96ms torch.cuda.memory_allocated()=369581568
14:07:14.985: step:818/1000 train_loss:4.7564 train_time:206015ms step_avg:254.97ms torch.cuda.memory_allocated()=369581568
14:07:15.243: step:819/1000 train_loss:4.9158 train_time:206273ms step_avg:254.97ms torch.cuda.memory_allocated()=369581568
14:07:15.515: step:820/1000 train_loss:5.0088 train_time:206545ms step_avg:254.99ms torch.cuda.memory_allocated()=369581568
14:07:15.773: step:821/1000 train_loss:4.7787 train_time:206803ms step_avg:255.00ms torch.cuda.memory_allocated()=369581568
14:07:16.038: step:822/1000 train_loss:4.7529 train_time:207068ms step_avg:255.01ms torch.cuda.memory_allocated()=369581568
14:07:16.307: step:823/1000 train_loss:4.7239 train_time:207337ms step_avg:255.03ms torch.cuda.memory_allocated()=369581568
14:07:16.581: step:824/1000 train_loss:4.7094 train_time:207611ms step_avg:255.05ms torch.cuda.memory_allocated()=369581568
14:07:16.849: step:825/1000 train_loss:4.8363 train_time:207880ms step_avg:255.07ms torch.cuda.memory_allocated()=369581568
14:07:17.113: step:826/1000 train_loss:4.8402 train_time:208144ms step_avg:255.08ms torch.cuda.memory_allocated()=369581568
14:07:17.378: step:827/1000 train_loss:4.6920 train_time:208408ms step_avg:255.09ms torch.cuda.memory_allocated()=369581568
14:07:17.650: step:828/1000 train_loss:4.6947 train_time:208680ms step_avg:255.11ms torch.cuda.memory_allocated()=369581568
14:07:17.936: step:829/1000 train_loss:4.6891 train_time:208966ms step_avg:255.15ms torch.cuda.memory_allocated()=369581568
14:07:18.204: step:830/1000 train_loss:4.8341 train_time:209235ms step_avg:255.16ms torch.cuda.memory_allocated()=369581568
14:07:18.471: step:831/1000 train_loss:5.0733 train_time:209501ms step_avg:255.18ms torch.cuda.memory_allocated()=369581568
14:07:18.754: step:832/1000 train_loss:5.6822 train_time:209784ms step_avg:255.21ms torch.cuda.memory_allocated()=369581568
14:07:19.038: step:833/1000 train_loss:5.2385 train_time:210069ms step_avg:255.25ms torch.cuda.memory_allocated()=369581568
14:07:19.304: step:834/1000 train_loss:4.7464 train_time:210334ms step_avg:255.26ms torch.cuda.memory_allocated()=369581568
14:07:19.580: step:835/1000 train_loss:4.7589 train_time:210610ms step_avg:255.28ms torch.cuda.memory_allocated()=369581568
14:07:19.871: step:836/1000 train_loss:4.9358 train_time:210901ms step_avg:255.33ms torch.cuda.memory_allocated()=369581568
14:07:20.143: step:837/1000 train_loss:4.8828 train_time:211174ms step_avg:255.35ms torch.cuda.memory_allocated()=369581568
14:07:20.423: step:838/1000 train_loss:4.6496 train_time:211453ms step_avg:255.38ms torch.cuda.memory_allocated()=369581568
14:07:20.689: step:839/1000 train_loss:4.7997 train_time:211719ms step_avg:255.39ms torch.cuda.memory_allocated()=369581568
14:07:20.994: step:840/1000 train_loss:4.6267 train_time:212024ms step_avg:255.45ms torch.cuda.memory_allocated()=369581568
14:07:21.283: step:841/1000 train_loss:4.8108 train_time:212313ms step_avg:255.49ms torch.cuda.memory_allocated()=369581568
14:07:21.553: step:842/1000 train_loss:4.9173 train_time:212584ms step_avg:255.51ms torch.cuda.memory_allocated()=369581568
14:07:21.823: step:843/1000 train_loss:4.5809 train_time:212853ms step_avg:255.53ms torch.cuda.memory_allocated()=369581568
14:07:22.087: step:844/1000 train_loss:4.8320 train_time:213117ms step_avg:255.54ms torch.cuda.memory_allocated()=369581568
14:07:22.351: step:845/1000 train_loss:4.9754 train_time:213381ms step_avg:255.55ms torch.cuda.memory_allocated()=369581568
14:07:22.611: step:846/1000 train_loss:5.0062 train_time:213641ms step_avg:255.55ms torch.cuda.memory_allocated()=369581568
14:07:22.877: step:847/1000 train_loss:4.9134 train_time:213907ms step_avg:255.56ms torch.cuda.memory_allocated()=369581568
14:07:23.141: step:848/1000 train_loss:4.8368 train_time:214171ms step_avg:255.57ms torch.cuda.memory_allocated()=369581568
14:07:23.399: step:849/1000 train_loss:4.8313 train_time:214429ms step_avg:255.58ms torch.cuda.memory_allocated()=369581568
14:07:23.663: step:850/1000 train_loss:4.9779 train_time:214693ms step_avg:255.59ms torch.cuda.memory_allocated()=369581568
14:07:23.939: step:851/1000 train_loss:4.7905 train_time:214969ms step_avg:255.61ms torch.cuda.memory_allocated()=369581568
14:07:24.202: step:852/1000 train_loss:4.7793 train_time:215232ms step_avg:255.62ms torch.cuda.memory_allocated()=369581568
14:07:24.471: step:853/1000 train_loss:4.5863 train_time:215501ms step_avg:255.64ms torch.cuda.memory_allocated()=369581568
14:07:24.738: step:854/1000 train_loss:5.1238 train_time:215768ms step_avg:255.65ms torch.cuda.memory_allocated()=369581568
14:07:24.995: step:855/1000 train_loss:4.8141 train_time:216026ms step_avg:255.65ms torch.cuda.memory_allocated()=369581568
14:07:25.257: step:856/1000 train_loss:4.8171 train_time:216287ms step_avg:255.66ms torch.cuda.memory_allocated()=369581568
14:07:25.516: step:857/1000 train_loss:4.9440 train_time:216547ms step_avg:255.66ms torch.cuda.memory_allocated()=369581568
14:07:25.773: step:858/1000 train_loss:4.7435 train_time:216803ms step_avg:255.66ms torch.cuda.memory_allocated()=369581568
14:07:26.035: step:859/1000 train_loss:4.7450 train_time:217066ms step_avg:255.67ms torch.cuda.memory_allocated()=369581568
14:07:26.298: step:860/1000 train_loss:5.2109 train_time:217328ms step_avg:255.68ms torch.cuda.memory_allocated()=369581568
14:07:26.556: step:861/1000 train_loss:4.9424 train_time:217587ms step_avg:255.68ms torch.cuda.memory_allocated()=369581568
14:07:26.816: step:862/1000 train_loss:4.5762 train_time:217847ms step_avg:255.69ms torch.cuda.memory_allocated()=369581568
14:07:27.084: step:863/1000 train_loss:4.7493 train_time:218115ms step_avg:255.70ms torch.cuda.memory_allocated()=369581568
14:07:27.348: step:864/1000 train_loss:4.7347 train_time:218379ms step_avg:255.71ms torch.cuda.memory_allocated()=369581568
14:07:27.614: step:865/1000 train_loss:4.6860 train_time:218645ms step_avg:255.72ms torch.cuda.memory_allocated()=369581568
14:07:27.891: step:866/1000 train_loss:4.3844 train_time:218921ms step_avg:255.75ms torch.cuda.memory_allocated()=369581568
14:07:28.148: step:867/1000 train_loss:4.8342 train_time:219178ms step_avg:255.75ms torch.cuda.memory_allocated()=369581568
14:07:28.408: step:868/1000 train_loss:4.5687 train_time:219438ms step_avg:255.76ms torch.cuda.memory_allocated()=369581568
14:07:28.671: step:869/1000 train_loss:4.7824 train_time:219701ms step_avg:255.76ms torch.cuda.memory_allocated()=369581568
14:07:28.937: step:870/1000 train_loss:4.8141 train_time:219968ms step_avg:255.78ms torch.cuda.memory_allocated()=369581568
14:07:29.201: step:871/1000 train_loss:4.8162 train_time:220231ms step_avg:255.79ms torch.cuda.memory_allocated()=369581568
14:07:29.472: step:872/1000 train_loss:4.6991 train_time:220502ms step_avg:255.80ms torch.cuda.memory_allocated()=369581568
14:07:29.748: step:873/1000 train_loss:4.6653 train_time:220779ms step_avg:255.83ms torch.cuda.memory_allocated()=369581568
14:07:30.020: step:874/1000 train_loss:4.6525 train_time:221050ms step_avg:255.85ms torch.cuda.memory_allocated()=369581568
14:07:30.289: step:875/1000 train_loss:4.4748 train_time:221319ms step_avg:255.86ms torch.cuda.memory_allocated()=369581568
14:07:33.080: step:875/1000 val_loss:4.8862 train_time:221319ms step_avg:255.86ms
14:07:33.340: step:876/1000 train_loss:4.7496 train_time:221579ms step_avg:255.86ms torch.cuda.memory_allocated()=369581568
14:07:33.601: step:877/1000 train_loss:4.7323 train_time:221840ms step_avg:255.87ms torch.cuda.memory_allocated()=369581568
14:07:33.887: step:878/1000 train_loss:4.3390 train_time:222126ms step_avg:255.91ms torch.cuda.memory_allocated()=369581568
14:07:34.154: step:879/1000 train_loss:4.6639 train_time:222393ms step_avg:255.92ms torch.cuda.memory_allocated()=369581568
14:07:34.420: step:880/1000 train_loss:4.5345 train_time:222659ms step_avg:255.93ms torch.cuda.memory_allocated()=369581568
14:07:34.682: step:881/1000 train_loss:4.7164 train_time:222921ms step_avg:255.94ms torch.cuda.memory_allocated()=369581568
14:07:34.953: step:882/1000 train_loss:4.9493 train_time:223192ms step_avg:255.95ms torch.cuda.memory_allocated()=369581568
14:07:35.213: step:883/1000 train_loss:4.7226 train_time:223452ms step_avg:255.96ms torch.cuda.memory_allocated()=369581568
14:07:35.474: step:884/1000 train_loss:4.6722 train_time:223713ms step_avg:255.96ms torch.cuda.memory_allocated()=369581568
14:07:35.740: step:885/1000 train_loss:4.6421 train_time:223979ms step_avg:255.98ms torch.cuda.memory_allocated()=369581568
14:07:35.999: step:886/1000 train_loss:4.6725 train_time:224238ms step_avg:255.98ms torch.cuda.memory_allocated()=369581568
14:07:36.264: step:887/1000 train_loss:4.9963 train_time:224503ms step_avg:255.99ms torch.cuda.memory_allocated()=369581568
14:07:36.529: step:888/1000 train_loss:4.8645 train_time:224768ms step_avg:256.00ms torch.cuda.memory_allocated()=369581568
14:07:36.795: step:889/1000 train_loss:4.7225 train_time:225034ms step_avg:256.01ms torch.cuda.memory_allocated()=369581568
14:07:37.065: step:890/1000 train_loss:4.7629 train_time:225304ms step_avg:256.03ms torch.cuda.memory_allocated()=369581568
14:07:37.335: step:891/1000 train_loss:4.6658 train_time:225574ms step_avg:256.04ms torch.cuda.memory_allocated()=369581568
14:07:37.598: step:892/1000 train_loss:4.7624 train_time:225837ms step_avg:256.05ms torch.cuda.memory_allocated()=369581568
14:07:37.884: step:893/1000 train_loss:4.8881 train_time:226123ms step_avg:256.08ms torch.cuda.memory_allocated()=369581568
14:07:38.142: step:894/1000 train_loss:4.8368 train_time:226381ms step_avg:256.09ms torch.cuda.memory_allocated()=369581568
14:07:38.406: step:895/1000 train_loss:4.7124 train_time:226645ms step_avg:256.10ms torch.cuda.memory_allocated()=369581568
14:07:38.673: step:896/1000 train_loss:4.5194 train_time:226911ms step_avg:256.11ms torch.cuda.memory_allocated()=369581568
14:07:38.942: step:897/1000 train_loss:4.8681 train_time:227181ms step_avg:256.12ms torch.cuda.memory_allocated()=369581568
14:07:39.200: step:898/1000 train_loss:4.7396 train_time:227439ms step_avg:256.13ms torch.cuda.memory_allocated()=369581568
14:07:39.456: step:899/1000 train_loss:4.7721 train_time:227695ms step_avg:256.12ms torch.cuda.memory_allocated()=369581568
14:07:39.728: step:900/1000 train_loss:4.8405 train_time:227967ms step_avg:256.14ms torch.cuda.memory_allocated()=369581568
14:07:39.004: step:901/1000 train_loss:4.9168 train_time:228243ms step_avg:256.16ms torch.cuda.memory_allocated()=369581568
14:07:40.273: step:902/1000 train_loss:4.4834 train_time:228512ms step_avg:256.18ms torch.cuda.memory_allocated()=369581568
14:07:40.538: step:903/1000 train_loss:4.5462 train_time:228777ms step_avg:256.19ms torch.cuda.memory_allocated()=369581568
14:07:40.795: step:904/1000 train_loss:4.8527 train_time:229034ms step_avg:256.19ms torch.cuda.memory_allocated()=369581568
14:07:41.056: step:905/1000 train_loss:4.7471 train_time:229295ms step_avg:256.19ms torch.cuda.memory_allocated()=369581568
14:07:41.317: step:906/1000 train_loss:4.6609 train_time:229556ms step_avg:256.20ms torch.cuda.memory_allocated()=369581568
14:07:41.582: step:907/1000 train_loss:4.8171 train_time:229821ms step_avg:256.21ms torch.cuda.memory_allocated()=369581568
14:07:41.853: step:908/1000 train_loss:4.8625 train_time:230092ms step_avg:256.23ms torch.cuda.memory_allocated()=369581568
14:07:42.121: step:909/1000 train_loss:4.5394 train_time:230360ms step_avg:256.24ms torch.cuda.memory_allocated()=369581568
14:07:42.386: step:910/1000 train_loss:4.9355 train_time:230625ms step_avg:256.25ms torch.cuda.memory_allocated()=369581568
14:07:42.651: step:911/1000 train_loss:5.1521 train_time:230890ms step_avg:256.26ms torch.cuda.memory_allocated()=369581568
14:07:42.941: step:912/1000 train_loss:4.9291 train_time:231179ms step_avg:256.30ms torch.cuda.memory_allocated()=369581568
14:07:43.225: step:913/1000 train_loss:4.8677 train_time:231464ms step_avg:256.33ms torch.cuda.memory_allocated()=369581568
14:07:43.488: step:914/1000 train_loss:4.7729 train_time:231726ms step_avg:256.33ms torch.cuda.memory_allocated()=369581568
14:07:43.748: step:915/1000 train_loss:4.5548 train_time:231987ms step_avg:256.34ms torch.cuda.memory_allocated()=369581568
14:07:44.020: step:916/1000 train_loss:5.4038 train_time:232259ms step_avg:256.36ms torch.cuda.memory_allocated()=369581568
14:07:44.289: step:917/1000 train_loss:5.1529 train_time:232527ms step_avg:256.37ms torch.cuda.memory_allocated()=369581568
14:07:44.554: step:918/1000 train_loss:4.9737 train_time:232793ms step_avg:256.38ms torch.cuda.memory_allocated()=369581568
14:07:44.822: step:919/1000 train_loss:5.0492 train_time:233061ms step_avg:256.39ms torch.cuda.memory_allocated()=369581568
14:07:45.096: step:920/1000 train_loss:4.5160 train_time:233335ms step_avg:256.41ms torch.cuda.memory_allocated()=369581568
14:07:45.365: step:921/1000 train_loss:4.8045 train_time:233604ms step_avg:256.43ms torch.cuda.memory_allocated()=369581568
14:07:45.630: step:922/1000 train_loss:4.7248 train_time:233869ms step_avg:256.44ms torch.cuda.memory_allocated()=369581568
14:07:45.899: step:923/1000 train_loss:4.8615 train_time:234138ms step_avg:256.45ms torch.cuda.memory_allocated()=369581568
14:07:46.158: step:924/1000 train_loss:4.7778 train_time:234397ms step_avg:256.45ms torch.cuda.memory_allocated()=369581568
14:07:46.433: step:925/1000 train_loss:4.4297 train_time:234672ms step_avg:256.47ms torch.cuda.memory_allocated()=369581568
14:07:46.700: step:926/1000 train_loss:4.8928 train_time:234939ms step_avg:256.48ms torch.cuda.memory_allocated()=369581568
14:07:46.969: step:927/1000 train_loss:4.7669 train_time:235208ms step_avg:256.50ms torch.cuda.memory_allocated()=369581568
14:07:47.237: step:928/1000 train_loss:4.5066 train_time:235476ms step_avg:256.51ms torch.cuda.memory_allocated()=369581568
14:07:47.499: step:929/1000 train_loss:4.4674 train_time:235738ms step_avg:256.52ms torch.cuda.memory_allocated()=369581568
14:07:47.764: step:930/1000 train_loss:5.0218 train_time:236003ms step_avg:256.53ms torch.cuda.memory_allocated()=369581568
14:07:48.028: step:931/1000 train_loss:4.7395 train_time:236267ms step_avg:256.53ms torch.cuda.memory_allocated()=369581568
14:07:48.303: step:932/1000 train_loss:4.3986 train_time:236542ms step_avg:256.55ms torch.cuda.memory_allocated()=369581568
14:07:48.571: step:933/1000 train_loss:4.5122 train_time:236810ms step_avg:256.57ms torch.cuda.memory_allocated()=369581568
14:07:48.844: step:934/1000 train_loss:4.5587 train_time:237083ms step_avg:256.58ms torch.cuda.memory_allocated()=369581568
14:07:49.111: step:935/1000 train_loss:4.6666 train_time:237349ms step_avg:256.59ms torch.cuda.memory_allocated()=369581568
14:07:49.372: step:936/1000 train_loss:4.7114 train_time:237611ms step_avg:256.60ms torch.cuda.memory_allocated()=369581568
14:07:49.632: step:937/1000 train_loss:4.6731 train_time:237871ms step_avg:256.60ms torch.cuda.memory_allocated()=369581568
14:07:49.904: step:938/1000 train_loss:4.5553 train_time:238143ms step_avg:256.62ms torch.cuda.memory_allocated()=369581568
14:07:50.172: step:939/1000 train_loss:4.9252 train_time:238411ms step_avg:256.63ms torch.cuda.memory_allocated()=369581568
14:07:50.452: step:940/1000 train_loss:4.6727 train_time:238691ms step_avg:256.66ms torch.cuda.memory_allocated()=369581568
14:07:50.718: step:941/1000 train_loss:4.8442 train_time:238957ms step_avg:256.67ms torch.cuda.memory_allocated()=369581568
14:07:50.993: step:942/1000 train_loss:4.6986 train_time:239232ms step_avg:256.69ms torch.cuda.memory_allocated()=369581568
14:07:51.259: step:943/1000 train_loss:4.5392 train_time:239497ms step_avg:256.70ms torch.cuda.memory_allocated()=369581568
14:07:51.531: step:944/1000 train_loss:4.4296 train_time:239770ms step_avg:256.71ms torch.cuda.memory_allocated()=369581568
14:07:51.797: step:945/1000 train_loss:4.9419 train_time:240036ms step_avg:256.72ms torch.cuda.memory_allocated()=369581568
14:07:52.062: step:946/1000 train_loss:4.7888 train_time:240301ms step_avg:256.73ms torch.cuda.memory_allocated()=369581568
14:07:52.321: step:947/1000 train_loss:4.8981 train_time:240560ms step_avg:256.73ms torch.cuda.memory_allocated()=369581568
14:07:52.580: step:948/1000 train_loss:4.7911 train_time:240819ms step_avg:256.74ms torch.cuda.memory_allocated()=369581568
14:07:52.859: step:949/1000 train_loss:4.7118 train_time:241098ms step_avg:256.76ms torch.cuda.memory_allocated()=369581568
14:07:53.154: step:950/1000 train_loss:4.6367 train_time:241393ms step_avg:256.80ms torch.cuda.memory_allocated()=369581568
14:07:53.423: step:951/1000 train_loss:4.6123 train_time:241662ms step_avg:256.81ms torch.cuda.memory_allocated()=369581568
14:07:53.690: step:952/1000 train_loss:4.7363 train_time:241929ms step_avg:256.82ms torch.cuda.memory_allocated()=369581568
14:07:53.963: step:953/1000 train_loss:4.8970 train_time:242202ms step_avg:256.84ms torch.cuda.memory_allocated()=369581568
14:07:54.229: step:954/1000 train_loss:4.6857 train_time:242468ms step_avg:256.85ms torch.cuda.memory_allocated()=369581568
14:07:54.517: step:955/1000 train_loss:4.5401 train_time:242756ms step_avg:256.88ms torch.cuda.memory_allocated()=369581568
14:07:54.781: step:956/1000 train_loss:4.4341 train_time:243020ms step_avg:256.89ms torch.cuda.memory_allocated()=369581568
14:07:55.048: step:957/1000 train_loss:4.6039 train_time:243287ms step_avg:256.90ms torch.cuda.memory_allocated()=369581568
14:07:55.341: step:958/1000 train_loss:4.6947 train_time:243580ms step_avg:256.94ms torch.cuda.memory_allocated()=369581568
14:07:55.607: step:959/1000 train_loss:4.7026 train_time:243846ms step_avg:256.95ms torch.cuda.memory_allocated()=369581568
14:07:55.873: step:960/1000 train_loss:4.7937 train_time:244112ms step_avg:256.96ms torch.cuda.memory_allocated()=369581568
14:07:56.141: step:961/1000 train_loss:4.5431 train_time:244380ms step_avg:256.97ms torch.cuda.memory_allocated()=369581568
14:07:56.418: step:962/1000 train_loss:4.4430 train_time:244657ms step_avg:256.99ms torch.cuda.memory_allocated()=369581568
14:07:56.684: step:963/1000 train_loss:4.7930 train_time:244923ms step_avg:257.00ms torch.cuda.memory_allocated()=369581568
14:07:56.948: step:964/1000 train_loss:4.7400 train_time:245187ms step_avg:257.01ms torch.cuda.memory_allocated()=369581568
14:07:57.220: step:965/1000 train_loss:4.7074 train_time:245458ms step_avg:257.02ms torch.cuda.memory_allocated()=369581568
14:07:57.483: step:966/1000 train_loss:4.7601 train_time:245722ms step_avg:257.03ms torch.cuda.memory_allocated()=369581568
14:07:57.747: step:967/1000 train_loss:4.7324 train_time:245986ms step_avg:257.04ms torch.cuda.memory_allocated()=369581568
14:07:58.009: step:968/1000 train_loss:4.6797 train_time:246248ms step_avg:257.04ms torch.cuda.memory_allocated()=369581568
14:07:58.277: step:969/1000 train_loss:4.5910 train_time:246516ms step_avg:257.06ms torch.cuda.memory_allocated()=369581568
14:07:58.539: step:970/1000 train_loss:4.6088 train_time:246778ms step_avg:257.06ms torch.cuda.memory_allocated()=369581568
14:07:58.802: step:971/1000 train_loss:4.7720 train_time:247041ms step_avg:257.07ms torch.cuda.memory_allocated()=369581568
14:07:59.068: step:972/1000 train_loss:4.6339 train_time:247307ms step_avg:257.08ms torch.cuda.memory_allocated()=369581568
14:07:59.335: step:973/1000 train_loss:4.6740 train_time:247573ms step_avg:257.09ms torch.cuda.memory_allocated()=369581568
14:07:59.595: step:974/1000 train_loss:4.6366 train_time:247834ms step_avg:257.09ms torch.cuda.memory_allocated()=369581568
14:07:59.853: step:975/1000 train_loss:4.7255 train_time:248092ms step_avg:257.09ms torch.cuda.memory_allocated()=369581568
14:08:00.110: step:976/1000 train_loss:4.7959 train_time:248349ms step_avg:257.09ms torch.cuda.memory_allocated()=369581568
14:08:00.387: step:977/1000 train_loss:4.7896 train_time:248626ms step_avg:257.11ms torch.cuda.memory_allocated()=369581568
14:08:00.655: step:978/1000 train_loss:4.9752 train_time:248894ms step_avg:257.12ms torch.cuda.memory_allocated()=369581568
14:08:00.950: step:979/1000 train_loss:5.2601 train_time:249189ms step_avg:257.16ms torch.cuda.memory_allocated()=369581568
14:08:01.250: step:980/1000 train_loss:5.3974 train_time:249489ms step_avg:257.20ms torch.cuda.memory_allocated()=369581568
14:08:01.527: step:981/1000 train_loss:4.9703 train_time:249766ms step_avg:257.23ms torch.cuda.memory_allocated()=369581568
14:08:01.790: step:982/1000 train_loss:4.9730 train_time:250029ms step_avg:257.23ms torch.cuda.memory_allocated()=369581568
14:08:02.062: step:983/1000 train_loss:4.8597 train_time:250301ms step_avg:257.25ms torch.cuda.memory_allocated()=369581568
14:08:02.326: step:984/1000 train_loss:4.7488 train_time:250565ms step_avg:257.25ms torch.cuda.memory_allocated()=369581568
14:08:02.597: step:985/1000 train_loss:4.6256 train_time:250836ms step_avg:257.27ms torch.cuda.memory_allocated()=369581568
14:08:02.856: step:986/1000 train_loss:4.6408 train_time:251094ms step_avg:257.27ms torch.cuda.memory_allocated()=369581568
14:08:03.119: step:987/1000 train_loss:4.4976 train_time:251358ms step_avg:257.27ms torch.cuda.memory_allocated()=369581568
14:08:03.385: step:988/1000 train_loss:4.9022 train_time:251624ms step_avg:257.28ms torch.cuda.memory_allocated()=369581568
14:08:03.659: step:989/1000 train_loss:4.5574 train_time:251898ms step_avg:257.30ms torch.cuda.memory_allocated()=369581568
14:08:03.933: step:990/1000 train_loss:4.7282 train_time:252172ms step_avg:257.32ms torch.cuda.memory_allocated()=369581568
14:08:04.210: step:991/1000 train_loss:4.5140 train_time:252449ms step_avg:257.34ms torch.cuda.memory_allocated()=369581568
14:08:04.475: step:992/1000 train_loss:4.5954 train_time:252714ms step_avg:257.35ms torch.cuda.memory_allocated()=369581568
14:08:04.735: step:993/1000 train_loss:4.6773 train_time:252974ms step_avg:257.35ms torch.cuda.memory_allocated()=369581568
14:08:04.995: step:994/1000 train_loss:4.5772 train_time:253234ms step_avg:257.35ms torch.cuda.memory_allocated()=369581568
14:08:05.274: step:995/1000 train_loss:4.6781 train_time:253513ms step_avg:257.37ms torch.cuda.memory_allocated()=369581568
14:08:05.562: step:996/1000 train_loss:4.1238 train_time:253801ms step_avg:257.40ms torch.cuda.memory_allocated()=369581568
14:08:05.835: step:997/1000 train_loss:4.4243 train_time:254074ms step_avg:257.42ms torch.cuda.memory_allocated()=369581568
14:08:06.104: step:998/1000 train_loss:4.4574 train_time:254343ms step_avg:257.43ms torch.cuda.memory_allocated()=369581568
14:08:06.364: step:999/1000 train_loss:4.5547 train_time:254603ms step_avg:257.43ms torch.cuda.memory_allocated()=369581568
14:08:06.620: step:1000/1000 train_loss:4.5450 train_time:254858ms step_avg:257.43ms torch.cuda.memory_allocated()=369581568
14:08:09.431: step:1000/1000 val_loss:4.7876 train_time:254859ms step_avg:257.43ms
14:08:59.403: Renamed {old_logfile} -> {logfile}
14:08:59.405: peak memory allocated: 6688 MiB reserved: 10484 MiB
