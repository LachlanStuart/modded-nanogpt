14:49:40.528: from collections import defaultdict
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import atexit

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.profiler import profile, record_function, ProfilerActivity
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
# torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)
import triton
import triton.language as tl

try:
    from lovely_tensors import monkey_patch
    monkey_patch()
except ImportError:
    pass


# -----------------------------------------------------------------------------
#region  Custom operators : FP8 matmul by @YouJiacheng
@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        # x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        x_f8 = x.mul(x_s).to(torch.float8_e5m2)
        # w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e5m2)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    # return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)
    return x @ w.t(), x.to(torch.float8_e5m2), w.to(torch.float8_e5m2)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)
#endregion
# -----------------------------------------------------------------------------
#region Muon optimizer
@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()
#endregion
# -----------------------------------------------------------------------------
#region PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve
#endregion
# -----------------------------------------------------------------------------
#region The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

def create_block_masks(input_seq: Tensor, sliding_window_num_blocks: Tensor):
    BLOCK_SIZE = 128
    docs = (input_seq == 50256).cumsum(0)

    def document_causal(b, h, q_idx, kv_idx):
        causal_mask = q_idx >= kv_idx
        document_mask = docs[q_idx] == docs[kv_idx]
        return causal_mask & document_mask

    def dense_to_ordered(dense_mask: Tensor):
        num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
        indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
        return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

    # manual block mask creation by @YouJiacheng
    assert len(input_seq) % BLOCK_SIZE == 0
    NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
    block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
    any_causal_bm = block_idx[:, None] >= block_idx
    all_causal_bm = block_idx[:, None] > block_idx
    docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
    docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
    any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
    all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
    any_bm = any_causal_bm & any_document_bm
    all_bm = all_causal_bm & all_document_bm
    partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
    full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
    def build_bm(sw_num_blocks: Tensor) -> BlockMask:
        return BlockMask.from_kv_blocks(
            torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
            partial_kv_indices,
            torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
            full_kv_indices,
            BLOCK_SIZE=BLOCK_SIZE,
            mask_mod=document_causal,
        )
    # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
    return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        # self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977


    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i, block in enumerate(self.blocks[:self.num_encoder_layers]):
            x = block(x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i, block in enumerate(self.blocks[self.num_encoder_layers:]):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = block(x, ve_dec[i], x0, block_masks[i])

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

#endregion
# -----------------------------------------------------------------------------
#region MoEUT Triton kernels
# From https://github.com/RobertCsordas/moeut/blob/master/moeut/cvmm.py

@dataclass
class CVMMSel:
    raw_sel: torch.Tensor
    sel: torch.Tensor
    sel_index: torch.Tensor
    out_index: torch.Tensor | None = None
    reduction_weight: torch.Tensor | None = None

    def clone(self) -> 'CVMMSel':
        return CVMMSel(self.raw_sel, self.sel, self.sel_index, self.out_index, self.reduction_weight)


def cvmm_prepare_sel(sel: torch.Tensor, n_experts: int) -> CVMMSel:
    fsel = sel.flatten()
    ssel, sel_index = fsel.sort()
    return CVMMSel(sel, ssel.view_as(sel), sel_index, None)


def dtype_to_type_id(dtype: torch.dtype):
    if dtype == torch.float32:
        return 0
    elif dtype == torch.float16:
        return 1
    elif dtype == torch.bfloat16:
        return 2

    raise ValueError("Unknown dtype")


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
    ],
    key=['M', 'N', 'K', 'dtype_id', 'allow_tf32']
)
@triton.jit
def cvmm_kernel(
    # Pointers to matrices
    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,
    # Matrix dimensions
    M, N, K,
    # The stride variables represent how much to increase the ptr by when moving by 1
    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`
    # by to get the element one row down (A has M rows).
    stride_am, stride_ak,
    stride_bo, stride_bk, stride_bn,
    stride_cm, stride_cn,
    stride_index, stride_sel, stride_out_index,
    out_index_is_none: tl.constexpr,
    dtype_id: tl.constexpr, allow_tf32: tl.constexpr,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr
):
    """Kernel for computing the matmul C = A x B.
    A has shape (M, K), B has shape (K, N) and C has shape (M, N)
    """
    # -----------------------------------------------------------
    # Map program ids `pid` to the block of C it should compute.
    # This is done in a grouped ordering to promote L2 data reuse.
    # See above `L2 Cache Optimizations` section for details.
    pid = tl.program_id(axis=0)

    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_n = (pid % num_pid_in_group) // group_size_m

    pid_m = first_pid_m + (pid % group_size_m)

    sel_first = tl.load(sel_ptr + pid_m * BLOCK_SIZE_M * stride_sel)
    sel_last = tl.load(sel_ptr + (min((pid_m + 1) * BLOCK_SIZE_M, M) - 1) * stride_sel)
    sel_all = tl.load(sel_ptr + stride_sel * ((pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M))

    for matrix_id in range(sel_first, sel_last + 1):
        # ----------------------------------------------------------
        # Create pointers for the first blocks of A and B.
        # We will advance this pointer as we move in the K direction
        # and accumulate
        # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
        # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
        # See above `Pointer Arithmetics` section for details
        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N

        remap_offs_am = tl.load(index_ptr + stride_index * offs_am)

        # Create offset pointers
        offs_k = tl.arange(0, BLOCK_SIZE_K)
        a_ptrs = a_ptr + (remap_offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
        b_ptrs = b_ptr + matrix_id * stride_bo + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

        # -----------------------------------------------------------
        # Iterate to compute a block of the C matrix.
        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
        # of fp32 values for higher accuracy.
        # `accumulator` will be converted back to fp16 after the loop.
        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
            # Load the next block of A and B, generate a mask by checking the K dimension.
            # If it is out of bounds, set it to 0.
            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
            # We accumulate along the K dimension.

            # Triton was unhappy with passing dtypes as vars.
            if dtype_id == 1:
                a = a.to(tl.float16)
                b = b.to(tl.float16)
            elif dtype_id == 2:
                a = a.to(tl.bfloat16)
                b = b.to(tl.bfloat16)

            accumulator += tl.dot(a, b, allow_tf32=allow_tf32)

            # Advance the ptrs to the next K block.
            a_ptrs += BLOCK_SIZE_K * stride_ak
            b_ptrs += BLOCK_SIZE_K * stride_bk


        if dtype_id == 1:
            c = accumulator.to(tl.float16)
        elif dtype_id == 2:
            c = accumulator.to(tl.bfloat16)
        else:
            c = accumulator

        # -----------------------------------------------------------
        # Write back the block of the output matrix C with masks.
        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)

        if out_index_is_none:
            remap_offs_cm = remap_offs_am
        else:
            remap_offs_cm = tl.load(out_index_ptr + stride_out_index * offs_am)

        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
        c_ptrs = c_ptr + stride_cm * remap_offs_cm[:, None] + stride_cn * offs_cn[None, :]
        c_mask = ((offs_cm[:, None] < M) & (sel_all[:, None] == matrix_id)) & (offs_cn[None, :] < N)
        tl.store(c_ptrs, c, mask=c_mask)


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 128}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 4}, num_stages=4, num_warps=4),

        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        # triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 128}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),

        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 16}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 16}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),
    ],
    key=['M', 'N', 'K', 'out_dtype_id', 'allow_tf32', 'dtype_id'], reset_to_zero = ['c_ptr']
)
@triton.jit
def cvmm_backward_kernel3(
    # Pointers to matrices
    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,
    # Matrix dimensions
    M, N, K,
    # The stride variables represent how much to increase the ptr by when moving by 1
    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`
    # by to get the element one row down (A has M rows).
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_co, stride_cm, stride_cn,
    stride_index, stride_sel, stride_out_index,
    out_index_is_none: tl.constexpr,
    out_dtype_id: tl.constexpr, allow_tf32: tl.constexpr, dtype_id: tl.constexpr,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr, K_BLOCKS: tl.constexpr
):
    """Kernel for computing the matmul C = A x B.
    A has shape (M, K), B has shape (K, N) and C has shape (M, N)
    """
    # -----------------------------------------------------------
    # Map program ids `pid` to the block of C it should compute.
    # This is done in a grouped ordering to promote L2 data reuse.
    # See above `L2 Cache Optimizations` section for details.
    pid = tl.program_id(axis=0)
    k_block_id = tl.program_id(axis=1)

    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # ----------------------------------------------------------
    # Create pointers for the first blocks of A and B.
    # We will advance this pointer as we move in the K direction
    # and accumulate
    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
    # See above `Pointer Arithmetics` section for details
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N

    # -----------------------------------------------------------
    # Iterate to compute a block of the C matrix.
    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
    # of fp32 values for higher accuracy.
    # `accumulator` will be converted back to fp16 after the loop.

    a_ptrs_this = a_ptr + offs_am[:, None] * stride_am
    b_ptrs_this = b_ptr + offs_bn[None, :] * stride_bn

    # Kactual = end_i - start_i
    # Nblocks = (Kactual + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K

    # WORK_PER_WORKER = (Nblocks + K_BLOCKS - 1) // K_BLOCKS
    # WORK_PER_WORKER = WORK_PER_WORKER if WORK_PER_WORKER > MIN_WORK_SIZE else MIN_WORK_SIZE


    # # Kloop_start = (Kactual + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K

    # first_block_k = k_block_id * WORK_PER_WORKER
    # last_block_k = min((k_block_id+1) * WORK_PER_WORKER, Nblocks)

    block_start_index = k_block_id * BLOCK_SIZE_K * K_BLOCKS
    block_end_index = min(block_start_index + BLOCK_SIZE_K * K_BLOCKS, K) - 1

    first_mat = tl.load(sel_ptr + stride_sel * block_start_index)
    last_mat = tl.load(sel_ptr + stride_sel * block_end_index)


    for matrix_index in range(first_mat, last_mat + 1):
        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

        start_i = block_start_index
        end_i = block_end_index + 1
        while start_i < end_i:
            middle = (start_i + end_i) // 2
            middle_matrix = tl.load(sel_ptr + middle * stride_sel)
            if middle_matrix < matrix_index:
                start_i = middle + 1
            else:
                end_i = middle


        # # Continue binary search: find the first matrix that is > matrix_index
        start_i2 = start_i
        end_i = block_end_index + 1
        while start_i2 < end_i:
            middle = (start_i2 + end_i) // 2
            middle_matrix = tl.load(sel_ptr + middle * stride_sel)
            if middle_matrix <= matrix_index:
                start_i2 = middle + 1
            else:
                end_i = middle

        end_i = start_i2

        count = end_i - start_i

        block_mem_indices_f_base = start_i  + tl.arange(0, BLOCK_SIZE_K)

        if count > 0:
            for k in range((count + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K):
                # block_mem_indices = (k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)) % K
                block_mem_indices_f = block_mem_indices_f_base + k * BLOCK_SIZE_K
                block_mem_indices = block_mem_indices_f % K
                a_index = tl.load(index_ptr + stride_index * block_mem_indices)
                if out_index_is_none:
                    b_index = a_index
                else:
                    b_index = tl.load(out_index_ptr + stride_out_index * block_mem_indices)
                sel_ok = block_mem_indices_f < end_i

                a_ptrs = a_ptrs_this + a_index[None, :] * stride_ak
                b_ptrs = b_ptrs_this + b_index[:, None] * stride_bk

                # Load the next block of A and B, generate a mask by checking the K dimension.
                # If it is out of bounds, set it to 0.
                a = tl.load(a_ptrs, mask=sel_ok[None, :], other=0.0)
                b = tl.load(b_ptrs, mask=sel_ok[:, None], other=0.0)

                if dtype_id == 1:
                    a = a.to(tl.float16)
                    b = b.to(tl.float16)
                elif dtype_id == 2:
                    a = a.to(tl.bfloat16)
                    b = b.to(tl.bfloat16)

                # We accumulate along the K dimension.
                accumulator += tl.dot(a, b, allow_tf32=allow_tf32)

            if out_dtype_id == 1:
                c = accumulator.to(tl.float16)
            elif out_dtype_id == 2:
                c = accumulator.to(tl.bfloat16)
            else:
                c = accumulator

            # -----------------------------------------------------------
            # Write back the block of the output matrix C with masks.
            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
            c_ptrs = c_ptr + stride_co * matrix_index + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
            # tl.store(c_ptrs, c, mask=c_mask)
            tl.atomic_add(c_ptrs, c, mask=c_mask)


torch.library.define("mylib::cvmm_triton", "(Tensor x, Tensor sel_index, Tensor sel, Tensor keys, ScalarType out_dtype, Tensor out_index) -> Tensor")
@torch.library.impl("mylib::cvmm_triton", "default")
def cvmm_triton(
    x: torch.Tensor,
    sel_index: torch.Tensor,
    sel: torch.Tensor,
    keys: torch.Tensor,
    out_dtype: torch.dtype,
    out_index: torch.Tensor
):
    x = x.flatten(end_dim=-2)
    assert x.shape[-1] == keys.shape[1]

    sel_shape = sel.shape
    sel = sel.flatten()

    M = sel.shape[0]
    O, K, N = keys.shape
    # Allocates output.
    out = torch.empty((M, N), device=x.device, dtype=out_dtype)
    # out = torch.zeros((M, N), device=x.device, dtype=out_dtype)
    # 1D launch kernel where each block gets its own program.

    # expected_m_per_matrix = int(math.ceil(M / O * 1.5))
    # expected_m_per_matrix = M

    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )

    out_index_is_none = False
    if out_index.numel() == 1 and out_index == -1:
        out_index_is_none = True

    cvmm_kernel[grid](
        x, keys, out, sel_index, sel, out_index,
        M, N, K,
        x.stride(0), x.stride(1),
        keys.stride(0), keys.stride(1), keys.stride(2),
        out.stride(0), out.stride(1),
        sel_index.stride(0), sel.stride(0), 0 if out_index_is_none else out_index.stride(0),
        out_index_is_none=out_index_is_none,
        dtype_id = dtype_to_type_id(out.dtype), allow_tf32=False, #torch.backends.cuda.matmul.allow_tf32
    )

    return out.view(*sel_shape, N)


@torch.library.register_fake("mylib::cvmm_triton", cvmm_triton)
def cvmm_triton_abstract(x, sel_idx, sel, keys, out_dtype, out_index):
    sel_shape = sel.shape
    sel = sel.flatten()
    M = sel.shape[0]
    O, K, N = keys.shape
    out = torch.empty((M, N), device=x.device, dtype=out_dtype)
    sel_shape = sel.shape
    return out.view(*sel_shape, N)


def cvmm_triton_backward(
    x: torch.Tensor,
    sel_index: torch.Tensor,
    sel: torch.Tensor,
    grads: torch.Tensor,
    n_experts: int,
    key_dtype: torch.dtype,
    op_dtype: torch.dtype,
    out_index: torch.Tensor
):
    x = x.flatten(end_dim=-2)
    x = x.transpose(0, 1)
    grads = grads.flatten(end_dim=-2)
    sel = sel.flatten()
    M, _ = x.shape
    K, N = grads.shape
    # FIX: out must be atomic_add'able, which excludes bfloat16. Cast to key_dtype after. Maybe this could be f16
    out = torch.zeros((n_experts, M, N), device=x.device, dtype=torch.float32)
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), triton.cdiv(K, META['BLOCK_SIZE_K'] * META['K_BLOCKS'])
    )
    out_index_is_none = False
    if out_index.numel() == 1 and out_index == -1:
        out_index_is_none = True

    cvmm_backward_kernel3[grid](
        x, grads, out, sel_index, sel, out_index,
        M, N, K,
        x.stride(0), x.stride(1),
        grads.stride(0), grads.stride(1),
        out.stride(0), out.stride(1), out.stride(2),
        sel_index.stride(0), sel.stride(0), 0 if out_index_is_none else out_index.stride(0),
        out_index_is_none=out_index_is_none,
        out_dtype_id=dtype_to_type_id(out.dtype),
        dtype_id=dtype_to_type_id(op_dtype),
        allow_tf32=False #torch.backends.cuda.matmul.allow_tf32
    )
    return out.to(dtype=key_dtype)


class CVMM(torch.autograd.Function):
    warned = False

    @staticmethod
    def forward(
        ctx,
        x: torch.Tensor,
        sel_index: torch.Tensor,
        sel: torch.Tensor,
        keys: torch.Tensor,
        out_index: torch.Tensor | None = None,
        reduction_weight: torch.Tensor | None = None
    ):
        ctx.save_for_backward(x, keys, sel, sel_index, out_index, reduction_weight)

        # out_type = get_dtype()
        out_type = x.dtype
        # out_type = torch.float32
        if out_index is None:
            out_index = torch.tensor(-1).cuda()
        res = torch.ops.mylib.cvmm_triton(x, sel_index, sel, keys, out_type, out_index)

        if reduction_weight is not None:
            res = res.view(*reduction_weight.shape, res.shape[-1])
            res = (reduction_weight.unsqueeze(-2).type_as(res) @ res).squeeze(-2)

        ctx.op_type = out_type
        ctx.keys_type = keys.dtype
        ctx.dtype = out_type
        return res.type_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        x, keys, sel, sel_index, out_index, reduction_weight = ctx.saved_tensors
        keys_dt = keys

        # Backward for weight
        if reduction_weight is not None:
            # Project back the grads with he reduction weight, so the grad for the weight matrix is ok
            grad_output_w = reduction_weight.unsqueeze(-1).type_as(grad_output) @ grad_output.unsqueeze(-2)
        else:
            grad_output_w  = grad_output

        out_index_is_none = False
        if out_index is None:
            out_index_is_none = True
            out_index = torch.tensor(-1).cuda()

        grad_w = cvmm_triton_backward(
            x,
            sel_index,
            sel,
            grad_output_w,
            keys_dt.shape[0],
            ctx.keys_type,
            ctx.dtype,
            out_index=out_index
        )

        # Backward for input and reduction weight
        grad_w_off = None

        bw_index = sel_index if out_index_is_none else out_index
        bw_index_out = torch.tensor(-1).cuda()
        if reduction_weight is not None:
            # Hack the output indices to emulate repeats
            bw_index_out = bw_index
            bw_index = bw_index // reduction_weight.shape[-1]

        grad_x_full = torch.ops.mylib.cvmm_triton(
            grad_output,
            bw_index,
            sel,
            keys_dt.transpose(1,2),
            ctx.op_type,
            bw_index_out
        )

        grad_x_full = grad_x_full.view(*x.shape[:-1], -1, x.shape[-1])
        if reduction_weight is not None:
            # grad_x_full is the unscaled grad. For the input, we have to scale it, for the reduction wegiht,
            # we have to compute dot products with the input.
            grad_x = (reduction_weight.view(*grad_x_full.shape[:-1]).unsqueeze(-2).type_as(grad_x_full) @ grad_x_full).squeeze(-2)
            grad_w_off = (grad_x_full.type_as(reduction_weight) @ x.unsqueeze(-1).type_as(reduction_weight)).squeeze(-1).view_as(reduction_weight)
        elif grad_x_full.shape[-2] != 1:
            grad_x = grad_x_full.sum(-2)
        else:
            grad_x = grad_x_full

        grad_x = grad_x.view_as(x)

        return grad_x, None, None, grad_w, None, grad_w_off


def cvmm(x: torch.Tensor, sel: torch.Tensor | CVMMSel, keys: torch.Tensor):
    if not isinstance(sel, CVMMSel):
        sel = cvmm_prepare_sel(sel, keys.shape[0])
    assert x.dtype == keys.dtype, f"{x.dtype=} != {keys.dtype=}"

    return CVMM.apply(x, sel.sel_index, sel.sel, keys, sel.out_index, sel.reduction_weight)


def cvmm_prepare_sel2(sel: torch.Tensor, w: torch.Tensor | None = None) -> CVMMSel:
    # Has multiple selections for each batch element
    n_per_batch = sel.shape[-1]

    # indices = torch.arange(sel.nelement() // n_per_batch, device=sel.device, dtype=torch.int32)
    # indices = indices.repeat_interleave(n_per_batch).flatten()

    fsel = sel.flatten()
    ssel, sel_index = fsel.sort()

    # in_index = indices[sel_index]
    in_index = sel_index // n_per_batch

    return CVMMSel(sel, ssel.view_as(sel), in_index, sel_index, w)

#endregion
# -----------------------------------------------------------------------------
#region MoEUT
def log_mean(x: torch.Tensor, dim: int = 0):
    return x.logsumexp(dim) - torch.log(torch.tensor(x.shape[dim]))


def entropy_l(l: torch.Tensor) -> torch.Tensor:
    return - (l * l.exp()).sum(-1)


def entropy_reg(sel: torch.Tensor, dim: int) -> torch.Tensor:
    sel = F.log_softmax(sel, dim=-1)
    sel = log_mean(sel, dim)
    return - entropy_l(sel).mean()


class SigmaMoE(torch.nn.Module):
    def __init__(self, dmodel: int, n_experts: int, expert_size: int, k: int,
                 v_dim: int | None = None,
                 expert_dropout: float = 0.0):

        super().__init__()
        self.k_dim = dmodel
        self.v_dim = v_dim if v_dim is not None else dmodel
        self.n_experts = n_experts
        self.expert_size = expert_size
        self.size = self.n_experts * self.expert_size
        self.k_vec_dim = self.k_dim
        self.num_heads = k
        self.expert_dropout = expert_dropout

        self.keys = torch.nn.Parameter(torch.empty(self.n_experts, self.k_vec_dim, self.expert_size))
        self.values = torch.nn.Parameter(torch.empty(self.n_experts, self.expert_size, self.v_dim))
        self.expert_sel = torch.nn.Parameter(torch.empty(self.n_experts, self.k_vec_dim))

    @torch.no_grad
    def reset_parameters(self, std_scale: float):
        torch.nn.init.normal_(self.expert_sel, 0, std_scale / (self.k_dim) ** 0.5)
        torch.nn.init.normal_(self.keys, 0, std_scale / (self.k_dim) ** 0.5)
        torch.nn.init.normal_(self.values, 0, std_scale / (self.n_experts * self.expert_size) ** 0.5)

        # self.renorm_keep_std(self.expert_sel, dim=1)
        std = self.expert_sel.std()
        self.expert_sel.div_(self.expert_sel.norm(dim=1, keepdim=True))
        self.expert_sel.mul_(std / self.expert_sel.std())


    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        # Selection score calculation
        xnorm = norm(x)
        sel = F.linear(xnorm, self.expert_sel, None)
        sel_r = sel

        # Selection activation and topk
        sel = F.sigmoid(sel)

        if self.training and self.expert_dropout > 0:
            mask = torch.rand_like(sel) < self.expert_dropout
            sel = sel.masked_fill(mask, float("-inf"))

        sel_val, sel_index = sel.topk(self.num_heads, dim=-1, sorted=False)

        # Preprocess the selection indices. They will be needed for both layers and save some time
        sel_indices = cvmm_prepare_sel2(sel_index.int())

        # "Up-projection" layer for each head
        scores = cvmm(xnorm, sel_indices, self.keys)
        scores = F.relu(scores).square()

        # Down projection layer for each head
        sel_indices = sel_indices.clone()
        sel_indices.reduction_weight = sel_val
        sel_indices.sel_index = sel_indices.out_index
        sel_indices.out_index = None

        out = cvmm(scores, sel_indices, self.values)

        res = out.view(*x.shape[:-1], self.v_dim)
        return x + res, sel_r


class SwitchHeadRoPE(torch.nn.Module):
    def __init__(self, state_size: int, num_heads: int, n_experts: int, max_seq_len: int,
                 head_dim: int | None = None, expert_dropout: float = 0.0, moe_k: int = 2
                 ):

        super().__init__()

        self.input_size = state_size
        self.output_size = state_size
        self.pe_size = self.input_size
        self.expert_dropout = expert_dropout
        self.moe_k = moe_k
        self.attention_to_visualize = []
        self.selections_to_visualize = {}
        self.n_experts = n_experts

        self.num_heads = num_heads
        self.head_dim = head_dim or (state_size // num_heads)
        self.rotary = Rotary(self.head_dim, max_seq_len)

        self.q = torch.nn.Linear(self.input_size, self.head_dim * self.num_heads, bias=False)
        self.k = torch.nn.Linear(self.input_size, self.head_dim * self.num_heads, bias=False)

        if self.n_experts > 1:
            self.v = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size, self.head_dim))
            self.o = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.head_dim, self.output_size))
            self.sel_v = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size))
        else:
            self.v = torch.nn.Parameter(torch.empty(self.num_heads * self.head_dim, self.input_size))
            self.o = torch.nn.Parameter(torch.empty(self.output_size, self.num_heads * self.head_dim))

        self.sel_o = torch.nn.Parameter(torch.empty(self.num_heads * self.n_experts, self.input_size))

    @torch.no_grad
    def reset_parameters(self, std_scale: float):
        def renorm_rows(x: torch.Tensor):
            std_t = x.std(dim=-1, keepdim=True)
            x.div_(x.norm(dim=-1, keepdim=True))
            x.mul_(std_t / x.std())

        if self.n_experts > 1:
            torch.nn.init.normal_(self.sel_v, 0, std_scale / (self.input_size) ** 0.5)
            renorm_rows(self.sel_v)

        torch.nn.init.normal_(self.sel_o, 0, std_scale / (self.input_size) ** 0.5)
        renorm_rows(self.sel_o)

        torch.nn.init.normal_(self.k.weight, 0, std_scale / (self.input_size) ** 0.5)
        torch.nn.init.normal_(self.q.weight, 0, std_scale / (self.input_size) ** 0.5)
        torch.nn.init.normal_(self.v, 0, std_scale / (self.input_size) ** 0.5)
        torch.nn.init.normal_(self.o, 0, std_scale / (self.num_heads * self.head_dim) ** 0.5)


    def project_to_torch_order(self, x: torch.Tensor):
        return x.view(*x.shape[:-1], self.num_heads, -1).transpose(-2, -3)


    def get_sel(self, t: torch.Tensor, w: torch.Tensor) -> tuple[CVMMSel, torch.Tensor]:
        sel = F.linear(t, w).float()
        sel = sel_raw = sel.view(*sel.shape[:-1], self.num_heads, -1)
        sel = sel.sigmoid()

        with torch.no_grad():
            if self.expert_dropout > 0 and self.training:
                mask = torch.rand_like(sel) < self.expert_dropout
                sel2 = sel.masked_fill(mask, float('-inf'))
            else:
                sel2 = sel
            _, sel_index = sel2.topk(self.moe_k, dim=-1, sorted=False)
        sel_val = torch.gather(sel, -1, sel_index)

        sel_index_shifted = (torch.arange(self.num_heads, device=sel_index.device, dtype=sel_index.dtype) * self.n_experts).unsqueeze(-1) + sel_index
        return cvmm_prepare_sel2(sel_index_shifted.flatten(-2,-1), sel_val), sel_raw


    def forward(self, x: torch.Tensor, ve: torch.Tensor | None, block_mask: BlockMask
                ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        # *src: [batch_size, out_len, c]
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"

        xnorm = norm(x)
        q = self.q(xnorm)
        k = self.k(xnorm)

        if self.n_experts > 1:
            v_sel, v_sel_r = self.get_sel(xnorm, self.sel_v)
            o_sel, o_sel_r = self.get_sel(xnorm, self.sel_o)

            v = cvmm(x, v_sel, self.v).transpose(-2, -3)
        else:
            o_gate = F.sigmoid(F.linear(xnorm, self.sel_o))
            # v = self.project_to_torch_order(F.linear(v_src, self.v))
            # return x.view(*x.shape[:-1], self.num_heads, -1).transpose(-2, -3)
            v = F.linear(x, self.v).view(B, T, self.num_heads, self.head_dim).transpose(1,2)

        q = q.view(B, T, self.num_heads, self.head_dim)
        k = k.view(B, T, self.num_heads, self.head_dim)

        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q).transpose(1,2), self.rotary(k).transpose(1,2)
        # if ve is not None:
        #     v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        # else: # skip mid-layers token value embeddings by @YouJiacheng
        #     v = self.lambdas[0] * v

        res = flex_attention(q, k, v, block_mask=block_mask, scale=0.12)
        res = res.transpose(1, 2)

        if self.n_experts > 1:
            # The output selection indices are calculated from the current state and are also used for projecting "q".
            # But that projection needs to create multiple copies for the different heads. Here we already have the
            # heads, but we have to create copies for the top-k elements. We can calculate that from the reduction
            # weight. We also want to compute not only the weighted average between the top-k elements, but also
            # of the different heads. So reshape the reduction weight accordingly.
            o_sel.sel_index = o_sel.out_index // o_sel.reduction_weight.shape[-1]
            o_sel.reduction_weight = o_sel.reduction_weight.flatten(-2)
            out = cvmm(res, o_sel, self.o)
        else:
            res = res * o_gate[..., None]
            out = F.linear(res.flatten(-2), self.o)

        return x + out, o_sel_r, v_sel_r


class MoEUTLayer(torch.nn.Module):
    def __init__(self, d_model: int, num_heads: int, ff_expert_size: int, ff_n_experts: int,
                 att_n_experts: int, max_seq_len: int, head_dim: int | None = None, att_k: int = 2,
                 ff_k: int = 8, ff_expert_dropout: float = 0.0, att_expert_dropout: float = 0.0):

        super().__init__()
        self.attention = SwitchHeadRoPE(
            d_model, num_heads, att_n_experts, max_seq_len=max_seq_len, head_dim=head_dim, moe_k=att_k,
            expert_dropout=att_expert_dropout)
        self.ffn = SigmaMoE(d_model, ff_n_experts, ff_expert_size, k=ff_k, expert_dropout=ff_expert_dropout)

    def forward(self, x: torch.Tensor, ve: torch.Tensor | None, block_mask: BlockMask) -> torch.Tensor:
        x, o_sel_r, v_sel_r = self.attention(x, ve, block_mask)
        x, ffn_sel_r = self.ffn(x)
        return x, o_sel_r, v_sel_r, ffn_sel_r


class MoEUT(torch.nn.Module):
    def __init__(self, d_model: int, n_layers: int, num_heads: int, ff_expert_size: int, ff_n_experts: int,
                 att_n_experts: int, max_seq_len: int, head_dim: int | None = None, att_k: int = 2,
                 ff_k: int = 8, ff_expert_dropout: float = 0.0, att_expert_dropout: float = 0.0,
                 entropy_reg: float = 0.01, att_entropy_reg: float = 0.001,
                 group_size: int = 2):
        super().__init__()

        self.entropy_reg = entropy_reg
        self.att_entropy_reg = att_entropy_reg

        self.n_repeats = n_layers // group_size
        self.layers = torch.nn.ModuleList([
            MoEUTLayer(d_model, num_heads, ff_expert_size, ff_n_experts, att_n_experts,
                       max_seq_len, head_dim, att_k, ff_k,
                       ff_expert_dropout, att_expert_dropout)
            for _ in range(group_size)
        ])

        self.reset_parameters()

    def forward(self, x: torch.Tensor, block_mask: BlockMask) -> tuple[torch.Tensor, torch.Tensor]:
        # Run the model
        o_sels = defaultdict(list)
        v_sels = defaultdict(list)
        ffn_sels = defaultdict(list)
        for r in range(self.n_repeats):
            for li, layer in enumerate(self.layers):
                x, o_sel_r, v_sel_r, ffn_sel_r = layer(x, ve=None, block_mask=block_mask)
                o_sels[li].append(o_sel_r)
                v_sels[li].append(v_sel_r)
                ffn_sels[li].append(ffn_sel_r)

        ffn_reg_loss = torch.stack([
            entropy_reg(torch.stack(sel_hist, dim=-2).flatten(-3, -2), -2)
            for sel_hist in ffn_sels.values()
        ]).sum()
        att_reg_loss = torch.stack([
            entropy_reg(torch.stack(sel_hist, dim=-3).flatten(-4, -3), -3)
            for sel_hist in [*o_sels.values(), *v_sels.values()]
        ]).sum()

        reg_loss = self.entropy_reg * ffn_reg_loss + self.att_entropy_reg * att_reg_loss

        return x, reg_loss

    @torch.no_grad
    def reset_parameters(self):
        scale = (2 / (self.n_repeats * len(self.layers))) ** 0.5
        for layer in self.modules():
            if isinstance(layer, (SwitchHeadRoPE, SigmaMoE)):
                layer.reset_parameters(scale)
            elif isinstance(layer, torch.nn.LayerNorm):
                torch.nn.init.ones_(layer.weight)
                torch.nn.init.zeros_(layer.bias)


class MoEUTWrapper(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.moeut = MoEUT(
            d_model=model_dim,
            n_layers=num_layers,
            num_heads=num_heads,
            ff_expert_size=128,
            ff_n_experts=64,  # FIXME: Arbitrary decision
            att_n_experts=10, # !! From MoEUT paper, but they also used really weird numbers like d_head=41 so I don't trust their judgement
            max_seq_len=max_seq_len,
            head_dim=None,
            att_k=2,
            ff_k=8,
            ff_expert_dropout=0.0,
            att_expert_dropout=0.0,
            entropy_reg=0.01,
            att_entropy_reg=0.001,
            group_size=2,
        )
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977


    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, _ = create_block_masks(input_seq, sliding_window_num_blocks)

        x = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        x, reg_loss = self.moeut(x, long_bm)
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        if self.training:
            loss = loss + reg_loss
        return loss


#endregion
# -----------------------------------------------------------------------------
#region Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets
#endregion
# -----------------------------------------------------------------------------
#region utils, hyperparams
def print0(s, console=True):
    if master_process:
        timestamp = time.strftime("%H:%M:%S.") + f"{time.time() % 1:.3f}"[2:]
        s = f"{timestamp}: {s}"
        if console:
            print(s)
        if logfile:
            with open(logfile, "a") as f:
                print(s, file=f)

def log_mem():
    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )

@dataclass(frozen=True, kw_only=True)
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations: int = 1770 # number of iterations to run
    cooldown_frac: float = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len: int = 48*1024 # FlexAttention sequence length
    val_seq_len: int = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint: bool = False
    dev: bool = False

TEST_HPARAMS = Hyperparameters(
    train_files = "data/fineweb1B/fineweb_train_*.bin",
    val_files = "data/fineweb1B/fineweb_val_*.bin",
    val_tokens = 1048576,
    num_iterations = 1000, #770,
    cooldown_frac = 0.4,
    val_loss_every = 125,
    seq_len = 16*1024,
    val_seq_len = 4*16*1024,
    save_checkpoint = False,
    dev=False,
)
DEV_HPARAMS = Hyperparameters(
    train_files = "data/fineweb1B/fineweb_train_*.bin",
    val_files = "data/fineweb1B/fineweb_val_*.bin",
    val_tokens = 1024,
    num_iterations = 20,
    cooldown_frac = 0.4,
    val_loss_every = 125,
    seq_len = 512,
    val_seq_len = 512,
    save_checkpoint = False,
    dev=True,
)

#endregion
# -----------------------------------------------------------------------------
#region main()
master_process = None
logfile = None
def main(args = TEST_HPARAMS):
# def main(args = DEV_HPARAMS):
    global master_process, logfile
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    atexit.register(dist.destroy_process_group)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    if master_process and not args.dev:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)


    # begin by printing this file (the Python code)
    print0(code, console=False)
    print0("="*100, console=False)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}", console=False)
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}", console=False)
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi(), console=False)
    print0("="*100, console=False)
    atexit.register(log_mem)

    torch.random.manual_seed(0)
    torch.cuda.synchronize()
    print0("Init data")
    # load data
    train_batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

    torch.cuda.synchronize()
    print0("Init model")
    # REF: model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    model: nn.Module = MoEUTWrapper(vocab_size=50257, num_layers=12, num_heads=3, model_dim=384, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    model.bfloat16()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()

    # count parameters
    n_params_by_dtype = defaultdict(lambda: 0)
    for name, param in model.named_parameters():
        dist.broadcast(param.detach(), 0)
        n_params_by_dtype[param.dtype] += param.numel()
    for dt, n_params in n_params_by_dtype.items():
        print0(f"{dt}: {n_params/1024/1024:.3f}Mi params")
    print0(f"total: {sum(n_params_by_dtype.values())/1024/1024:.3f}Mi params")


    torch.cuda.synchronize()
    print0("Init optimizers")
    # collect the parameters to optimize
    hidden_matrix_params = [p for n, p in model.named_parameters() if p.ndim >= 2 and "embed" not in n and "lm_head" not in n]
    embed_params = [p for n, p in model.named_parameters() if "embed" in n]
    scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    lr_mod = (args.seq_len/Hyperparameters().seq_len) ** 0.5  # Correct LR based on difference in batch size vs 4
    print(f"{lr_mod=}")
    adam_params = [dict(params=head_params, lr=0.008*lr_mod), dict(params=embed_params, lr=0.6*lr_mod), dict(params=scalar_params, lr=0.04*lr_mod)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05*lr_mod, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(step: int):
        t = 1 - step / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    if not args.dev:
        model: nn.Module = torch.compile(model) #, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    print0("Starting train loop")
    train_steps = args.num_iterations
    prof = None
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # if step == 5:
        #     prof = profile(record_shapes=True, profile_memory=True, with_stack=True)
        #     prof.__enter__()
        #     prof.start()
        # if prof is not None:
        #     if step == 9:
        #         prof.__exit__(None, None, None)
        #         prof.export_chrome_trace("trace.json")
        #         prof = None
        #     else:
        #         prof.step()

        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_batch_size = world_size * args.val_seq_len
            assert args.val_tokens % val_batch_size == 0
            val_steps = args.val_tokens // val_batch_size
            val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for i in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION -----------------
        inputs, targets = next(train_loader)
        train_losses = []
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            loss = model(input_seq, target_seq, sw_num_blks(window_size))
            loss.backward()
            dist.all_reduce(loss, op=dist.ReduceOp.AVG)
            train_losses.append(loss.item())
            del loss
        train_loss = sum(train_losses or [torch.nan]) / max(len(train_losses), 1)
        for param in model.parameters():
            if param.grad is not None:
                dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        del param
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)

        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_loss:{train_loss:.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms {torch.cuda.memory_allocated()=}", console=True)

    if master_process and logfile is not None:
        new_logfile = input("Name run? ")
        if new_logfile:
            old_logfile = logfile
            logfile = f"logs/{new_logfile}.txt"
            os.rename(old_logfile, logfile)
            print0(f"Renamed {old_logfile} -> {logfile}")
    else:
        print(logfile)
#endregion
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    main()

14:49:40.529: ====================================================================================================
14:49:40.529: Running Python 3.12.7 (main, Oct 16 2024, 04:37:19) [Clang 18.1.8 ]
14:49:40.529: Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
14:49:40.609: Sun Feb  2 14:49:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090 Ti     On  |   00000000:2D:00.0 Off |                  Off |
| 30%   44C    P2             96W /  450W |     882MiB /  24564MiB |      4%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A        26      G   /Xwayland                                   N/A      |
|    0   N/A  N/A    259266      C   /python3.12                                 N/A      |
+-----------------------------------------------------------------------------------------+

14:49:40.609: ====================================================================================================
14:49:40.609: Init data
14:49:40.609: Init model
14:49:40.969: torch.bfloat16: 55.105Mi params
14:49:40.969: total: 55.105Mi params
14:49:40.969: Init optimizers
14:49:40.984: Starting train loop
14:50:07.698: step:0/1000 val_loss:10.8258 train_time:0ms step_avg:nanms
14:50:30.308: step:1/1000 train_loss:10.7336 train_time:22609ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:30.638: step:2/1000 train_loss:10.6963 train_time:22939ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:30.865: step:3/1000 train_loss:9.9011 train_time:23167ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:31.093: step:4/1000 train_loss:9.0026 train_time:23395ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:31.323: step:5/1000 train_loss:8.1026 train_time:23625ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:31.553: step:6/1000 train_loss:7.6576 train_time:23854ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:31.781: step:7/1000 train_loss:7.6466 train_time:24082ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:31.011: step:8/1000 train_loss:7.4322 train_time:24313ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:32.239: step:9/1000 train_loss:7.3837 train_time:24540ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:32.468: step:10/1000 train_loss:7.9919 train_time:24769ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:32.696: step:11/1000 train_loss:7.2711 train_time:228ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:32.924: step:12/1000 train_loss:6.9354 train_time:456ms step_avg:nanms torch.cuda.memory_allocated()=369551872
14:50:33.152: step:13/1000 train_loss:6.3853 train_time:683ms step_avg:227.75ms torch.cuda.memory_allocated()=369551872
14:50:33.379: step:14/1000 train_loss:7.0794 train_time:911ms step_avg:227.76ms torch.cuda.memory_allocated()=369551872
14:50:33.607: step:15/1000 train_loss:7.3467 train_time:1138ms step_avg:227.67ms torch.cuda.memory_allocated()=369551872
14:50:33.834: step:16/1000 train_loss:7.3977 train_time:1366ms step_avg:227.60ms torch.cuda.memory_allocated()=369551872
14:50:34.063: step:17/1000 train_loss:7.7037 train_time:1595ms step_avg:227.80ms torch.cuda.memory_allocated()=369551872
14:50:34.291: step:18/1000 train_loss:8.2629 train_time:1823ms step_avg:227.85ms torch.cuda.memory_allocated()=369551872
14:50:34.518: step:19/1000 train_loss:7.2200 train_time:2050ms step_avg:227.76ms torch.cuda.memory_allocated()=369551872
14:50:34.745: step:20/1000 train_loss:7.1437 train_time:2277ms step_avg:227.65ms torch.cuda.memory_allocated()=369551872
14:50:34.972: step:21/1000 train_loss:7.7482 train_time:2504ms step_avg:227.63ms torch.cuda.memory_allocated()=369551872
14:50:35.199: step:22/1000 train_loss:7.0303 train_time:2730ms step_avg:227.53ms torch.cuda.memory_allocated()=369551872
14:50:35.426: step:23/1000 train_loss:6.8707 train_time:2958ms step_avg:227.52ms torch.cuda.memory_allocated()=369551872
14:50:35.655: step:24/1000 train_loss:6.9350 train_time:3186ms step_avg:227.60ms torch.cuda.memory_allocated()=369551872
14:50:35.883: step:25/1000 train_loss:6.9136 train_time:3415ms step_avg:227.66ms torch.cuda.memory_allocated()=369551872
14:50:36.112: step:26/1000 train_loss:6.9634 train_time:3643ms step_avg:227.71ms torch.cuda.memory_allocated()=369551872
14:50:36.339: step:27/1000 train_loss:6.9261 train_time:3871ms step_avg:227.71ms torch.cuda.memory_allocated()=369551872
14:50:36.567: step:28/1000 train_loss:7.2032 train_time:4099ms step_avg:227.72ms torch.cuda.memory_allocated()=369551872
14:50:36.795: step:29/1000 train_loss:7.4009 train_time:4327ms step_avg:227.74ms torch.cuda.memory_allocated()=369551872
14:50:37.023: step:30/1000 train_loss:6.8700 train_time:4555ms step_avg:227.74ms torch.cuda.memory_allocated()=369551872
14:50:37.250: step:31/1000 train_loss:6.9797 train_time:4782ms step_avg:227.70ms torch.cuda.memory_allocated()=369551872
14:50:37.478: step:32/1000 train_loss:6.6635 train_time:5009ms step_avg:227.69ms torch.cuda.memory_allocated()=369551872
14:50:37.706: step:33/1000 train_loss:6.8729 train_time:5238ms step_avg:227.72ms torch.cuda.memory_allocated()=369551872
14:50:37.934: step:34/1000 train_loss:6.5483 train_time:5466ms step_avg:227.75ms torch.cuda.memory_allocated()=369551872
14:50:38.162: step:35/1000 train_loss:6.9351 train_time:5694ms step_avg:227.76ms torch.cuda.memory_allocated()=369551872
14:50:38.390: step:36/1000 train_loss:6.6232 train_time:5921ms step_avg:227.74ms torch.cuda.memory_allocated()=369551872
14:50:38.636: step:37/1000 train_loss:6.7350 train_time:6167ms step_avg:228.42ms torch.cuda.memory_allocated()=369551872
14:50:38.872: step:38/1000 train_loss:6.6206 train_time:6404ms step_avg:228.72ms torch.cuda.memory_allocated()=369551872
14:50:39.101: step:39/1000 train_loss:6.8364 train_time:6633ms step_avg:228.72ms torch.cuda.memory_allocated()=369551872
14:50:39.329: step:40/1000 train_loss:6.4872 train_time:6860ms step_avg:228.68ms torch.cuda.memory_allocated()=369551872
14:50:39.557: step:41/1000 train_loss:6.5143 train_time:7088ms step_avg:228.65ms torch.cuda.memory_allocated()=369551872
14:50:39.784: step:42/1000 train_loss:6.4137 train_time:7316ms step_avg:228.61ms torch.cuda.memory_allocated()=369551872
14:50:39.013: step:43/1000 train_loss:6.5962 train_time:7544ms step_avg:228.62ms torch.cuda.memory_allocated()=369551872
14:50:40.241: step:44/1000 train_loss:6.2410 train_time:7772ms step_avg:228.60ms torch.cuda.memory_allocated()=369551872
14:50:40.468: step:45/1000 train_loss:7.3034 train_time:7999ms step_avg:228.55ms torch.cuda.memory_allocated()=369551872
14:50:40.695: step:46/1000 train_loss:6.4434 train_time:8226ms step_avg:228.51ms torch.cuda.memory_allocated()=369551872
14:50:40.921: step:47/1000 train_loss:6.5320 train_time:8452ms step_avg:228.44ms torch.cuda.memory_allocated()=369551872
14:50:41.147: step:48/1000 train_loss:6.8647 train_time:8679ms step_avg:228.38ms torch.cuda.memory_allocated()=369551872
14:50:41.374: step:49/1000 train_loss:6.5995 train_time:8906ms step_avg:228.36ms torch.cuda.memory_allocated()=369551872
14:50:41.602: step:50/1000 train_loss:6.6182 train_time:9134ms step_avg:228.35ms torch.cuda.memory_allocated()=369551872
14:50:41.829: step:51/1000 train_loss:6.4358 train_time:9361ms step_avg:228.32ms torch.cuda.memory_allocated()=369551872
14:50:42.056: step:52/1000 train_loss:6.7178 train_time:9588ms step_avg:228.29ms torch.cuda.memory_allocated()=369551872
14:50:42.283: step:53/1000 train_loss:6.0384 train_time:9815ms step_avg:228.26ms torch.cuda.memory_allocated()=369551872
14:50:42.512: step:54/1000 train_loss:6.4260 train_time:10044ms step_avg:228.27ms torch.cuda.memory_allocated()=369551872
14:50:42.739: step:55/1000 train_loss:6.9588 train_time:10271ms step_avg:228.25ms torch.cuda.memory_allocated()=369551872
14:50:42.967: step:56/1000 train_loss:6.2877 train_time:10499ms step_avg:228.24ms torch.cuda.memory_allocated()=369551872
14:50:43.196: step:57/1000 train_loss:6.5556 train_time:10727ms step_avg:228.24ms torch.cuda.memory_allocated()=369551872
14:50:43.424: step:58/1000 train_loss:6.4893 train_time:10955ms step_avg:228.24ms torch.cuda.memory_allocated()=369551872
14:50:43.652: step:59/1000 train_loss:6.2490 train_time:11184ms step_avg:228.25ms torch.cuda.memory_allocated()=369551872
14:50:43.882: step:60/1000 train_loss:6.6338 train_time:11413ms step_avg:228.26ms torch.cuda.memory_allocated()=369551872
14:50:44.109: step:61/1000 train_loss:6.8282 train_time:11641ms step_avg:228.25ms torch.cuda.memory_allocated()=369551872
14:50:44.336: step:62/1000 train_loss:6.7120 train_time:11867ms step_avg:228.22ms torch.cuda.memory_allocated()=369551872
14:50:44.564: step:63/1000 train_loss:6.6114 train_time:12095ms step_avg:228.21ms torch.cuda.memory_allocated()=369551872
14:50:44.791: step:64/1000 train_loss:6.5413 train_time:12323ms step_avg:228.20ms torch.cuda.memory_allocated()=369551872
14:50:45.020: step:65/1000 train_loss:6.6500 train_time:12552ms step_avg:228.21ms torch.cuda.memory_allocated()=369551872
14:50:45.247: step:66/1000 train_loss:6.2987 train_time:12779ms step_avg:228.20ms torch.cuda.memory_allocated()=369551872
14:50:45.476: step:67/1000 train_loss:6.7377 train_time:13007ms step_avg:228.20ms torch.cuda.memory_allocated()=369551872
14:50:45.704: step:68/1000 train_loss:6.3321 train_time:13236ms step_avg:228.21ms torch.cuda.memory_allocated()=369551872
14:50:45.932: step:69/1000 train_loss:6.2366 train_time:13464ms step_avg:228.20ms torch.cuda.memory_allocated()=369551872
14:50:46.158: step:70/1000 train_loss:6.3333 train_time:13690ms step_avg:228.17ms torch.cuda.memory_allocated()=369551872
14:50:46.386: step:71/1000 train_loss:6.6116 train_time:13918ms step_avg:228.17ms torch.cuda.memory_allocated()=369551872
14:50:46.615: step:72/1000 train_loss:6.4783 train_time:14147ms step_avg:228.17ms torch.cuda.memory_allocated()=369551872
14:50:46.844: step:73/1000 train_loss:6.3835 train_time:14376ms step_avg:228.18ms torch.cuda.memory_allocated()=369551872
14:50:47.073: step:74/1000 train_loss:6.5184 train_time:14604ms step_avg:228.19ms torch.cuda.memory_allocated()=369551872
14:50:47.299: step:75/1000 train_loss:6.1120 train_time:14831ms step_avg:228.17ms torch.cuda.memory_allocated()=369551872
14:50:47.534: step:76/1000 train_loss:6.3367 train_time:15066ms step_avg:228.27ms torch.cuda.memory_allocated()=369551872
14:50:47.767: step:77/1000 train_loss:6.2988 train_time:15298ms step_avg:228.34ms torch.cuda.memory_allocated()=369551872
14:50:47.001: step:78/1000 train_loss:6.1215 train_time:15533ms step_avg:228.43ms torch.cuda.memory_allocated()=369551872
14:50:48.235: step:79/1000 train_loss:6.4005 train_time:15767ms step_avg:228.51ms torch.cuda.memory_allocated()=369551872
14:50:48.469: step:80/1000 train_loss:6.1184 train_time:16000ms step_avg:228.58ms torch.cuda.memory_allocated()=369551872
14:50:48.703: step:81/1000 train_loss:6.1046 train_time:16234ms step_avg:228.65ms torch.cuda.memory_allocated()=369551872
14:50:48.938: step:82/1000 train_loss:6.1958 train_time:16469ms step_avg:228.74ms torch.cuda.memory_allocated()=369551872
14:50:49.172: step:83/1000 train_loss:6.4912 train_time:16703ms step_avg:228.81ms torch.cuda.memory_allocated()=369551872
14:50:49.405: step:84/1000 train_loss:6.8169 train_time:16937ms step_avg:228.88ms torch.cuda.memory_allocated()=369551872
14:50:49.639: step:85/1000 train_loss:6.1756 train_time:17171ms step_avg:228.95ms torch.cuda.memory_allocated()=369551872
14:50:49.873: step:86/1000 train_loss:6.4580 train_time:17405ms step_avg:229.01ms torch.cuda.memory_allocated()=369551872
14:50:50.108: step:87/1000 train_loss:6.4200 train_time:17640ms step_avg:229.09ms torch.cuda.memory_allocated()=369551872
14:50:50.343: step:88/1000 train_loss:6.2896 train_time:17874ms step_avg:229.16ms torch.cuda.memory_allocated()=369551872
14:50:50.577: step:89/1000 train_loss:6.2284 train_time:18108ms step_avg:229.22ms torch.cuda.memory_allocated()=369551872
14:50:50.810: step:90/1000 train_loss:6.3697 train_time:18341ms step_avg:229.27ms torch.cuda.memory_allocated()=369551872
14:50:51.047: step:91/1000 train_loss:6.2148 train_time:18579ms step_avg:229.36ms torch.cuda.memory_allocated()=369551872
14:50:51.286: step:92/1000 train_loss:6.5204 train_time:18818ms step_avg:229.49ms torch.cuda.memory_allocated()=369551872
14:50:51.522: step:93/1000 train_loss:7.0404 train_time:19053ms step_avg:229.56ms torch.cuda.memory_allocated()=369551872
14:50:51.756: step:94/1000 train_loss:6.1827 train_time:19287ms step_avg:229.61ms torch.cuda.memory_allocated()=369551872
14:50:51.990: step:95/1000 train_loss:6.3419 train_time:19522ms step_avg:229.67ms torch.cuda.memory_allocated()=369551872
14:50:52.223: step:96/1000 train_loss:6.0025 train_time:19755ms step_avg:229.71ms torch.cuda.memory_allocated()=369551872
14:50:52.457: step:97/1000 train_loss:6.0604 train_time:19988ms step_avg:229.75ms torch.cuda.memory_allocated()=369551872
14:50:52.690: step:98/1000 train_loss:6.3527 train_time:20222ms step_avg:229.80ms torch.cuda.memory_allocated()=369551872
14:50:52.925: step:99/1000 train_loss:5.7632 train_time:20456ms step_avg:229.85ms torch.cuda.memory_allocated()=369551872
14:50:53.165: step:100/1000 train_loss:6.2460 train_time:20696ms step_avg:229.96ms torch.cuda.memory_allocated()=369551872
14:50:53.401: step:101/1000 train_loss:6.1700 train_time:20933ms step_avg:230.03ms torch.cuda.memory_allocated()=369551872
14:50:53.635: step:102/1000 train_loss:5.9445 train_time:21167ms step_avg:230.08ms torch.cuda.memory_allocated()=369551872
14:50:53.869: step:103/1000 train_loss:6.0643 train_time:21401ms step_avg:230.12ms torch.cuda.memory_allocated()=369551872
14:50:54.104: step:104/1000 train_loss:6.0383 train_time:21636ms step_avg:230.17ms torch.cuda.memory_allocated()=369551872
14:50:54.337: step:105/1000 train_loss:6.1171 train_time:21869ms step_avg:230.20ms torch.cuda.memory_allocated()=369551872
14:50:54.572: step:106/1000 train_loss:6.4047 train_time:22103ms step_avg:230.24ms torch.cuda.memory_allocated()=369551872
14:50:54.805: step:107/1000 train_loss:6.1621 train_time:22337ms step_avg:230.28ms torch.cuda.memory_allocated()=369551872
14:50:55.039: step:108/1000 train_loss:6.2390 train_time:22570ms step_avg:230.31ms torch.cuda.memory_allocated()=369551872
14:50:55.273: step:109/1000 train_loss:6.1997 train_time:22805ms step_avg:230.35ms torch.cuda.memory_allocated()=369551872
14:50:55.506: step:110/1000 train_loss:5.7543 train_time:23038ms step_avg:230.38ms torch.cuda.memory_allocated()=369551872
14:50:55.741: step:111/1000 train_loss:6.0714 train_time:23272ms step_avg:230.42ms torch.cuda.memory_allocated()=369551872
14:50:55.975: step:112/1000 train_loss:6.2497 train_time:23507ms step_avg:230.46ms torch.cuda.memory_allocated()=369551872
14:50:56.209: step:113/1000 train_loss:5.9714 train_time:23740ms step_avg:230.49ms torch.cuda.memory_allocated()=369551872
14:50:56.442: step:114/1000 train_loss:5.9648 train_time:23973ms step_avg:230.51ms torch.cuda.memory_allocated()=369551872
14:50:56.676: step:115/1000 train_loss:5.9104 train_time:24208ms step_avg:230.55ms torch.cuda.memory_allocated()=369551872
14:50:56.911: step:116/1000 train_loss:6.1184 train_time:24443ms step_avg:230.59ms torch.cuda.memory_allocated()=369551872
14:50:57.146: step:117/1000 train_loss:6.2034 train_time:24678ms step_avg:230.63ms torch.cuda.memory_allocated()=369551872
14:50:57.381: step:118/1000 train_loss:5.9979 train_time:24912ms step_avg:230.67ms torch.cuda.memory_allocated()=369551872
14:50:57.615: step:119/1000 train_loss:5.8645 train_time:25146ms step_avg:230.70ms torch.cuda.memory_allocated()=369551872
14:50:57.850: step:120/1000 train_loss:5.7277 train_time:25381ms step_avg:230.74ms torch.cuda.memory_allocated()=369551872
14:50:58.083: step:121/1000 train_loss:6.5417 train_time:25615ms step_avg:230.77ms torch.cuda.memory_allocated()=369551872
14:50:58.317: step:122/1000 train_loss:6.3834 train_time:25849ms step_avg:230.79ms torch.cuda.memory_allocated()=369551872
14:50:58.550: step:123/1000 train_loss:6.1239 train_time:26082ms step_avg:230.81ms torch.cuda.memory_allocated()=369551872
14:50:58.785: step:124/1000 train_loss:6.1119 train_time:26317ms step_avg:230.85ms torch.cuda.memory_allocated()=369551872
14:50:59.020: step:125/1000 train_loss:6.1244 train_time:26552ms step_avg:230.89ms torch.cuda.memory_allocated()=369551872
14:51:01.930: step:125/1000 val_loss:6.2537 train_time:26552ms step_avg:230.89ms
14:51:02.165: step:126/1000 train_loss:6.5241 train_time:26787ms step_avg:230.92ms torch.cuda.memory_allocated()=369551872
14:51:02.401: step:127/1000 train_loss:6.3512 train_time:27023ms step_avg:230.96ms torch.cuda.memory_allocated()=369551872
14:51:02.638: step:128/1000 train_loss:6.0981 train_time:27259ms step_avg:231.01ms torch.cuda.memory_allocated()=369551872
14:51:02.872: step:129/1000 train_loss:5.8824 train_time:27493ms step_avg:231.04ms torch.cuda.memory_allocated()=369551872
14:51:03.106: step:130/1000 train_loss:6.3570 train_time:27728ms step_avg:231.06ms torch.cuda.memory_allocated()=369551872
14:51:03.341: step:131/1000 train_loss:6.0169 train_time:27963ms step_avg:231.10ms torch.cuda.memory_allocated()=369551872
14:51:03.576: step:132/1000 train_loss:6.1502 train_time:28197ms step_avg:231.13ms torch.cuda.memory_allocated()=369551872
14:51:03.810: step:133/1000 train_loss:6.1288 train_time:28432ms step_avg:231.15ms torch.cuda.memory_allocated()=369551872
14:51:04.046: step:134/1000 train_loss:6.0234 train_time:28668ms step_avg:231.19ms torch.cuda.memory_allocated()=369551872
14:51:04.281: step:135/1000 train_loss:6.5796 train_time:28903ms step_avg:231.22ms torch.cuda.memory_allocated()=369551872
14:51:04.516: step:136/1000 train_loss:6.1654 train_time:29137ms step_avg:231.25ms torch.cuda.memory_allocated()=369551872
14:51:04.751: step:137/1000 train_loss:6.0136 train_time:29372ms step_avg:231.28ms torch.cuda.memory_allocated()=369551872
14:51:04.987: step:138/1000 train_loss:5.9871 train_time:29609ms step_avg:231.32ms torch.cuda.memory_allocated()=369551872
14:51:05.222: step:139/1000 train_loss:6.2866 train_time:29843ms step_avg:231.34ms torch.cuda.memory_allocated()=369551872
14:51:05.457: step:140/1000 train_loss:5.7539 train_time:30078ms step_avg:231.37ms torch.cuda.memory_allocated()=369551872
14:51:05.691: step:141/1000 train_loss:6.0807 train_time:30312ms step_avg:231.39ms torch.cuda.memory_allocated()=369551872
14:51:05.925: step:142/1000 train_loss:6.0130 train_time:30547ms step_avg:231.41ms torch.cuda.memory_allocated()=369551872
14:51:06.161: step:143/1000 train_loss:6.0742 train_time:30782ms step_avg:231.45ms torch.cuda.memory_allocated()=369551872
14:51:06.396: step:144/1000 train_loss:6.6515 train_time:31018ms step_avg:231.47ms torch.cuda.memory_allocated()=369551872
14:51:06.631: step:145/1000 train_loss:6.0274 train_time:31253ms step_avg:231.50ms torch.cuda.memory_allocated()=369551872
14:51:06.866: step:146/1000 train_loss:6.2017 train_time:31487ms step_avg:231.53ms torch.cuda.memory_allocated()=369551872
14:51:07.101: step:147/1000 train_loss:6.0197 train_time:31722ms step_avg:231.55ms torch.cuda.memory_allocated()=369551872
14:51:07.336: step:148/1000 train_loss:5.7124 train_time:31958ms step_avg:231.58ms torch.cuda.memory_allocated()=369551872
14:51:07.578: step:149/1000 train_loss:5.7297 train_time:32199ms step_avg:231.65ms torch.cuda.memory_allocated()=369551872
14:51:07.817: step:150/1000 train_loss:5.7423 train_time:32439ms step_avg:231.70ms torch.cuda.memory_allocated()=369551872
14:51:08.058: step:151/1000 train_loss:5.8725 train_time:32680ms step_avg:231.77ms torch.cuda.memory_allocated()=369551872
14:51:08.297: step:152/1000 train_loss:6.1007 train_time:32919ms step_avg:231.82ms torch.cuda.memory_allocated()=369551872
14:51:08.535: step:153/1000 train_loss:5.9388 train_time:33157ms step_avg:231.87ms torch.cuda.memory_allocated()=369551872
14:51:08.775: step:154/1000 train_loss:6.0057 train_time:33397ms step_avg:231.92ms torch.cuda.memory_allocated()=369551872
14:51:09.019: step:155/1000 train_loss:5.7811 train_time:33640ms step_avg:232.00ms torch.cuda.memory_allocated()=369551872
14:51:09.259: step:156/1000 train_loss:6.1038 train_time:33880ms step_avg:232.06ms torch.cuda.memory_allocated()=369551872
14:51:09.500: step:157/1000 train_loss:6.0146 train_time:34122ms step_avg:232.12ms torch.cuda.memory_allocated()=369551872
14:51:09.740: step:158/1000 train_loss:6.0784 train_time:34361ms step_avg:232.17ms torch.cuda.memory_allocated()=369551872
14:51:09.979: step:159/1000 train_loss:5.8772 train_time:34600ms step_avg:232.22ms torch.cuda.memory_allocated()=369551872
14:51:10.218: step:160/1000 train_loss:5.8959 train_time:34840ms step_avg:232.27ms torch.cuda.memory_allocated()=369551872
14:51:10.457: step:161/1000 train_loss:5.7336 train_time:35079ms step_avg:232.31ms torch.cuda.memory_allocated()=369551872
14:51:10.697: step:162/1000 train_loss:5.9390 train_time:35319ms step_avg:232.36ms torch.cuda.memory_allocated()=369551872
14:51:10.939: step:163/1000 train_loss:5.8312 train_time:35561ms step_avg:232.42ms torch.cuda.memory_allocated()=369551872
14:51:11.179: step:164/1000 train_loss:5.6971 train_time:35801ms step_avg:232.47ms torch.cuda.memory_allocated()=369551872
14:51:11.421: step:165/1000 train_loss:5.8388 train_time:36042ms step_avg:232.53ms torch.cuda.memory_allocated()=369551872
14:51:11.660: step:166/1000 train_loss:5.7474 train_time:36282ms step_avg:232.58ms torch.cuda.memory_allocated()=369551872
14:51:11.903: step:167/1000 train_loss:6.0322 train_time:36525ms step_avg:232.64ms torch.cuda.memory_allocated()=369551872
14:51:12.144: step:168/1000 train_loss:6.0853 train_time:36765ms step_avg:232.69ms torch.cuda.memory_allocated()=369551872
14:51:12.387: step:169/1000 train_loss:5.7245 train_time:37009ms step_avg:232.76ms torch.cuda.memory_allocated()=369551872
14:51:12.627: step:170/1000 train_loss:5.7488 train_time:37249ms step_avg:232.80ms torch.cuda.memory_allocated()=369551872
14:51:12.868: step:171/1000 train_loss:5.8974 train_time:37489ms step_avg:232.85ms torch.cuda.memory_allocated()=369551872
14:51:13.108: step:172/1000 train_loss:5.8625 train_time:37730ms step_avg:232.90ms torch.cuda.memory_allocated()=369551872
14:51:13.348: step:173/1000 train_loss:5.6844 train_time:37969ms step_avg:232.94ms torch.cuda.memory_allocated()=369551872
14:51:13.588: step:174/1000 train_loss:6.0199 train_time:38210ms step_avg:232.99ms torch.cuda.memory_allocated()=369551872
14:51:13.828: step:175/1000 train_loss:5.9384 train_time:38450ms step_avg:233.03ms torch.cuda.memory_allocated()=369551872
14:51:14.069: step:176/1000 train_loss:5.9254 train_time:38691ms step_avg:233.08ms torch.cuda.memory_allocated()=369551872
14:51:14.309: step:177/1000 train_loss:5.7858 train_time:38930ms step_avg:233.12ms torch.cuda.memory_allocated()=369551872
14:51:14.548: step:178/1000 train_loss:5.9895 train_time:39170ms step_avg:233.16ms torch.cuda.memory_allocated()=369551872
14:51:14.789: step:179/1000 train_loss:5.7882 train_time:39411ms step_avg:233.20ms torch.cuda.memory_allocated()=369551872
14:51:15.027: step:180/1000 train_loss:5.6948 train_time:39649ms step_avg:233.23ms torch.cuda.memory_allocated()=369551872
14:51:15.267: step:181/1000 train_loss:5.8638 train_time:39889ms step_avg:233.27ms torch.cuda.memory_allocated()=369551872
14:51:15.506: step:182/1000 train_loss:5.6321 train_time:40128ms step_avg:233.30ms torch.cuda.memory_allocated()=369551872
14:51:15.755: step:183/1000 train_loss:5.7908 train_time:40377ms step_avg:233.39ms torch.cuda.memory_allocated()=369551872
14:51:15.998: step:184/1000 train_loss:5.9001 train_time:40619ms step_avg:233.44ms torch.cuda.memory_allocated()=369551872
14:51:16.238: step:185/1000 train_loss:5.7197 train_time:40859ms step_avg:233.48ms torch.cuda.memory_allocated()=369551872
14:51:16.478: step:186/1000 train_loss:5.9304 train_time:41100ms step_avg:233.52ms torch.cuda.memory_allocated()=369551872
14:51:16.720: step:187/1000 train_loss:5.9490 train_time:41342ms step_avg:233.57ms torch.cuda.memory_allocated()=369551872
14:51:16.964: step:188/1000 train_loss:6.1099 train_time:41586ms step_avg:233.63ms torch.cuda.memory_allocated()=369551872
14:51:17.205: step:189/1000 train_loss:5.9454 train_time:41827ms step_avg:233.67ms torch.cuda.memory_allocated()=369551872
14:51:17.445: step:190/1000 train_loss:5.9744 train_time:42067ms step_avg:233.70ms torch.cuda.memory_allocated()=369551872
14:51:17.685: step:191/1000 train_loss:5.9415 train_time:42307ms step_avg:233.74ms torch.cuda.memory_allocated()=369551872
14:51:17.926: step:192/1000 train_loss:6.1233 train_time:42548ms step_avg:233.78ms torch.cuda.memory_allocated()=369551872
14:51:18.166: step:193/1000 train_loss:5.9077 train_time:42788ms step_avg:233.81ms torch.cuda.memory_allocated()=369551872
14:51:18.407: step:194/1000 train_loss:5.8220 train_time:43029ms step_avg:233.85ms torch.cuda.memory_allocated()=369551872
14:51:18.647: step:195/1000 train_loss:6.4459 train_time:43269ms step_avg:233.89ms torch.cuda.memory_allocated()=369551872
14:51:18.886: step:196/1000 train_loss:6.1981 train_time:43508ms step_avg:233.91ms torch.cuda.memory_allocated()=369551872
14:51:19.127: step:197/1000 train_loss:5.7848 train_time:43749ms step_avg:233.95ms torch.cuda.memory_allocated()=369551872
14:51:19.365: step:198/1000 train_loss:5.8122 train_time:43987ms step_avg:233.97ms torch.cuda.memory_allocated()=369551872
14:51:19.604: step:199/1000 train_loss:5.8489 train_time:44226ms step_avg:234.00ms torch.cuda.memory_allocated()=369551872
14:51:19.843: step:200/1000 train_loss:5.8434 train_time:44465ms step_avg:234.03ms torch.cuda.memory_allocated()=369551872
14:51:20.085: step:201/1000 train_loss:5.7256 train_time:44707ms step_avg:234.07ms torch.cuda.memory_allocated()=369551872
14:51:20.324: step:202/1000 train_loss:6.0082 train_time:44946ms step_avg:234.09ms torch.cuda.memory_allocated()=369551872
14:51:20.565: step:203/1000 train_loss:6.0394 train_time:45187ms step_avg:234.13ms torch.cuda.memory_allocated()=369551872
14:51:20.806: step:204/1000 train_loss:5.5372 train_time:45428ms step_avg:234.16ms torch.cuda.memory_allocated()=369551872
14:51:21.046: step:205/1000 train_loss:6.3002 train_time:45668ms step_avg:234.19ms torch.cuda.memory_allocated()=369551872
14:51:21.286: step:206/1000 train_loss:6.1841 train_time:45908ms step_avg:234.22ms torch.cuda.memory_allocated()=369551872
14:51:21.527: step:207/1000 train_loss:6.0589 train_time:46149ms step_avg:234.26ms torch.cuda.memory_allocated()=369551872
14:51:21.766: step:208/1000 train_loss:6.0324 train_time:46388ms step_avg:234.28ms torch.cuda.memory_allocated()=369551872
14:51:21.007: step:209/1000 train_loss:5.8130 train_time:46629ms step_avg:234.32ms torch.cuda.memory_allocated()=369551872
14:51:22.246: step:210/1000 train_loss:6.0694 train_time:46868ms step_avg:234.34ms torch.cuda.memory_allocated()=369551872
14:51:22.486: step:211/1000 train_loss:6.0981 train_time:47108ms step_avg:234.37ms torch.cuda.memory_allocated()=369551872
14:51:22.726: step:212/1000 train_loss:6.0104 train_time:47348ms step_avg:234.40ms torch.cuda.memory_allocated()=369551872
14:51:22.966: step:213/1000 train_loss:5.8810 train_time:47588ms step_avg:234.42ms torch.cuda.memory_allocated()=369551872
14:51:23.205: step:214/1000 train_loss:5.7480 train_time:47827ms step_avg:234.45ms torch.cuda.memory_allocated()=369551872
14:51:23.444: step:215/1000 train_loss:5.9248 train_time:48066ms step_avg:234.47ms torch.cuda.memory_allocated()=369551872
14:51:23.685: step:216/1000 train_loss:6.0495 train_time:48306ms step_avg:234.50ms torch.cuda.memory_allocated()=369551872
14:51:23.926: step:217/1000 train_loss:5.7642 train_time:48548ms step_avg:234.53ms torch.cuda.memory_allocated()=369551872
14:51:24.166: step:218/1000 train_loss:5.8352 train_time:48788ms step_avg:234.56ms torch.cuda.memory_allocated()=369551872
14:51:24.408: step:219/1000 train_loss:5.8752 train_time:49030ms step_avg:234.59ms torch.cuda.memory_allocated()=369551872
14:51:24.649: step:220/1000 train_loss:5.8662 train_time:49271ms step_avg:234.62ms torch.cuda.memory_allocated()=369551872
14:51:24.890: step:221/1000 train_loss:6.0544 train_time:49512ms step_avg:234.65ms torch.cuda.memory_allocated()=369551872
14:51:25.132: step:222/1000 train_loss:5.9359 train_time:49754ms step_avg:234.69ms torch.cuda.memory_allocated()=369551872
14:51:25.373: step:223/1000 train_loss:5.8908 train_time:49994ms step_avg:234.72ms torch.cuda.memory_allocated()=369551872
14:51:25.618: step:224/1000 train_loss:5.7321 train_time:50240ms step_avg:234.77ms torch.cuda.memory_allocated()=369551872
14:51:25.863: step:225/1000 train_loss:5.6781 train_time:50485ms step_avg:234.81ms torch.cuda.memory_allocated()=369551872
14:51:26.110: step:226/1000 train_loss:6.4795 train_time:50732ms step_avg:234.87ms torch.cuda.memory_allocated()=369551872
14:51:26.356: step:227/1000 train_loss:5.7680 train_time:50978ms step_avg:234.92ms torch.cuda.memory_allocated()=369551872
14:51:26.602: step:228/1000 train_loss:5.7391 train_time:51224ms step_avg:234.97ms torch.cuda.memory_allocated()=369551872
14:51:26.846: step:229/1000 train_loss:5.7232 train_time:51467ms step_avg:235.01ms torch.cuda.memory_allocated()=369551872
14:51:27.090: step:230/1000 train_loss:5.7861 train_time:51712ms step_avg:235.05ms torch.cuda.memory_allocated()=369551872
14:51:27.333: step:231/1000 train_loss:5.9707 train_time:51954ms step_avg:235.09ms torch.cuda.memory_allocated()=369551872
14:51:27.578: step:232/1000 train_loss:5.6117 train_time:52200ms step_avg:235.14ms torch.cuda.memory_allocated()=369551872
14:51:27.821: step:233/1000 train_loss:5.9781 train_time:52443ms step_avg:235.17ms torch.cuda.memory_allocated()=369551872
14:51:28.066: step:234/1000 train_loss:5.6238 train_time:52688ms step_avg:235.21ms torch.cuda.memory_allocated()=369551872
14:51:28.310: step:235/1000 train_loss:5.8870 train_time:52932ms step_avg:235.25ms torch.cuda.memory_allocated()=369551872
14:51:28.552: step:236/1000 train_loss:5.6641 train_time:53174ms step_avg:235.28ms torch.cuda.memory_allocated()=369551872
14:51:28.795: step:237/1000 train_loss:5.5421 train_time:53416ms step_avg:235.31ms torch.cuda.memory_allocated()=369551872
14:51:29.039: step:238/1000 train_loss:5.6736 train_time:53661ms step_avg:235.35ms torch.cuda.memory_allocated()=369551872
14:51:29.282: step:239/1000 train_loss:5.8203 train_time:53904ms step_avg:235.39ms torch.cuda.memory_allocated()=369551872
14:51:29.525: step:240/1000 train_loss:5.6209 train_time:54146ms step_avg:235.42ms torch.cuda.memory_allocated()=369551872
14:51:29.769: step:241/1000 train_loss:4.7827 train_time:54391ms step_avg:235.46ms torch.cuda.memory_allocated()=369551872
14:51:29.013: step:242/1000 train_loss:5.7993 train_time:54634ms step_avg:235.49ms torch.cuda.memory_allocated()=369551872
14:51:30.256: step:243/1000 train_loss:5.9732 train_time:54878ms step_avg:235.53ms torch.cuda.memory_allocated()=369551872
14:51:30.499: step:244/1000 train_loss:6.0983 train_time:55121ms step_avg:235.56ms torch.cuda.memory_allocated()=369551872
14:51:30.742: step:245/1000 train_loss:5.6623 train_time:55364ms step_avg:235.59ms torch.cuda.memory_allocated()=369551872
14:51:30.987: step:246/1000 train_loss:5.6898 train_time:55609ms step_avg:235.63ms torch.cuda.memory_allocated()=369551872
14:51:31.231: step:247/1000 train_loss:5.8669 train_time:55853ms step_avg:235.67ms torch.cuda.memory_allocated()=369551872
14:51:31.476: step:248/1000 train_loss:5.7022 train_time:56098ms step_avg:235.71ms torch.cuda.memory_allocated()=369551872
14:51:31.720: step:249/1000 train_loss:6.2187 train_time:56341ms step_avg:235.74ms torch.cuda.memory_allocated()=369551872
14:51:31.963: step:250/1000 train_loss:5.7994 train_time:56585ms step_avg:235.77ms torch.cuda.memory_allocated()=369551872
14:51:34.632: step:250/1000 val_loss:5.9656 train_time:56586ms step_avg:235.77ms
14:51:34.876: step:251/1000 train_loss:5.6108 train_time:56829ms step_avg:235.81ms torch.cuda.memory_allocated()=369551872
14:51:35.121: step:252/1000 train_loss:5.9354 train_time:57074ms step_avg:235.84ms torch.cuda.memory_allocated()=369551872
14:51:35.367: step:253/1000 train_loss:5.5092 train_time:57320ms step_avg:235.89ms torch.cuda.memory_allocated()=369551872
14:51:35.613: step:254/1000 train_loss:5.3603 train_time:57567ms step_avg:235.93ms torch.cuda.memory_allocated()=369551872
14:51:35.861: step:255/1000 train_loss:5.5603 train_time:57815ms step_avg:235.98ms torch.cuda.memory_allocated()=369551872
14:51:36.107: step:256/1000 train_loss:5.8141 train_time:58060ms step_avg:236.02ms torch.cuda.memory_allocated()=369551872
14:51:36.351: step:257/1000 train_loss:6.0047 train_time:58305ms step_avg:236.05ms torch.cuda.memory_allocated()=369551872
14:51:36.597: step:258/1000 train_loss:5.7763 train_time:58551ms step_avg:236.09ms torch.cuda.memory_allocated()=369551872
14:51:36.842: step:259/1000 train_loss:5.8368 train_time:58795ms step_avg:236.13ms torch.cuda.memory_allocated()=369551872
14:51:37.086: step:260/1000 train_loss:5.8713 train_time:59040ms step_avg:236.16ms torch.cuda.memory_allocated()=369551872
14:51:37.330: step:261/1000 train_loss:6.0475 train_time:59284ms step_avg:236.19ms torch.cuda.memory_allocated()=369551872
14:51:37.575: step:262/1000 train_loss:5.7229 train_time:59529ms step_avg:236.23ms torch.cuda.memory_allocated()=369551872
14:51:37.820: step:263/1000 train_loss:5.7975 train_time:59773ms step_avg:236.26ms torch.cuda.memory_allocated()=369551872
14:51:38.065: step:264/1000 train_loss:5.7477 train_time:60018ms step_avg:236.29ms torch.cuda.memory_allocated()=369551872
14:51:38.308: step:265/1000 train_loss:5.9470 train_time:60261ms step_avg:236.32ms torch.cuda.memory_allocated()=369551872
14:51:38.551: step:266/1000 train_loss:5.6557 train_time:60505ms step_avg:236.35ms torch.cuda.memory_allocated()=369551872
14:51:38.795: step:267/1000 train_loss:5.8996 train_time:60748ms step_avg:236.37ms torch.cuda.memory_allocated()=369551872
14:51:39.039: step:268/1000 train_loss:5.8328 train_time:60992ms step_avg:236.40ms torch.cuda.memory_allocated()=369551872
14:51:39.283: step:269/1000 train_loss:5.8828 train_time:61237ms step_avg:236.43ms torch.cuda.memory_allocated()=369551872
14:51:39.528: step:270/1000 train_loss:5.9326 train_time:61481ms step_avg:236.47ms torch.cuda.memory_allocated()=369551872
14:51:39.772: step:271/1000 train_loss:5.8539 train_time:61726ms step_avg:236.50ms torch.cuda.memory_allocated()=369551872
14:51:40.017: step:272/1000 train_loss:6.1261 train_time:61971ms step_avg:236.53ms torch.cuda.memory_allocated()=369551872
14:51:40.260: step:273/1000 train_loss:5.9459 train_time:62213ms step_avg:236.55ms torch.cuda.memory_allocated()=369551872
14:51:40.503: step:274/1000 train_loss:5.8530 train_time:62456ms step_avg:236.58ms torch.cuda.memory_allocated()=369551872
14:51:40.746: step:275/1000 train_loss:5.9103 train_time:62699ms step_avg:236.60ms torch.cuda.memory_allocated()=369551872
14:51:40.991: step:276/1000 train_loss:6.1166 train_time:62944ms step_avg:236.63ms torch.cuda.memory_allocated()=369551872
14:51:41.235: step:277/1000 train_loss:5.6809 train_time:63188ms step_avg:236.66ms torch.cuda.memory_allocated()=369551872
14:51:41.480: step:278/1000 train_loss:5.6502 train_time:63433ms step_avg:236.69ms torch.cuda.memory_allocated()=369551872
14:51:41.724: step:279/1000 train_loss:5.7752 train_time:63677ms step_avg:236.72ms torch.cuda.memory_allocated()=369551872
14:51:41.967: step:280/1000 train_loss:5.8306 train_time:63920ms step_avg:236.74ms torch.cuda.memory_allocated()=369551872
14:51:42.211: step:281/1000 train_loss:5.6029 train_time:64164ms step_avg:236.77ms torch.cuda.memory_allocated()=369551872
14:51:42.456: step:282/1000 train_loss:5.6804 train_time:64410ms step_avg:236.80ms torch.cuda.memory_allocated()=369551872
14:51:42.701: step:283/1000 train_loss:5.5377 train_time:64655ms step_avg:236.83ms torch.cuda.memory_allocated()=369551872
14:51:42.945: step:284/1000 train_loss:5.7671 train_time:64899ms step_avg:236.86ms torch.cuda.memory_allocated()=369551872
14:51:43.190: step:285/1000 train_loss:6.0385 train_time:65143ms step_avg:236.88ms torch.cuda.memory_allocated()=369551872
14:51:43.434: step:286/1000 train_loss:6.8384 train_time:65387ms step_avg:236.91ms torch.cuda.memory_allocated()=369551872
14:51:43.678: step:287/1000 train_loss:6.3525 train_time:65632ms step_avg:236.94ms torch.cuda.memory_allocated()=369551872
14:51:43.923: step:288/1000 train_loss:6.0381 train_time:65876ms step_avg:236.97ms torch.cuda.memory_allocated()=369551872
14:51:44.169: step:289/1000 train_loss:6.0627 train_time:66122ms step_avg:237.00ms torch.cuda.memory_allocated()=369551872
14:51:44.413: step:290/1000 train_loss:5.7833 train_time:66366ms step_avg:237.02ms torch.cuda.memory_allocated()=369551872
14:51:44.658: step:291/1000 train_loss:5.3842 train_time:66611ms step_avg:237.05ms torch.cuda.memory_allocated()=369551872
14:51:44.902: step:292/1000 train_loss:6.0644 train_time:66855ms step_avg:237.08ms torch.cuda.memory_allocated()=369551872
14:51:45.146: step:293/1000 train_loss:5.4736 train_time:67100ms step_avg:237.10ms torch.cuda.memory_allocated()=369551872
14:51:45.391: step:294/1000 train_loss:5.4512 train_time:67344ms step_avg:237.13ms torch.cuda.memory_allocated()=369551872
14:51:45.635: step:295/1000 train_loss:5.7708 train_time:67588ms step_avg:237.15ms torch.cuda.memory_allocated()=369551872
14:51:45.879: step:296/1000 train_loss:5.7937 train_time:67832ms step_avg:237.18ms torch.cuda.memory_allocated()=369551872
14:51:46.122: step:297/1000 train_loss:5.6810 train_time:68076ms step_avg:237.20ms torch.cuda.memory_allocated()=369551872
14:51:46.371: step:298/1000 train_loss:5.5056 train_time:68324ms step_avg:237.24ms torch.cuda.memory_allocated()=369551872
14:51:46.620: step:299/1000 train_loss:5.9926 train_time:68573ms step_avg:237.28ms torch.cuda.memory_allocated()=369551872
14:51:46.867: step:300/1000 train_loss:5.8232 train_time:68821ms step_avg:237.31ms torch.cuda.memory_allocated()=369551872
14:51:47.117: step:301/1000 train_loss:5.6994 train_time:69070ms step_avg:237.35ms torch.cuda.memory_allocated()=369551872
14:51:47.366: step:302/1000 train_loss:5.3527 train_time:69319ms step_avg:237.39ms torch.cuda.memory_allocated()=369551872
14:51:47.615: step:303/1000 train_loss:5.5977 train_time:69568ms step_avg:237.43ms torch.cuda.memory_allocated()=369551872
14:51:47.862: step:304/1000 train_loss:5.8278 train_time:69815ms step_avg:237.47ms torch.cuda.memory_allocated()=369551872
14:51:48.110: step:305/1000 train_loss:5.6505 train_time:70063ms step_avg:237.50ms torch.cuda.memory_allocated()=369551872
14:51:48.357: step:306/1000 train_loss:5.9215 train_time:70310ms step_avg:237.53ms torch.cuda.memory_allocated()=369551872
14:51:48.604: step:307/1000 train_loss:5.6889 train_time:70558ms step_avg:237.57ms torch.cuda.memory_allocated()=369551872
14:51:48.851: step:308/1000 train_loss:5.8455 train_time:70804ms step_avg:237.60ms torch.cuda.memory_allocated()=369551872
14:51:49.100: step:309/1000 train_loss:5.6179 train_time:71053ms step_avg:237.64ms torch.cuda.memory_allocated()=369551872
14:51:49.346: step:310/1000 train_loss:5.7339 train_time:71299ms step_avg:237.66ms torch.cuda.memory_allocated()=369551872
14:51:49.592: step:311/1000 train_loss:5.6058 train_time:71545ms step_avg:237.69ms torch.cuda.memory_allocated()=369551872
14:51:49.839: step:312/1000 train_loss:5.6741 train_time:71793ms step_avg:237.73ms torch.cuda.memory_allocated()=369551872
14:51:50.089: step:313/1000 train_loss:5.4820 train_time:72043ms step_avg:237.76ms torch.cuda.memory_allocated()=369551872
14:51:50.337: step:314/1000 train_loss:5.6751 train_time:72290ms step_avg:237.80ms torch.cuda.memory_allocated()=369551872
14:51:50.586: step:315/1000 train_loss:5.9100 train_time:72539ms step_avg:237.83ms torch.cuda.memory_allocated()=369551872
14:51:50.834: step:316/1000 train_loss:5.8115 train_time:72787ms step_avg:237.87ms torch.cuda.memory_allocated()=369551872
14:51:51.082: step:317/1000 train_loss:5.6408 train_time:73035ms step_avg:237.90ms torch.cuda.memory_allocated()=369551872
14:51:51.329: step:318/1000 train_loss:5.6612 train_time:73283ms step_avg:237.93ms torch.cuda.memory_allocated()=369551872
14:51:51.581: step:319/1000 train_loss:5.7726 train_time:73534ms step_avg:237.97ms torch.cuda.memory_allocated()=369551872
14:51:51.828: step:320/1000 train_loss:5.5827 train_time:73781ms step_avg:238.00ms torch.cuda.memory_allocated()=369551872
14:51:52.077: step:321/1000 train_loss:5.5369 train_time:74030ms step_avg:238.04ms torch.cuda.memory_allocated()=369551872
14:51:52.325: step:322/1000 train_loss:5.7288 train_time:74279ms step_avg:238.07ms torch.cuda.memory_allocated()=369551872
14:51:52.575: step:323/1000 train_loss:5.8987 train_time:74528ms step_avg:238.11ms torch.cuda.memory_allocated()=369551872
14:51:52.822: step:324/1000 train_loss:5.8299 train_time:74776ms step_avg:238.14ms torch.cuda.memory_allocated()=369551872
14:51:53.070: step:325/1000 train_loss:5.6566 train_time:75023ms step_avg:238.17ms torch.cuda.memory_allocated()=369551872
14:51:53.317: step:326/1000 train_loss:5.7319 train_time:75270ms step_avg:238.20ms torch.cuda.memory_allocated()=369551872
14:51:53.566: step:327/1000 train_loss:5.7056 train_time:75519ms step_avg:238.23ms torch.cuda.memory_allocated()=369551872
14:51:53.813: step:328/1000 train_loss:5.6679 train_time:75767ms step_avg:238.26ms torch.cuda.memory_allocated()=369551872
14:51:54.063: step:329/1000 train_loss:5.6917 train_time:76016ms step_avg:238.30ms torch.cuda.memory_allocated()=369551872
14:51:54.313: step:330/1000 train_loss:5.5938 train_time:76266ms step_avg:238.33ms torch.cuda.memory_allocated()=369551872
14:51:54.560: step:331/1000 train_loss:5.5961 train_time:76513ms step_avg:238.36ms torch.cuda.memory_allocated()=369551872
14:51:54.807: step:332/1000 train_loss:6.0020 train_time:76760ms step_avg:238.39ms torch.cuda.memory_allocated()=369551872
14:51:55.056: step:333/1000 train_loss:5.5686 train_time:77010ms step_avg:238.42ms torch.cuda.memory_allocated()=369551872
14:51:55.303: step:334/1000 train_loss:5.4496 train_time:77256ms step_avg:238.45ms torch.cuda.memory_allocated()=369551872
14:51:55.551: step:335/1000 train_loss:5.8082 train_time:77504ms step_avg:238.47ms torch.cuda.memory_allocated()=369551872
14:51:55.800: step:336/1000 train_loss:5.5336 train_time:77753ms step_avg:238.51ms torch.cuda.memory_allocated()=369551872
14:51:56.049: step:337/1000 train_loss:5.7212 train_time:78003ms step_avg:238.54ms torch.cuda.memory_allocated()=369551872
14:51:56.296: step:338/1000 train_loss:5.6598 train_time:78250ms step_avg:238.57ms torch.cuda.memory_allocated()=369551872
14:51:56.547: step:339/1000 train_loss:5.7873 train_time:78500ms step_avg:238.60ms torch.cuda.memory_allocated()=369551872
14:51:56.793: step:340/1000 train_loss:5.6490 train_time:78746ms step_avg:238.63ms torch.cuda.memory_allocated()=369551872
14:51:57.040: step:341/1000 train_loss:5.5023 train_time:78993ms step_avg:238.65ms torch.cuda.memory_allocated()=369551872
14:51:57.288: step:342/1000 train_loss:6.0083 train_time:79241ms step_avg:238.68ms torch.cuda.memory_allocated()=369551872
14:51:57.534: step:343/1000 train_loss:5.5664 train_time:79487ms step_avg:238.70ms torch.cuda.memory_allocated()=369551872
14:51:57.779: step:344/1000 train_loss:5.7993 train_time:79733ms step_avg:238.72ms torch.cuda.memory_allocated()=369551872
14:51:58.028: step:345/1000 train_loss:5.7079 train_time:79981ms step_avg:238.75ms torch.cuda.memory_allocated()=369551872
14:51:58.274: step:346/1000 train_loss:5.4632 train_time:80227ms step_avg:238.77ms torch.cuda.memory_allocated()=369551872
14:51:58.520: step:347/1000 train_loss:5.6373 train_time:80474ms step_avg:238.79ms torch.cuda.memory_allocated()=369551872
14:51:58.769: step:348/1000 train_loss:5.5590 train_time:80722ms step_avg:238.82ms torch.cuda.memory_allocated()=369551872
14:51:59.017: step:349/1000 train_loss:6.4040 train_time:80970ms step_avg:238.85ms torch.cuda.memory_allocated()=369551872
14:51:59.266: step:350/1000 train_loss:5.5966 train_time:81219ms step_avg:238.88ms torch.cuda.memory_allocated()=369551872
14:51:59.513: step:351/1000 train_loss:5.7709 train_time:81466ms step_avg:238.90ms torch.cuda.memory_allocated()=369551872
14:51:59.761: step:352/1000 train_loss:5.5389 train_time:81714ms step_avg:238.93ms torch.cuda.memory_allocated()=369551872
14:51:59.008: step:353/1000 train_loss:5.6455 train_time:81962ms step_avg:238.96ms torch.cuda.memory_allocated()=369551872
14:52:00.257: step:354/1000 train_loss:5.8127 train_time:82211ms step_avg:238.99ms torch.cuda.memory_allocated()=369551872
14:52:00.506: step:355/1000 train_loss:5.5633 train_time:82460ms step_avg:239.01ms torch.cuda.memory_allocated()=369551872
14:52:00.756: step:356/1000 train_loss:5.6874 train_time:82709ms step_avg:239.04ms torch.cuda.memory_allocated()=369551872
14:52:00.002: step:357/1000 train_loss:5.7081 train_time:82955ms step_avg:239.06ms torch.cuda.memory_allocated()=369551872
14:52:01.248: step:358/1000 train_loss:5.3829 train_time:83202ms step_avg:239.09ms torch.cuda.memory_allocated()=369551872
14:52:01.496: step:359/1000 train_loss:5.8406 train_time:83450ms step_avg:239.11ms torch.cuda.memory_allocated()=369551872
14:52:01.743: step:360/1000 train_loss:5.7234 train_time:83697ms step_avg:239.13ms torch.cuda.memory_allocated()=369551872
14:52:01.991: step:361/1000 train_loss:5.7367 train_time:83945ms step_avg:239.16ms torch.cuda.memory_allocated()=369551872
14:52:02.239: step:362/1000 train_loss:5.8440 train_time:84192ms step_avg:239.18ms torch.cuda.memory_allocated()=369551872
14:52:02.486: step:363/1000 train_loss:5.6602 train_time:84440ms step_avg:239.21ms torch.cuda.memory_allocated()=369551872
14:52:02.734: step:364/1000 train_loss:5.4859 train_time:84688ms step_avg:239.23ms torch.cuda.memory_allocated()=369551872
14:52:02.983: step:365/1000 train_loss:5.5864 train_time:84936ms step_avg:239.26ms torch.cuda.memory_allocated()=369551872
14:52:03.229: step:366/1000 train_loss:5.6571 train_time:85183ms step_avg:239.28ms torch.cuda.memory_allocated()=369551872
14:52:03.476: step:367/1000 train_loss:5.3511 train_time:85430ms step_avg:239.30ms torch.cuda.memory_allocated()=369551872
14:52:03.725: step:368/1000 train_loss:6.0185 train_time:85678ms step_avg:239.32ms torch.cuda.memory_allocated()=369551872
14:52:03.971: step:369/1000 train_loss:5.5793 train_time:85925ms step_avg:239.35ms torch.cuda.memory_allocated()=369551872
14:52:04.220: step:370/1000 train_loss:5.5434 train_time:86174ms step_avg:239.37ms torch.cuda.memory_allocated()=369551872
14:52:04.467: step:371/1000 train_loss:5.7834 train_time:86421ms step_avg:239.39ms torch.cuda.memory_allocated()=369551872
14:52:04.717: step:372/1000 train_loss:5.4622 train_time:86670ms step_avg:239.42ms torch.cuda.memory_allocated()=369551872
14:52:04.966: step:373/1000 train_loss:5.7759 train_time:86920ms step_avg:239.45ms torch.cuda.memory_allocated()=369551872
14:52:05.217: step:374/1000 train_loss:5.7699 train_time:87171ms step_avg:239.48ms torch.cuda.memory_allocated()=369551872
14:52:05.467: step:375/1000 train_loss:5.9727 train_time:87421ms step_avg:239.51ms torch.cuda.memory_allocated()=369551872
14:52:08.166: step:375/1000 val_loss:5.7881 train_time:87421ms step_avg:239.51ms
14:52:08.419: step:376/1000 train_loss:5.9787 train_time:87674ms step_avg:239.55ms torch.cuda.memory_allocated()=369551872
14:52:08.671: step:377/1000 train_loss:5.5878 train_time:87926ms step_avg:239.58ms torch.cuda.memory_allocated()=369551872
14:52:08.919: step:378/1000 train_loss:5.8481 train_time:88175ms step_avg:239.60ms torch.cuda.memory_allocated()=369551872
14:52:09.168: step:379/1000 train_loss:5.7437 train_time:88423ms step_avg:239.63ms torch.cuda.memory_allocated()=369551872
14:52:09.419: step:380/1000 train_loss:5.5180 train_time:88674ms step_avg:239.66ms torch.cuda.memory_allocated()=369551872
14:52:09.672: step:381/1000 train_loss:5.4145 train_time:88927ms step_avg:239.69ms torch.cuda.memory_allocated()=369551872
14:52:09.922: step:382/1000 train_loss:5.7693 train_time:89177ms step_avg:239.72ms torch.cuda.memory_allocated()=369551872
14:52:10.173: step:383/1000 train_loss:5.7779 train_time:89428ms step_avg:239.75ms torch.cuda.memory_allocated()=369551872
14:52:10.425: step:384/1000 train_loss:5.5571 train_time:89680ms step_avg:239.78ms torch.cuda.memory_allocated()=369551872
14:52:10.674: step:385/1000 train_loss:5.5545 train_time:89930ms step_avg:239.81ms torch.cuda.memory_allocated()=369551872
14:52:10.922: step:386/1000 train_loss:5.5491 train_time:90177ms step_avg:239.83ms torch.cuda.memory_allocated()=369551872
14:52:11.172: step:387/1000 train_loss:5.6348 train_time:90427ms step_avg:239.86ms torch.cuda.memory_allocated()=369551872
14:52:11.423: step:388/1000 train_loss:5.6227 train_time:90678ms step_avg:239.89ms torch.cuda.memory_allocated()=369551872
14:52:11.671: step:389/1000 train_loss:5.5387 train_time:90926ms step_avg:239.91ms torch.cuda.memory_allocated()=369551872
14:52:11.922: step:390/1000 train_loss:5.7372 train_time:91177ms step_avg:239.94ms torch.cuda.memory_allocated()=369551872
14:52:12.173: step:391/1000 train_loss:5.5869 train_time:91428ms step_avg:239.97ms torch.cuda.memory_allocated()=369551872
14:52:12.424: step:392/1000 train_loss:5.6315 train_time:91679ms step_avg:240.00ms torch.cuda.memory_allocated()=369551872
14:52:12.675: step:393/1000 train_loss:5.7558 train_time:91930ms step_avg:240.03ms torch.cuda.memory_allocated()=369551872
14:52:12.929: step:394/1000 train_loss:6.0468 train_time:92184ms step_avg:240.06ms torch.cuda.memory_allocated()=369551872
14:52:13.179: step:395/1000 train_loss:5.7694 train_time:92434ms step_avg:240.09ms torch.cuda.memory_allocated()=369551872
14:52:13.426: step:396/1000 train_loss:5.6561 train_time:92681ms step_avg:240.11ms torch.cuda.memory_allocated()=369551872
14:52:13.679: step:397/1000 train_loss:5.7520 train_time:92934ms step_avg:240.14ms torch.cuda.memory_allocated()=369551872
14:52:13.933: step:398/1000 train_loss:5.4373 train_time:93189ms step_avg:240.18ms torch.cuda.memory_allocated()=369551872
14:52:14.182: step:399/1000 train_loss:5.8980 train_time:93438ms step_avg:240.20ms torch.cuda.memory_allocated()=369551872
14:52:14.431: step:400/1000 train_loss:5.5900 train_time:93686ms step_avg:240.22ms torch.cuda.memory_allocated()=369551872
14:52:14.681: step:401/1000 train_loss:5.6091 train_time:93936ms step_avg:240.24ms torch.cuda.memory_allocated()=369551872
14:52:14.930: step:402/1000 train_loss:5.6493 train_time:94185ms step_avg:240.27ms torch.cuda.memory_allocated()=369551872
14:52:15.178: step:403/1000 train_loss:5.7355 train_time:94433ms step_avg:240.29ms torch.cuda.memory_allocated()=369551872
14:52:15.429: step:404/1000 train_loss:5.5543 train_time:94684ms step_avg:240.31ms torch.cuda.memory_allocated()=369551872
14:52:15.681: step:405/1000 train_loss:5.4969 train_time:94936ms step_avg:240.34ms torch.cuda.memory_allocated()=369551872
14:52:15.932: step:406/1000 train_loss:5.4261 train_time:95187ms step_avg:240.37ms torch.cuda.memory_allocated()=369551872
14:52:16.181: step:407/1000 train_loss:5.6899 train_time:95436ms step_avg:240.39ms torch.cuda.memory_allocated()=369551872
14:52:16.431: step:408/1000 train_loss:5.4011 train_time:95686ms step_avg:240.42ms torch.cuda.memory_allocated()=369551872
14:52:16.683: step:409/1000 train_loss:6.0155 train_time:95938ms step_avg:240.45ms torch.cuda.memory_allocated()=369551872
14:52:16.935: step:410/1000 train_loss:5.6190 train_time:96190ms step_avg:240.48ms torch.cuda.memory_allocated()=369551872
14:52:17.185: step:411/1000 train_loss:5.6067 train_time:96440ms step_avg:240.50ms torch.cuda.memory_allocated()=369551872
14:52:17.436: step:412/1000 train_loss:5.8230 train_time:96691ms step_avg:240.53ms torch.cuda.memory_allocated()=369551872
14:52:17.685: step:413/1000 train_loss:5.6502 train_time:96940ms step_avg:240.55ms torch.cuda.memory_allocated()=369551872
14:52:17.933: step:414/1000 train_loss:5.3898 train_time:97188ms step_avg:240.57ms torch.cuda.memory_allocated()=369551872
14:52:18.186: step:415/1000 train_loss:5.5756 train_time:97441ms step_avg:240.60ms torch.cuda.memory_allocated()=369551872
14:52:18.435: step:416/1000 train_loss:5.5525 train_time:97690ms step_avg:240.62ms torch.cuda.memory_allocated()=369551872
14:52:18.686: step:417/1000 train_loss:5.4402 train_time:97941ms step_avg:240.64ms torch.cuda.memory_allocated()=369551872
14:52:18.936: step:418/1000 train_loss:5.5678 train_time:98191ms step_avg:240.66ms torch.cuda.memory_allocated()=369551872
14:52:19.185: step:419/1000 train_loss:5.7538 train_time:98440ms step_avg:240.68ms torch.cuda.memory_allocated()=369551872
14:52:19.436: step:420/1000 train_loss:5.5660 train_time:98691ms step_avg:240.71ms torch.cuda.memory_allocated()=369551872
14:52:19.688: step:421/1000 train_loss:5.6890 train_time:98943ms step_avg:240.74ms torch.cuda.memory_allocated()=369551872
14:52:19.936: step:422/1000 train_loss:5.7124 train_time:99191ms step_avg:240.75ms torch.cuda.memory_allocated()=369551872
14:52:20.189: step:423/1000 train_loss:5.5673 train_time:99444ms step_avg:240.78ms torch.cuda.memory_allocated()=369551872
14:52:20.436: step:424/1000 train_loss:5.5474 train_time:99691ms step_avg:240.80ms torch.cuda.memory_allocated()=369551872
14:52:20.688: step:425/1000 train_loss:5.1878 train_time:99943ms step_avg:240.83ms torch.cuda.memory_allocated()=369551872
14:52:20.937: step:426/1000 train_loss:5.3642 train_time:100192ms step_avg:240.85ms torch.cuda.memory_allocated()=369551872
14:52:21.190: step:427/1000 train_loss:5.4209 train_time:100445ms step_avg:240.87ms torch.cuda.memory_allocated()=369551872
14:52:21.444: step:428/1000 train_loss:5.4000 train_time:100699ms step_avg:240.91ms torch.cuda.memory_allocated()=369551872
14:52:21.696: step:429/1000 train_loss:5.8320 train_time:100951ms step_avg:240.93ms torch.cuda.memory_allocated()=369551872
14:52:21.946: step:430/1000 train_loss:6.1828 train_time:101201ms step_avg:240.96ms torch.cuda.memory_allocated()=369551872
14:52:22.199: step:431/1000 train_loss:5.6078 train_time:101454ms step_avg:240.98ms torch.cuda.memory_allocated()=369551872
14:52:22.450: step:432/1000 train_loss:5.4794 train_time:101705ms step_avg:241.01ms torch.cuda.memory_allocated()=369551872
14:52:22.700: step:433/1000 train_loss:5.5974 train_time:101955ms step_avg:241.03ms torch.cuda.memory_allocated()=369551872
14:52:22.950: step:434/1000 train_loss:5.7900 train_time:102205ms step_avg:241.05ms torch.cuda.memory_allocated()=369551872
14:52:23.203: step:435/1000 train_loss:5.4095 train_time:102458ms step_avg:241.08ms torch.cuda.memory_allocated()=369551872
14:52:23.452: step:436/1000 train_loss:5.3848 train_time:102707ms step_avg:241.10ms torch.cuda.memory_allocated()=369551872
14:52:23.701: step:437/1000 train_loss:5.4113 train_time:102956ms step_avg:241.11ms torch.cuda.memory_allocated()=369551872
14:52:23.949: step:438/1000 train_loss:5.4226 train_time:103204ms step_avg:241.13ms torch.cuda.memory_allocated()=369551872
14:52:24.199: step:439/1000 train_loss:5.2335 train_time:103454ms step_avg:241.15ms torch.cuda.memory_allocated()=369551872
14:52:24.450: step:440/1000 train_loss:5.2719 train_time:103705ms step_avg:241.17ms torch.cuda.memory_allocated()=369551872
14:52:24.702: step:441/1000 train_loss:5.5318 train_time:103957ms step_avg:241.20ms torch.cuda.memory_allocated()=369551872
14:52:24.955: step:442/1000 train_loss:5.4633 train_time:104210ms step_avg:241.23ms torch.cuda.memory_allocated()=369551872
14:52:25.204: step:443/1000 train_loss:5.5963 train_time:104459ms step_avg:241.24ms torch.cuda.memory_allocated()=369551872
14:52:25.460: step:444/1000 train_loss:5.3571 train_time:104715ms step_avg:241.28ms torch.cuda.memory_allocated()=369551872
14:52:25.711: step:445/1000 train_loss:5.6373 train_time:104966ms step_avg:241.30ms torch.cuda.memory_allocated()=369551872
14:52:25.963: step:446/1000 train_loss:5.5761 train_time:105218ms step_avg:241.33ms torch.cuda.memory_allocated()=369551872
14:52:26.214: step:447/1000 train_loss:5.6138 train_time:105469ms step_avg:241.35ms torch.cuda.memory_allocated()=369551872
14:52:26.470: step:448/1000 train_loss:5.6222 train_time:105725ms step_avg:241.38ms torch.cuda.memory_allocated()=369551872
14:52:26.723: step:449/1000 train_loss:5.4330 train_time:105978ms step_avg:241.41ms torch.cuda.memory_allocated()=369551872
14:52:26.977: step:450/1000 train_loss:5.5647 train_time:106232ms step_avg:241.44ms torch.cuda.memory_allocated()=369551872
14:52:27.233: step:451/1000 train_loss:5.5769 train_time:106488ms step_avg:241.47ms torch.cuda.memory_allocated()=369551872
14:52:27.490: step:452/1000 train_loss:5.6049 train_time:106745ms step_avg:241.51ms torch.cuda.memory_allocated()=369551872
14:52:27.743: step:453/1000 train_loss:5.3845 train_time:106998ms step_avg:241.53ms torch.cuda.memory_allocated()=369551872
14:52:27.994: step:454/1000 train_loss:5.5997 train_time:107249ms step_avg:241.55ms torch.cuda.memory_allocated()=369551872
14:52:28.248: step:455/1000 train_loss:5.6071 train_time:107503ms step_avg:241.58ms torch.cuda.memory_allocated()=369551872
14:52:28.502: step:456/1000 train_loss:5.7442 train_time:107757ms step_avg:241.61ms torch.cuda.memory_allocated()=369551872
14:52:28.755: step:457/1000 train_loss:5.5784 train_time:108010ms step_avg:241.63ms torch.cuda.memory_allocated()=369551872
14:52:28.007: step:458/1000 train_loss:5.5185 train_time:108263ms step_avg:241.66ms torch.cuda.memory_allocated()=369551872
14:52:29.260: step:459/1000 train_loss:5.4735 train_time:108515ms step_avg:241.68ms torch.cuda.memory_allocated()=369551872
14:52:29.514: step:460/1000 train_loss:5.5267 train_time:108769ms step_avg:241.71ms torch.cuda.memory_allocated()=369551872
14:52:29.764: step:461/1000 train_loss:5.4368 train_time:109020ms step_avg:241.73ms torch.cuda.memory_allocated()=369551872
14:52:30.021: step:462/1000 train_loss:5.4841 train_time:109276ms step_avg:241.76ms torch.cuda.memory_allocated()=369551872
14:52:30.276: step:463/1000 train_loss:5.8346 train_time:109531ms step_avg:241.79ms torch.cuda.memory_allocated()=369551872
14:52:30.528: step:464/1000 train_loss:5.7017 train_time:109783ms step_avg:241.81ms torch.cuda.memory_allocated()=369551872
14:52:30.779: step:465/1000 train_loss:5.5876 train_time:110035ms step_avg:241.83ms torch.cuda.memory_allocated()=369551872
14:52:31.033: step:466/1000 train_loss:5.7484 train_time:110288ms step_avg:241.86ms torch.cuda.memory_allocated()=369551872
14:52:31.285: step:467/1000 train_loss:5.5162 train_time:110540ms step_avg:241.88ms torch.cuda.memory_allocated()=369551872
14:52:31.538: step:468/1000 train_loss:5.4015 train_time:110793ms step_avg:241.91ms torch.cuda.memory_allocated()=369551872
14:52:31.788: step:469/1000 train_loss:5.7700 train_time:111043ms step_avg:241.92ms torch.cuda.memory_allocated()=369551872
14:52:32.041: step:470/1000 train_loss:5.3308 train_time:111297ms step_avg:241.95ms torch.cuda.memory_allocated()=369551872
14:52:32.292: step:471/1000 train_loss:5.4564 train_time:111547ms step_avg:241.97ms torch.cuda.memory_allocated()=369551872
14:52:32.542: step:472/1000 train_loss:5.6399 train_time:111797ms step_avg:241.98ms torch.cuda.memory_allocated()=369551872
14:52:32.791: step:473/1000 train_loss:5.4456 train_time:112047ms step_avg:242.00ms torch.cuda.memory_allocated()=369551872
14:52:33.045: step:474/1000 train_loss:5.7529 train_time:112300ms step_avg:242.03ms torch.cuda.memory_allocated()=369551872
14:52:33.301: step:475/1000 train_loss:5.7654 train_time:112556ms step_avg:242.06ms torch.cuda.memory_allocated()=369551872
14:52:33.554: step:476/1000 train_loss:5.7285 train_time:112809ms step_avg:242.08ms torch.cuda.memory_allocated()=369551872
14:52:33.804: step:477/1000 train_loss:5.3159 train_time:113059ms step_avg:242.10ms torch.cuda.memory_allocated()=369551872
14:52:34.058: step:478/1000 train_loss:5.5354 train_time:113314ms step_avg:242.12ms torch.cuda.memory_allocated()=369551872
14:52:34.309: step:479/1000 train_loss:5.9318 train_time:113564ms step_avg:242.14ms torch.cuda.memory_allocated()=369551872
14:52:34.559: step:480/1000 train_loss:5.3786 train_time:113815ms step_avg:242.16ms torch.cuda.memory_allocated()=369551872
14:52:34.810: step:481/1000 train_loss:5.9280 train_time:114065ms step_avg:242.18ms torch.cuda.memory_allocated()=369551872
14:52:35.064: step:482/1000 train_loss:5.5085 train_time:114319ms step_avg:242.20ms torch.cuda.memory_allocated()=369551872
14:52:35.314: step:483/1000 train_loss:5.3842 train_time:114569ms step_avg:242.22ms torch.cuda.memory_allocated()=369551872
14:52:35.567: step:484/1000 train_loss:5.5652 train_time:114822ms step_avg:242.24ms torch.cuda.memory_allocated()=369551872
14:52:35.817: step:485/1000 train_loss:5.6517 train_time:115072ms step_avg:242.26ms torch.cuda.memory_allocated()=369551872
14:52:36.073: step:486/1000 train_loss:5.5972 train_time:115328ms step_avg:242.29ms torch.cuda.memory_allocated()=369551872
14:52:36.325: step:487/1000 train_loss:5.6474 train_time:115580ms step_avg:242.31ms torch.cuda.memory_allocated()=369551872
14:52:36.580: step:488/1000 train_loss:5.4639 train_time:115835ms step_avg:242.33ms torch.cuda.memory_allocated()=369551872
14:52:36.829: step:489/1000 train_loss:5.4257 train_time:116084ms step_avg:242.35ms torch.cuda.memory_allocated()=369551872
14:52:37.084: step:490/1000 train_loss:5.6217 train_time:116339ms step_avg:242.37ms torch.cuda.memory_allocated()=369551872
14:52:37.334: step:491/1000 train_loss:5.3482 train_time:116589ms step_avg:242.39ms torch.cuda.memory_allocated()=369551872
14:52:37.591: step:492/1000 train_loss:5.3919 train_time:116846ms step_avg:242.42ms torch.cuda.memory_allocated()=369551872
14:52:37.842: step:493/1000 train_loss:5.2834 train_time:117098ms step_avg:242.44ms torch.cuda.memory_allocated()=369551872
14:52:38.096: step:494/1000 train_loss:6.2041 train_time:117351ms step_avg:242.46ms torch.cuda.memory_allocated()=369551872
14:52:38.354: step:495/1000 train_loss:5.1022 train_time:117609ms step_avg:242.49ms torch.cuda.memory_allocated()=369551872
14:52:38.604: step:496/1000 train_loss:5.3384 train_time:117859ms step_avg:242.51ms torch.cuda.memory_allocated()=369551872
14:52:38.855: step:497/1000 train_loss:5.3532 train_time:118110ms step_avg:242.53ms torch.cuda.memory_allocated()=369551872
14:52:39.105: step:498/1000 train_loss:5.5569 train_time:118360ms step_avg:242.54ms torch.cuda.memory_allocated()=369551872
14:52:39.362: step:499/1000 train_loss:5.5961 train_time:118617ms step_avg:242.57ms torch.cuda.memory_allocated()=369551872
14:52:39.613: step:500/1000 train_loss:5.2776 train_time:118868ms step_avg:242.59ms torch.cuda.memory_allocated()=369551872
14:52:42.323: step:500/1000 val_loss:5.7052 train_time:118868ms step_avg:242.59ms
14:52:42.583: step:501/1000 train_loss:5.9108 train_time:119128ms step_avg:242.62ms torch.cuda.memory_allocated()=369551872
14:52:42.838: step:502/1000 train_loss:5.5747 train_time:119383ms step_avg:242.65ms torch.cuda.memory_allocated()=369551872
14:52:43.089: step:503/1000 train_loss:5.5515 train_time:119635ms step_avg:242.67ms torch.cuda.memory_allocated()=369551872
14:52:43.341: step:504/1000 train_loss:5.6206 train_time:119887ms step_avg:242.69ms torch.cuda.memory_allocated()=369551872
14:52:43.597: step:505/1000 train_loss:5.7744 train_time:120142ms step_avg:242.71ms torch.cuda.memory_allocated()=369551872
14:52:43.851: step:506/1000 train_loss:5.7060 train_time:120396ms step_avg:242.73ms torch.cuda.memory_allocated()=369551872
14:52:44.114: step:507/1000 train_loss:5.7891 train_time:120659ms step_avg:242.77ms torch.cuda.memory_allocated()=369551872
14:52:44.369: step:508/1000 train_loss:5.6325 train_time:120914ms step_avg:242.80ms torch.cuda.memory_allocated()=369551872
14:52:44.620: step:509/1000 train_loss:5.4716 train_time:121165ms step_avg:242.82ms torch.cuda.memory_allocated()=369551872
14:52:44.878: step:510/1000 train_loss:5.5167 train_time:121423ms step_avg:242.85ms torch.cuda.memory_allocated()=369551872
14:52:45.132: step:511/1000 train_loss:5.7958 train_time:121677ms step_avg:242.87ms torch.cuda.memory_allocated()=369551872
14:52:45.387: step:512/1000 train_loss:5.6278 train_time:121932ms step_avg:242.89ms torch.cuda.memory_allocated()=369551872
14:52:45.644: step:513/1000 train_loss:5.5471 train_time:122189ms step_avg:242.92ms torch.cuda.memory_allocated()=369551872
14:52:45.895: step:514/1000 train_loss:5.8925 train_time:122440ms step_avg:242.94ms torch.cuda.memory_allocated()=369551872
14:52:46.146: step:515/1000 train_loss:5.7191 train_time:122691ms step_avg:242.95ms torch.cuda.memory_allocated()=369551872
14:52:46.402: step:516/1000 train_loss:5.6658 train_time:122947ms step_avg:242.98ms torch.cuda.memory_allocated()=369551872
14:52:46.652: step:517/1000 train_loss:5.4625 train_time:123197ms step_avg:242.99ms torch.cuda.memory_allocated()=369551872
14:52:46.901: step:518/1000 train_loss:5.4898 train_time:123446ms step_avg:243.00ms torch.cuda.memory_allocated()=369551872
14:52:47.153: step:519/1000 train_loss:5.6667 train_time:123698ms step_avg:243.02ms torch.cuda.memory_allocated()=369551872
14:52:47.410: step:520/1000 train_loss:5.8640 train_time:123955ms step_avg:243.05ms torch.cuda.memory_allocated()=369551872
14:52:47.666: step:521/1000 train_loss:4.9301 train_time:124211ms step_avg:243.07ms torch.cuda.memory_allocated()=369551872
14:52:47.923: step:522/1000 train_loss:5.4067 train_time:124468ms step_avg:243.10ms torch.cuda.memory_allocated()=369551872
14:52:48.181: step:523/1000 train_loss:5.8482 train_time:124726ms step_avg:243.13ms torch.cuda.memory_allocated()=369551872
14:52:48.439: step:524/1000 train_loss:5.8371 train_time:124984ms step_avg:243.16ms torch.cuda.memory_allocated()=369551872
14:52:48.691: step:525/1000 train_loss:5.3594 train_time:125236ms step_avg:243.18ms torch.cuda.memory_allocated()=369551872
14:52:48.942: step:526/1000 train_loss:5.4538 train_time:125487ms step_avg:243.19ms torch.cuda.memory_allocated()=369551872
14:52:49.195: step:527/1000 train_loss:5.7920 train_time:125740ms step_avg:243.21ms torch.cuda.memory_allocated()=369551872
14:52:49.456: step:528/1000 train_loss:5.7847 train_time:126001ms step_avg:243.25ms torch.cuda.memory_allocated()=369551872
14:52:49.709: step:529/1000 train_loss:5.2884 train_time:126254ms step_avg:243.26ms torch.cuda.memory_allocated()=369551872
14:52:49.959: step:530/1000 train_loss:5.5489 train_time:126504ms step_avg:243.28ms torch.cuda.memory_allocated()=369551872
14:52:50.213: step:531/1000 train_loss:5.3587 train_time:126758ms step_avg:243.30ms torch.cuda.memory_allocated()=369551872
14:52:50.471: step:532/1000 train_loss:5.6125 train_time:127016ms step_avg:243.33ms torch.cuda.memory_allocated()=369551872
14:52:50.727: step:533/1000 train_loss:6.5039 train_time:127272ms step_avg:243.35ms torch.cuda.memory_allocated()=369551872
14:52:50.977: step:534/1000 train_loss:5.6928 train_time:127523ms step_avg:243.36ms torch.cuda.memory_allocated()=369551872
14:52:51.235: step:535/1000 train_loss:5.9574 train_time:127780ms step_avg:243.39ms torch.cuda.memory_allocated()=369551872
14:52:51.487: step:536/1000 train_loss:5.8014 train_time:128032ms step_avg:243.41ms torch.cuda.memory_allocated()=369551872
14:52:51.749: step:537/1000 train_loss:5.3957 train_time:128294ms step_avg:243.44ms torch.cuda.memory_allocated()=369551872
14:52:51.003: step:538/1000 train_loss:5.3431 train_time:128548ms step_avg:243.46ms torch.cuda.memory_allocated()=369551872
14:52:52.260: step:539/1000 train_loss:6.1461 train_time:128805ms step_avg:243.49ms torch.cuda.memory_allocated()=369551872
14:52:52.521: step:540/1000 train_loss:5.6014 train_time:129066ms step_avg:243.52ms torch.cuda.memory_allocated()=369551872
14:52:52.775: step:541/1000 train_loss:5.4458 train_time:129321ms step_avg:243.54ms torch.cuda.memory_allocated()=369551872
14:52:53.030: step:542/1000 train_loss:5.7314 train_time:129576ms step_avg:243.56ms torch.cuda.memory_allocated()=369551872
14:52:53.286: step:543/1000 train_loss:6.1554 train_time:129831ms step_avg:243.59ms torch.cuda.memory_allocated()=369551872
14:52:53.542: step:544/1000 train_loss:5.7528 train_time:130087ms step_avg:243.61ms torch.cuda.memory_allocated()=369551872
14:52:53.795: step:545/1000 train_loss:5.6143 train_time:130340ms step_avg:243.63ms torch.cuda.memory_allocated()=369551872
14:52:54.049: step:546/1000 train_loss:5.7100 train_time:130594ms step_avg:243.65ms torch.cuda.memory_allocated()=369551872
14:52:54.307: step:547/1000 train_loss:5.5557 train_time:130852ms step_avg:243.67ms torch.cuda.memory_allocated()=369551872
14:52:54.561: step:548/1000 train_loss:5.4278 train_time:131107ms step_avg:243.69ms torch.cuda.memory_allocated()=369551872
14:52:54.819: step:549/1000 train_loss:5.6446 train_time:131364ms step_avg:243.72ms torch.cuda.memory_allocated()=369551872
14:52:55.075: step:550/1000 train_loss:5.6911 train_time:131620ms step_avg:243.74ms torch.cuda.memory_allocated()=369551872
14:52:55.328: step:551/1000 train_loss:5.8718 train_time:131874ms step_avg:243.76ms torch.cuda.memory_allocated()=369551872
14:52:55.582: step:552/1000 train_loss:5.5791 train_time:132128ms step_avg:243.78ms torch.cuda.memory_allocated()=369551872
14:52:55.847: step:553/1000 train_loss:7.1174 train_time:132392ms step_avg:243.82ms torch.cuda.memory_allocated()=369551872
14:52:56.111: step:554/1000 train_loss:5.3770 train_time:132657ms step_avg:243.85ms torch.cuda.memory_allocated()=369551872
14:52:56.366: step:555/1000 train_loss:5.8312 train_time:132912ms step_avg:243.87ms torch.cuda.memory_allocated()=369551872
14:52:56.623: step:556/1000 train_loss:6.8123 train_time:133168ms step_avg:243.90ms torch.cuda.memory_allocated()=369551872
14:52:56.876: step:557/1000 train_loss:5.7329 train_time:133422ms step_avg:243.92ms torch.cuda.memory_allocated()=369551872
14:52:57.136: step:558/1000 train_loss:5.8867 train_time:133681ms step_avg:243.94ms torch.cuda.memory_allocated()=369551872
14:52:57.396: step:559/1000 train_loss:6.3037 train_time:133942ms step_avg:243.97ms torch.cuda.memory_allocated()=369551872
14:52:57.648: step:560/1000 train_loss:5.7190 train_time:134194ms step_avg:243.99ms torch.cuda.memory_allocated()=369551872
14:52:57.902: step:561/1000 train_loss:5.8289 train_time:134447ms step_avg:244.01ms torch.cuda.memory_allocated()=369551872
14:52:58.159: step:562/1000 train_loss:6.0498 train_time:134705ms step_avg:244.03ms torch.cuda.memory_allocated()=369551872
14:52:58.424: step:563/1000 train_loss:6.3104 train_time:134970ms step_avg:244.07ms torch.cuda.memory_allocated()=369551872
14:52:58.689: step:564/1000 train_loss:6.2479 train_time:135235ms step_avg:244.11ms torch.cuda.memory_allocated()=369551872
14:52:58.944: step:565/1000 train_loss:5.9371 train_time:135489ms step_avg:244.12ms torch.cuda.memory_allocated()=369551872
14:52:59.199: step:566/1000 train_loss:5.8828 train_time:135745ms step_avg:244.15ms torch.cuda.memory_allocated()=369551872
14:52:59.454: step:567/1000 train_loss:5.6578 train_time:136000ms step_avg:244.16ms torch.cuda.memory_allocated()=369551872
14:52:59.710: step:568/1000 train_loss:5.4630 train_time:136256ms step_avg:244.19ms torch.cuda.memory_allocated()=369551872
14:52:59.964: step:569/1000 train_loss:6.1212 train_time:136509ms step_avg:244.20ms torch.cuda.memory_allocated()=369551872
14:53:00.217: step:570/1000 train_loss:5.6809 train_time:136763ms step_avg:244.22ms torch.cuda.memory_allocated()=369551872
14:53:00.472: step:571/1000 train_loss:5.6662 train_time:137017ms step_avg:244.24ms torch.cuda.memory_allocated()=369551872
14:53:00.985: step:572/1000 train_loss:5.6884 train_time:137531ms step_avg:244.72ms torch.cuda.memory_allocated()=369551872
14:53:01.241: step:573/1000 train_loss:6.2160 train_time:137786ms step_avg:244.74ms torch.cuda.memory_allocated()=369551872
14:53:01.492: step:574/1000 train_loss:5.6817 train_time:138037ms step_avg:244.75ms torch.cuda.memory_allocated()=369551872
14:53:01.743: step:575/1000 train_loss:6.0572 train_time:138288ms step_avg:244.76ms torch.cuda.memory_allocated()=369551872
14:53:01.997: step:576/1000 train_loss:6.0949 train_time:138542ms step_avg:244.77ms torch.cuda.memory_allocated()=369551872
14:53:02.249: step:577/1000 train_loss:5.6418 train_time:138794ms step_avg:244.79ms torch.cuda.memory_allocated()=369551872
14:53:02.502: step:578/1000 train_loss:6.0538 train_time:139047ms step_avg:244.80ms torch.cuda.memory_allocated()=369551872
14:53:02.750: step:579/1000 train_loss:5.7569 train_time:139295ms step_avg:244.81ms torch.cuda.memory_allocated()=369551872
14:53:03.008: step:580/1000 train_loss:6.3769 train_time:139553ms step_avg:244.83ms torch.cuda.memory_allocated()=369551872
14:53:03.267: step:581/1000 train_loss:6.3642 train_time:139812ms step_avg:244.86ms torch.cuda.memory_allocated()=369551872
14:53:03.520: step:582/1000 train_loss:6.4278 train_time:140065ms step_avg:244.87ms torch.cuda.memory_allocated()=369551872
14:53:03.778: step:583/1000 train_loss:5.5846 train_time:140323ms step_avg:244.89ms torch.cuda.memory_allocated()=369551872
14:53:04.031: step:584/1000 train_loss:5.8684 train_time:140576ms step_avg:244.91ms torch.cuda.memory_allocated()=369551872
14:53:04.281: step:585/1000 train_loss:6.2215 train_time:140827ms step_avg:244.92ms torch.cuda.memory_allocated()=369551872
14:53:04.546: step:586/1000 train_loss:6.0995 train_time:141091ms step_avg:244.95ms torch.cuda.memory_allocated()=369551872
14:53:04.802: step:587/1000 train_loss:5.9000 train_time:141348ms step_avg:244.97ms torch.cuda.memory_allocated()=369551872
14:53:05.056: step:588/1000 train_loss:5.8175 train_time:141601ms step_avg:244.98ms torch.cuda.memory_allocated()=369551872
14:53:05.312: step:589/1000 train_loss:6.2467 train_time:141857ms step_avg:245.00ms torch.cuda.memory_allocated()=369551872
14:53:05.576: step:590/1000 train_loss:7.1212 train_time:142121ms step_avg:245.04ms torch.cuda.memory_allocated()=369551872
14:53:05.827: step:591/1000 train_loss:5.9928 train_time:142372ms step_avg:245.05ms torch.cuda.memory_allocated()=369551872
14:53:06.075: step:592/1000 train_loss:5.8161 train_time:142620ms step_avg:245.05ms torch.cuda.memory_allocated()=369551872
14:53:06.330: step:593/1000 train_loss:5.4504 train_time:142876ms step_avg:245.07ms torch.cuda.memory_allocated()=369551872
14:53:06.591: step:594/1000 train_loss:6.4449 train_time:143136ms step_avg:245.10ms torch.cuda.memory_allocated()=369551872
14:53:06.848: step:595/1000 train_loss:6.0461 train_time:143393ms step_avg:245.12ms torch.cuda.memory_allocated()=369551872
14:53:07.109: step:596/1000 train_loss:5.6557 train_time:143654ms step_avg:245.14ms torch.cuda.memory_allocated()=369551872
14:53:07.374: step:597/1000 train_loss:5.9778 train_time:143919ms step_avg:245.18ms torch.cuda.memory_allocated()=369551872
14:53:07.642: step:598/1000 train_loss:5.8290 train_time:144187ms step_avg:245.22ms torch.cuda.memory_allocated()=369551872
14:53:07.895: step:599/1000 train_loss:5.7614 train_time:144441ms step_avg:245.23ms torch.cuda.memory_allocated()=369551872
14:53:08.161: step:600/1000 train_loss:6.5654 train_time:144706ms step_avg:245.26ms torch.cuda.memory_allocated()=369551872
14:53:08.414: step:601/1000 train_loss:5.4904 train_time:144959ms step_avg:245.28ms torch.cuda.memory_allocated()=369551872
14:53:08.672: step:602/1000 train_loss:5.6992 train_time:145217ms step_avg:245.30ms torch.cuda.memory_allocated()=369551872
14:53:08.924: step:603/1000 train_loss:5.9774 train_time:145469ms step_avg:245.31ms torch.cuda.memory_allocated()=369551872
14:53:09.177: step:604/1000 train_loss:5.9328 train_time:145723ms step_avg:245.32ms torch.cuda.memory_allocated()=369551872
14:53:09.435: step:605/1000 train_loss:5.7536 train_time:145980ms step_avg:245.34ms torch.cuda.memory_allocated()=369551872
14:53:09.694: step:606/1000 train_loss:5.9431 train_time:146239ms step_avg:245.37ms torch.cuda.memory_allocated()=369551872
14:53:09.947: step:607/1000 train_loss:5.9234 train_time:146492ms step_avg:245.38ms torch.cuda.memory_allocated()=369551872
14:53:10.210: step:608/1000 train_loss:6.0383 train_time:146756ms step_avg:245.41ms torch.cuda.memory_allocated()=369551872
14:53:10.465: step:609/1000 train_loss:5.6933 train_time:147010ms step_avg:245.43ms torch.cuda.memory_allocated()=369551872
14:53:10.723: step:610/1000 train_loss:5.7088 train_time:147268ms step_avg:245.45ms torch.cuda.memory_allocated()=369551872
14:53:10.984: step:611/1000 train_loss:5.1798 train_time:147529ms step_avg:245.47ms torch.cuda.memory_allocated()=369551872
14:53:11.241: step:612/1000 train_loss:5.4556 train_time:147786ms step_avg:245.49ms torch.cuda.memory_allocated()=369551872
14:53:11.499: step:613/1000 train_loss:5.5418 train_time:148044ms step_avg:245.51ms torch.cuda.memory_allocated()=369551872
14:53:11.757: step:614/1000 train_loss:5.6791 train_time:148302ms step_avg:245.53ms torch.cuda.memory_allocated()=369551872
14:53:12.015: step:615/1000 train_loss:5.5109 train_time:148560ms step_avg:245.55ms torch.cuda.memory_allocated()=369551872
14:53:12.266: step:616/1000 train_loss:5.7996 train_time:148811ms step_avg:245.56ms torch.cuda.memory_allocated()=369551872
14:53:12.524: step:617/1000 train_loss:5.6376 train_time:149069ms step_avg:245.58ms torch.cuda.memory_allocated()=369551872
14:53:12.776: step:618/1000 train_loss:5.9169 train_time:149321ms step_avg:245.59ms torch.cuda.memory_allocated()=369551872
14:53:13.034: step:619/1000 train_loss:5.7920 train_time:149579ms step_avg:245.61ms torch.cuda.memory_allocated()=369551872
14:53:13.288: step:620/1000 train_loss:5.6259 train_time:149833ms step_avg:245.63ms torch.cuda.memory_allocated()=369551872
14:53:13.542: step:621/1000 train_loss:5.7267 train_time:150087ms step_avg:245.64ms torch.cuda.memory_allocated()=369551872
14:53:13.801: step:622/1000 train_loss:5.6890 train_time:150346ms step_avg:245.66ms torch.cuda.memory_allocated()=369551872
14:53:14.058: step:623/1000 train_loss:5.6587 train_time:150604ms step_avg:245.68ms torch.cuda.memory_allocated()=369551872
14:53:14.310: step:624/1000 train_loss:5.8261 train_time:150855ms step_avg:245.69ms torch.cuda.memory_allocated()=369551872
14:53:14.563: step:625/1000 train_loss:5.7547 train_time:151108ms step_avg:245.70ms torch.cuda.memory_allocated()=369551872
14:53:17.293: step:625/1000 val_loss:5.8563 train_time:151109ms step_avg:245.70ms
14:53:17.546: step:626/1000 train_loss:5.8656 train_time:151361ms step_avg:245.72ms torch.cuda.memory_allocated()=369551872
14:53:17.802: step:627/1000 train_loss:5.8193 train_time:151617ms step_avg:245.73ms torch.cuda.memory_allocated()=369551872
14:53:18.063: step:628/1000 train_loss:6.0625 train_time:151879ms step_avg:245.76ms torch.cuda.memory_allocated()=369551872
14:53:18.323: step:629/1000 train_loss:5.5498 train_time:152138ms step_avg:245.78ms torch.cuda.memory_allocated()=369551872
14:53:18.582: step:630/1000 train_loss:5.8242 train_time:152398ms step_avg:245.80ms torch.cuda.memory_allocated()=369551872
14:53:18.840: step:631/1000 train_loss:6.0282 train_time:152655ms step_avg:245.82ms torch.cuda.memory_allocated()=369551872
14:53:19.100: step:632/1000 train_loss:5.5558 train_time:152916ms step_avg:245.84ms torch.cuda.memory_allocated()=369551872
14:53:19.359: step:633/1000 train_loss:5.6696 train_time:153174ms step_avg:245.87ms torch.cuda.memory_allocated()=369551872
14:53:19.622: step:634/1000 train_loss:5.5190 train_time:153438ms step_avg:245.89ms torch.cuda.memory_allocated()=369551872
14:53:19.875: step:635/1000 train_loss:5.9528 train_time:153690ms step_avg:245.90ms torch.cuda.memory_allocated()=369551872
14:53:20.137: step:636/1000 train_loss:6.3425 train_time:153952ms step_avg:245.93ms torch.cuda.memory_allocated()=369551872
14:53:20.390: step:637/1000 train_loss:5.8751 train_time:154205ms step_avg:245.94ms torch.cuda.memory_allocated()=369551872
14:53:20.644: step:638/1000 train_loss:5.8820 train_time:154460ms step_avg:245.95ms torch.cuda.memory_allocated()=369551872
14:53:20.895: step:639/1000 train_loss:5.7600 train_time:154711ms step_avg:245.96ms torch.cuda.memory_allocated()=369551872
14:53:21.145: step:640/1000 train_loss:5.5982 train_time:154960ms step_avg:245.97ms torch.cuda.memory_allocated()=369551872
14:53:21.401: step:641/1000 train_loss:5.5469 train_time:155216ms step_avg:245.98ms torch.cuda.memory_allocated()=369551872
14:53:21.659: step:642/1000 train_loss:5.9095 train_time:155474ms step_avg:246.00ms torch.cuda.memory_allocated()=369551872
14:53:21.918: step:643/1000 train_loss:6.9052 train_time:155734ms step_avg:246.02ms torch.cuda.memory_allocated()=369551872
14:53:22.178: step:644/1000 train_loss:5.8647 train_time:155993ms step_avg:246.05ms torch.cuda.memory_allocated()=369551872
14:53:22.433: step:645/1000 train_loss:5.9138 train_time:156248ms step_avg:246.06ms torch.cuda.memory_allocated()=369551872
14:53:22.693: step:646/1000 train_loss:5.7127 train_time:156508ms step_avg:246.08ms torch.cuda.memory_allocated()=369551872
14:53:22.955: step:647/1000 train_loss:5.7109 train_time:156771ms step_avg:246.11ms torch.cuda.memory_allocated()=369551872
14:53:23.212: step:648/1000 train_loss:5.7188 train_time:157027ms step_avg:246.12ms torch.cuda.memory_allocated()=369551872
14:53:23.465: step:649/1000 train_loss:5.8883 train_time:157281ms step_avg:246.14ms torch.cuda.memory_allocated()=369551872
14:53:23.720: step:650/1000 train_loss:5.6558 train_time:157535ms step_avg:246.15ms torch.cuda.memory_allocated()=369551872
14:53:23.981: step:651/1000 train_loss:5.5846 train_time:157796ms step_avg:246.17ms torch.cuda.memory_allocated()=369551872
14:53:24.242: step:652/1000 train_loss:6.0533 train_time:158057ms step_avg:246.19ms torch.cuda.memory_allocated()=369551872
14:53:24.501: step:653/1000 train_loss:5.4968 train_time:158316ms step_avg:246.21ms torch.cuda.memory_allocated()=369551872
14:53:24.763: step:654/1000 train_loss:5.6584 train_time:158579ms step_avg:246.24ms torch.cuda.memory_allocated()=369551872
14:53:25.020: step:655/1000 train_loss:5.5582 train_time:158836ms step_avg:246.26ms torch.cuda.memory_allocated()=369551872
14:53:25.273: step:656/1000 train_loss:5.8487 train_time:159088ms step_avg:246.27ms torch.cuda.memory_allocated()=369551872
14:53:25.525: step:657/1000 train_loss:5.7694 train_time:159340ms step_avg:246.28ms torch.cuda.memory_allocated()=369551872
14:53:25.782: step:658/1000 train_loss:5.6957 train_time:159597ms step_avg:246.29ms torch.cuda.memory_allocated()=369551872
14:53:26.041: step:659/1000 train_loss:6.2283 train_time:159857ms step_avg:246.31ms torch.cuda.memory_allocated()=369551872
14:53:26.302: step:660/1000 train_loss:5.9555 train_time:160117ms step_avg:246.33ms torch.cuda.memory_allocated()=369551872
14:53:26.556: step:661/1000 train_loss:5.7848 train_time:160371ms step_avg:246.35ms torch.cuda.memory_allocated()=369551872
14:53:26.814: step:662/1000 train_loss:5.6398 train_time:160629ms step_avg:246.36ms torch.cuda.memory_allocated()=369551872
14:53:27.069: step:663/1000 train_loss:5.7520 train_time:160884ms step_avg:246.38ms torch.cuda.memory_allocated()=369551872
14:53:27.325: step:664/1000 train_loss:5.7822 train_time:161141ms step_avg:246.39ms torch.cuda.memory_allocated()=369551872
14:53:27.577: step:665/1000 train_loss:5.9823 train_time:161393ms step_avg:246.40ms torch.cuda.memory_allocated()=369551872
14:53:27.832: step:666/1000 train_loss:6.2752 train_time:161648ms step_avg:246.41ms torch.cuda.memory_allocated()=369551872
14:53:28.087: step:667/1000 train_loss:5.6110 train_time:161903ms step_avg:246.43ms torch.cuda.memory_allocated()=369551872
14:53:28.342: step:668/1000 train_loss:6.0231 train_time:162158ms step_avg:246.44ms torch.cuda.memory_allocated()=369551872
14:53:28.604: step:669/1000 train_loss:6.0591 train_time:162419ms step_avg:246.46ms torch.cuda.memory_allocated()=369551872
14:53:28.858: step:670/1000 train_loss:5.9447 train_time:162673ms step_avg:246.47ms torch.cuda.memory_allocated()=369551872
14:53:29.115: step:671/1000 train_loss:5.9674 train_time:162930ms step_avg:246.49ms torch.cuda.memory_allocated()=369551872
14:53:29.366: step:672/1000 train_loss:5.7312 train_time:163182ms step_avg:246.50ms torch.cuda.memory_allocated()=369551872
14:53:29.625: step:673/1000 train_loss:6.0680 train_time:163441ms step_avg:246.52ms torch.cuda.memory_allocated()=369551872
14:53:29.897: step:674/1000 train_loss:6.6535 train_time:163712ms step_avg:246.55ms torch.cuda.memory_allocated()=369551872
14:53:30.154: step:675/1000 train_loss:5.7423 train_time:163970ms step_avg:246.57ms torch.cuda.memory_allocated()=369551872
14:53:30.413: step:676/1000 train_loss:5.7572 train_time:164229ms step_avg:246.59ms torch.cuda.memory_allocated()=369551872
14:53:30.681: step:677/1000 train_loss:5.9738 train_time:164497ms step_avg:246.62ms torch.cuda.memory_allocated()=369551872
14:53:30.957: step:678/1000 train_loss:6.7779 train_time:164772ms step_avg:246.67ms torch.cuda.memory_allocated()=369551872
14:53:31.209: step:679/1000 train_loss:5.9451 train_time:165024ms step_avg:246.67ms torch.cuda.memory_allocated()=369551872
14:53:31.486: step:680/1000 train_loss:6.9246 train_time:165301ms step_avg:246.72ms torch.cuda.memory_allocated()=369551872
14:53:31.763: step:681/1000 train_loss:6.7631 train_time:165579ms step_avg:246.76ms torch.cuda.memory_allocated()=369551872
14:53:32.034: step:682/1000 train_loss:6.6731 train_time:165849ms step_avg:246.80ms torch.cuda.memory_allocated()=369551872
14:53:32.291: step:683/1000 train_loss:5.7992 train_time:166106ms step_avg:246.81ms torch.cuda.memory_allocated()=369551872
14:53:32.554: step:684/1000 train_loss:6.4528 train_time:166369ms step_avg:246.84ms torch.cuda.memory_allocated()=369551872
14:53:32.815: step:685/1000 train_loss:6.2076 train_time:166631ms step_avg:246.86ms torch.cuda.memory_allocated()=369551872
14:53:33.085: step:686/1000 train_loss:5.7207 train_time:166901ms step_avg:246.89ms torch.cuda.memory_allocated()=369551872
14:53:33.343: step:687/1000 train_loss:5.7269 train_time:167158ms step_avg:246.91ms torch.cuda.memory_allocated()=369551872
14:53:33.606: step:688/1000 train_loss:6.4583 train_time:167422ms step_avg:246.93ms torch.cuda.memory_allocated()=369551872
14:53:33.860: step:689/1000 train_loss:5.6235 train_time:167676ms step_avg:246.94ms torch.cuda.memory_allocated()=369551872
14:53:34.116: step:690/1000 train_loss:5.6871 train_time:167932ms step_avg:246.96ms torch.cuda.memory_allocated()=369551872
14:53:34.377: step:691/1000 train_loss:6.0238 train_time:168193ms step_avg:246.98ms torch.cuda.memory_allocated()=369551872
14:53:34.635: step:692/1000 train_loss:5.5003 train_time:168451ms step_avg:247.00ms torch.cuda.memory_allocated()=369551872
14:53:34.900: step:693/1000 train_loss:5.6779 train_time:168715ms step_avg:247.02ms torch.cuda.memory_allocated()=369551872
14:53:35.170: step:694/1000 train_loss:5.8944 train_time:168986ms step_avg:247.06ms torch.cuda.memory_allocated()=369551872
14:53:35.423: step:695/1000 train_loss:5.8303 train_time:169238ms step_avg:247.06ms torch.cuda.memory_allocated()=369551872
14:53:35.684: step:696/1000 train_loss:5.8682 train_time:169500ms step_avg:247.08ms torch.cuda.memory_allocated()=369551872
14:53:35.942: step:697/1000 train_loss:5.9071 train_time:169757ms step_avg:247.10ms torch.cuda.memory_allocated()=369551872
14:53:36.195: step:698/1000 train_loss:5.7007 train_time:170011ms step_avg:247.11ms torch.cuda.memory_allocated()=369551872
14:53:36.452: step:699/1000 train_loss:5.9347 train_time:170267ms step_avg:247.12ms torch.cuda.memory_allocated()=369551872
14:53:36.723: step:700/1000 train_loss:6.9031 train_time:170538ms step_avg:247.16ms torch.cuda.memory_allocated()=369551872
14:53:36.986: step:701/1000 train_loss:5.5273 train_time:170802ms step_avg:247.18ms torch.cuda.memory_allocated()=369551872
14:53:37.249: step:702/1000 train_loss:5.4664 train_time:171064ms step_avg:247.20ms torch.cuda.memory_allocated()=369551872
14:53:37.504: step:703/1000 train_loss:6.5165 train_time:171319ms step_avg:247.21ms torch.cuda.memory_allocated()=369551872
14:53:37.763: step:704/1000 train_loss:6.2696 train_time:171578ms step_avg:247.23ms torch.cuda.memory_allocated()=369551872
14:53:38.020: step:705/1000 train_loss:5.9885 train_time:171835ms step_avg:247.24ms torch.cuda.memory_allocated()=369551872
14:53:38.270: step:706/1000 train_loss:5.7084 train_time:172085ms step_avg:247.25ms torch.cuda.memory_allocated()=369551872
14:53:38.527: step:707/1000 train_loss:6.0332 train_time:172342ms step_avg:247.26ms torch.cuda.memory_allocated()=369551872
14:53:38.786: step:708/1000 train_loss:5.6840 train_time:172602ms step_avg:247.28ms torch.cuda.memory_allocated()=369551872
14:53:39.045: step:709/1000 train_loss:6.1650 train_time:172860ms step_avg:247.30ms torch.cuda.memory_allocated()=369551872
14:53:39.302: step:710/1000 train_loss:5.8257 train_time:173117ms step_avg:247.31ms torch.cuda.memory_allocated()=369551872
14:53:39.573: step:711/1000 train_loss:6.4299 train_time:173389ms step_avg:247.34ms torch.cuda.memory_allocated()=369551872
14:53:39.828: step:712/1000 train_loss:5.8587 train_time:173643ms step_avg:247.35ms torch.cuda.memory_allocated()=369551872
14:53:40.088: step:713/1000 train_loss:6.1136 train_time:173904ms step_avg:247.37ms torch.cuda.memory_allocated()=369551872
14:53:40.349: step:714/1000 train_loss:5.7985 train_time:174165ms step_avg:247.39ms torch.cuda.memory_allocated()=369551872
14:53:40.619: step:715/1000 train_loss:5.6368 train_time:174435ms step_avg:247.42ms torch.cuda.memory_allocated()=369551872
14:53:40.879: step:716/1000 train_loss:5.7793 train_time:174694ms step_avg:247.44ms torch.cuda.memory_allocated()=369551872
14:53:41.141: step:717/1000 train_loss:5.6524 train_time:174956ms step_avg:247.46ms torch.cuda.memory_allocated()=369551872
14:53:41.400: step:718/1000 train_loss:5.9401 train_time:175215ms step_avg:247.48ms torch.cuda.memory_allocated()=369551872
14:53:41.665: step:719/1000 train_loss:5.8994 train_time:175480ms step_avg:247.50ms torch.cuda.memory_allocated()=369551872
14:53:41.924: step:720/1000 train_loss:5.6609 train_time:175740ms step_avg:247.52ms torch.cuda.memory_allocated()=369551872
14:53:42.182: step:721/1000 train_loss:5.9276 train_time:175997ms step_avg:247.53ms torch.cuda.memory_allocated()=369551872
14:53:42.438: step:722/1000 train_loss:5.9555 train_time:176253ms step_avg:247.55ms torch.cuda.memory_allocated()=369551872
14:53:42.698: step:723/1000 train_loss:5.5457 train_time:176513ms step_avg:247.56ms torch.cuda.memory_allocated()=369551872
14:53:42.954: step:724/1000 train_loss:5.6073 train_time:176769ms step_avg:247.58ms torch.cuda.memory_allocated()=369551872
14:53:43.222: step:725/1000 train_loss:5.8942 train_time:177038ms step_avg:247.61ms torch.cuda.memory_allocated()=369551872
14:53:43.480: step:726/1000 train_loss:5.8295 train_time:177295ms step_avg:247.62ms torch.cuda.memory_allocated()=369551872
14:53:43.734: step:727/1000 train_loss:5.7869 train_time:177549ms step_avg:247.63ms torch.cuda.memory_allocated()=369551872
14:53:43.988: step:728/1000 train_loss:5.8613 train_time:177803ms step_avg:247.64ms torch.cuda.memory_allocated()=369551872
14:53:44.245: step:729/1000 train_loss:5.6100 train_time:178060ms step_avg:247.65ms torch.cuda.memory_allocated()=369551872
14:53:44.505: step:730/1000 train_loss:5.7213 train_time:178320ms step_avg:247.67ms torch.cuda.memory_allocated()=369551872
14:53:44.762: step:731/1000 train_loss:7.0091 train_time:178577ms step_avg:247.68ms torch.cuda.memory_allocated()=369551872
14:53:45.016: step:732/1000 train_loss:5.6279 train_time:178831ms step_avg:247.69ms torch.cuda.memory_allocated()=369551872
14:53:45.280: step:733/1000 train_loss:5.1020 train_time:179095ms step_avg:247.71ms torch.cuda.memory_allocated()=369551872
14:53:45.541: step:734/1000 train_loss:5.7151 train_time:179356ms step_avg:247.73ms torch.cuda.memory_allocated()=369551872
14:53:45.798: step:735/1000 train_loss:5.7226 train_time:179613ms step_avg:247.74ms torch.cuda.memory_allocated()=369551872
14:53:46.069: step:736/1000 train_loss:5.7695 train_time:179885ms step_avg:247.77ms torch.cuda.memory_allocated()=369551872
14:53:46.334: step:737/1000 train_loss:6.6508 train_time:180149ms step_avg:247.80ms torch.cuda.memory_allocated()=369551872
14:53:46.595: step:738/1000 train_loss:6.0326 train_time:180410ms step_avg:247.82ms torch.cuda.memory_allocated()=369551872
14:53:46.853: step:739/1000 train_loss:5.6764 train_time:180668ms step_avg:247.83ms torch.cuda.memory_allocated()=369551872
14:53:47.114: step:740/1000 train_loss:5.6298 train_time:180929ms step_avg:247.85ms torch.cuda.memory_allocated()=369551872
14:53:47.373: step:741/1000 train_loss:5.6815 train_time:181188ms step_avg:247.86ms torch.cuda.memory_allocated()=369551872
14:53:47.636: step:742/1000 train_loss:6.4060 train_time:181451ms step_avg:247.88ms torch.cuda.memory_allocated()=369551872
14:53:47.899: step:743/1000 train_loss:5.7777 train_time:181714ms step_avg:247.90ms torch.cuda.memory_allocated()=369551872
14:53:48.161: step:744/1000 train_loss:5.6407 train_time:181976ms step_avg:247.92ms torch.cuda.memory_allocated()=369551872
14:53:48.422: step:745/1000 train_loss:5.6279 train_time:182237ms step_avg:247.94ms torch.cuda.memory_allocated()=369551872
14:53:48.681: step:746/1000 train_loss:5.8475 train_time:182496ms step_avg:247.96ms torch.cuda.memory_allocated()=369551872
14:53:48.940: step:747/1000 train_loss:5.9860 train_time:182755ms step_avg:247.97ms torch.cuda.memory_allocated()=369551872
14:53:49.209: step:748/1000 train_loss:5.6657 train_time:183024ms step_avg:248.00ms torch.cuda.memory_allocated()=369551872
14:53:49.475: step:749/1000 train_loss:5.7228 train_time:183290ms step_avg:248.03ms torch.cuda.memory_allocated()=369551872
14:53:49.743: step:750/1000 train_loss:5.8247 train_time:183558ms step_avg:248.05ms torch.cuda.memory_allocated()=369551872
14:53:52.500: step:750/1000 val_loss:5.8744 train_time:183558ms step_avg:248.05ms
14:53:52.756: step:751/1000 train_loss:5.7297 train_time:183814ms step_avg:248.06ms torch.cuda.memory_allocated()=369551872
14:53:53.024: step:752/1000 train_loss:5.6669 train_time:184082ms step_avg:248.09ms torch.cuda.memory_allocated()=369551872
14:53:53.293: step:753/1000 train_loss:5.5710 train_time:184351ms step_avg:248.12ms torch.cuda.memory_allocated()=369551872
14:53:53.548: step:754/1000 train_loss:6.0185 train_time:184606ms step_avg:248.13ms torch.cuda.memory_allocated()=369551872
14:53:53.801: step:755/1000 train_loss:5.7284 train_time:184859ms step_avg:248.13ms torch.cuda.memory_allocated()=369551872
14:53:54.058: step:756/1000 train_loss:6.0613 train_time:185116ms step_avg:248.14ms torch.cuda.memory_allocated()=369551872
14:53:54.315: step:757/1000 train_loss:6.2523 train_time:185373ms step_avg:248.16ms torch.cuda.memory_allocated()=369551872
14:53:54.572: step:758/1000 train_loss:6.0693 train_time:185629ms step_avg:248.17ms torch.cuda.memory_allocated()=369551872
14:53:54.828: step:759/1000 train_loss:5.4446 train_time:185886ms step_avg:248.18ms torch.cuda.memory_allocated()=369551872
14:53:55.087: step:760/1000 train_loss:5.7990 train_time:186145ms step_avg:248.19ms torch.cuda.memory_allocated()=369551872
14:53:55.344: step:761/1000 train_loss:5.6469 train_time:186402ms step_avg:248.20ms torch.cuda.memory_allocated()=369551872
14:53:55.601: step:762/1000 train_loss:5.5810 train_time:186658ms step_avg:248.22ms torch.cuda.memory_allocated()=369551872
14:53:55.863: step:763/1000 train_loss:5.5938 train_time:186920ms step_avg:248.23ms torch.cuda.memory_allocated()=369551872
14:53:56.121: step:764/1000 train_loss:5.6725 train_time:187179ms step_avg:248.25ms torch.cuda.memory_allocated()=369551872
14:53:56.380: step:765/1000 train_loss:5.6822 train_time:187438ms step_avg:248.26ms torch.cuda.memory_allocated()=369551872
14:53:56.633: step:766/1000 train_loss:6.0531 train_time:187691ms step_avg:248.27ms torch.cuda.memory_allocated()=369551872
14:53:56.894: step:767/1000 train_loss:5.7820 train_time:187952ms step_avg:248.28ms torch.cuda.memory_allocated()=369551872
14:53:57.156: step:768/1000 train_loss:6.1754 train_time:188214ms step_avg:248.30ms torch.cuda.memory_allocated()=369551872
14:53:57.414: step:769/1000 train_loss:5.6071 train_time:188472ms step_avg:248.32ms torch.cuda.memory_allocated()=369551872
14:53:57.668: step:770/1000 train_loss:5.9293 train_time:188726ms step_avg:248.32ms torch.cuda.memory_allocated()=369551872
14:53:57.924: step:771/1000 train_loss:6.0304 train_time:188982ms step_avg:248.33ms torch.cuda.memory_allocated()=369551872
14:53:58.195: step:772/1000 train_loss:6.3199 train_time:189253ms step_avg:248.36ms torch.cuda.memory_allocated()=369551872
14:53:58.446: step:773/1000 train_loss:6.0295 train_time:189504ms step_avg:248.37ms torch.cuda.memory_allocated()=369551872
14:53:58.715: step:774/1000 train_loss:5.8045 train_time:189773ms step_avg:248.39ms torch.cuda.memory_allocated()=369551872
14:53:58.973: step:775/1000 train_loss:6.0618 train_time:190031ms step_avg:248.41ms torch.cuda.memory_allocated()=369551872
14:53:59.231: step:776/1000 train_loss:5.7301 train_time:190289ms step_avg:248.42ms torch.cuda.memory_allocated()=369551872
14:53:59.488: step:777/1000 train_loss:5.5736 train_time:190546ms step_avg:248.43ms torch.cuda.memory_allocated()=369551872
14:53:59.742: step:778/1000 train_loss:5.7770 train_time:190800ms step_avg:248.44ms torch.cuda.memory_allocated()=369551872
14:54:00.012: step:779/1000 train_loss:5.2693 train_time:191070ms step_avg:248.47ms torch.cuda.memory_allocated()=369551872
14:54:00.268: step:780/1000 train_loss:5.6943 train_time:191326ms step_avg:248.48ms torch.cuda.memory_allocated()=369551872
14:54:00.521: step:781/1000 train_loss:5.9933 train_time:191579ms step_avg:248.48ms torch.cuda.memory_allocated()=369551872
14:54:00.773: step:782/1000 train_loss:5.6796 train_time:191831ms step_avg:248.49ms torch.cuda.memory_allocated()=369551872
14:54:01.037: step:783/1000 train_loss:6.0116 train_time:192095ms step_avg:248.51ms torch.cuda.memory_allocated()=369551872
14:54:01.303: step:784/1000 train_loss:5.5718 train_time:192361ms step_avg:248.53ms torch.cuda.memory_allocated()=369551872
14:54:01.575: step:785/1000 train_loss:6.1405 train_time:192633ms step_avg:248.56ms torch.cuda.memory_allocated()=369551872
14:54:01.831: step:786/1000 train_loss:5.5793 train_time:192889ms step_avg:248.57ms torch.cuda.memory_allocated()=369551872
14:54:02.104: step:787/1000 train_loss:5.4661 train_time:193162ms step_avg:248.60ms torch.cuda.memory_allocated()=369551872
14:54:02.369: step:788/1000 train_loss:5.5743 train_time:193427ms step_avg:248.62ms torch.cuda.memory_allocated()=369551872
14:54:02.627: step:789/1000 train_loss:5.5447 train_time:193685ms step_avg:248.63ms torch.cuda.memory_allocated()=369551872
14:54:02.889: step:790/1000 train_loss:5.6222 train_time:193947ms step_avg:248.65ms torch.cuda.memory_allocated()=369551872
14:54:03.149: step:791/1000 train_loss:5.8626 train_time:194206ms step_avg:248.66ms torch.cuda.memory_allocated()=369551872
14:54:03.403: step:792/1000 train_loss:5.7390 train_time:194460ms step_avg:248.67ms torch.cuda.memory_allocated()=369551872
14:54:03.661: step:793/1000 train_loss:5.8812 train_time:194719ms step_avg:248.68ms torch.cuda.memory_allocated()=369551872
14:54:03.918: step:794/1000 train_loss:5.6991 train_time:194976ms step_avg:248.69ms torch.cuda.memory_allocated()=369551872
14:54:04.173: step:795/1000 train_loss:5.9391 train_time:195231ms step_avg:248.70ms torch.cuda.memory_allocated()=369551872
14:54:04.435: step:796/1000 train_loss:5.5814 train_time:195493ms step_avg:248.72ms torch.cuda.memory_allocated()=369551872
14:54:04.700: step:797/1000 train_loss:6.1645 train_time:195758ms step_avg:248.74ms torch.cuda.memory_allocated()=369551872
14:54:04.957: step:798/1000 train_loss:5.5830 train_time:196015ms step_avg:248.75ms torch.cuda.memory_allocated()=369551872
14:54:05.218: step:799/1000 train_loss:5.8157 train_time:196276ms step_avg:248.77ms torch.cuda.memory_allocated()=369551872
14:54:05.473: step:800/1000 train_loss:5.8929 train_time:196531ms step_avg:248.77ms torch.cuda.memory_allocated()=369551872
14:54:05.730: step:801/1000 train_loss:5.8700 train_time:196788ms step_avg:248.78ms torch.cuda.memory_allocated()=369551872
14:54:05.996: step:802/1000 train_loss:5.2002 train_time:197054ms step_avg:248.81ms torch.cuda.memory_allocated()=369551872
14:54:06.258: step:803/1000 train_loss:5.3951 train_time:197316ms step_avg:248.82ms torch.cuda.memory_allocated()=369551872
14:54:06.540: step:804/1000 train_loss:6.1657 train_time:197598ms step_avg:248.86ms torch.cuda.memory_allocated()=369551872
14:54:06.812: step:805/1000 train_loss:6.9029 train_time:197870ms step_avg:248.89ms torch.cuda.memory_allocated()=369551872
14:54:07.074: step:806/1000 train_loss:5.9298 train_time:198132ms step_avg:248.91ms torch.cuda.memory_allocated()=369551872
14:54:07.333: step:807/1000 train_loss:5.8725 train_time:198391ms step_avg:248.92ms torch.cuda.memory_allocated()=369551872
14:54:07.593: step:808/1000 train_loss:5.6194 train_time:198651ms step_avg:248.94ms torch.cuda.memory_allocated()=369551872
14:54:07.848: step:809/1000 train_loss:5.4894 train_time:198906ms step_avg:248.94ms torch.cuda.memory_allocated()=369551872
14:54:08.114: step:810/1000 train_loss:5.3760 train_time:199172ms step_avg:248.97ms torch.cuda.memory_allocated()=369551872
14:54:08.369: step:811/1000 train_loss:5.8826 train_time:199427ms step_avg:248.97ms torch.cuda.memory_allocated()=369551872
14:54:08.627: step:812/1000 train_loss:5.7873 train_time:199685ms step_avg:248.98ms torch.cuda.memory_allocated()=369551872
14:54:08.880: step:813/1000 train_loss:5.6651 train_time:199938ms step_avg:248.99ms torch.cuda.memory_allocated()=369551872
14:54:09.133: step:814/1000 train_loss:5.8263 train_time:200191ms step_avg:248.99ms torch.cuda.memory_allocated()=369551872
14:54:09.390: step:815/1000 train_loss:5.7656 train_time:200448ms step_avg:249.00ms torch.cuda.memory_allocated()=369551872
14:54:09.646: step:816/1000 train_loss:5.5080 train_time:200703ms step_avg:249.01ms torch.cuda.memory_allocated()=369551872
14:54:09.909: step:817/1000 train_loss:5.6054 train_time:200967ms step_avg:249.03ms torch.cuda.memory_allocated()=369551872
14:54:10.168: step:818/1000 train_loss:5.4705 train_time:201225ms step_avg:249.04ms torch.cuda.memory_allocated()=369551872
14:54:10.420: step:819/1000 train_loss:5.7559 train_time:201478ms step_avg:249.05ms torch.cuda.memory_allocated()=369551872
14:54:10.687: step:820/1000 train_loss:5.8098 train_time:201744ms step_avg:249.07ms torch.cuda.memory_allocated()=369551872
14:54:10.940: step:821/1000 train_loss:5.5077 train_time:201998ms step_avg:249.07ms torch.cuda.memory_allocated()=369551872
14:54:11.200: step:822/1000 train_loss:5.5692 train_time:202258ms step_avg:249.09ms torch.cuda.memory_allocated()=369551872
14:54:11.463: step:823/1000 train_loss:5.4581 train_time:202521ms step_avg:249.10ms torch.cuda.memory_allocated()=369551872
14:54:11.733: step:824/1000 train_loss:5.4732 train_time:202791ms step_avg:249.13ms torch.cuda.memory_allocated()=369551872
14:54:11.996: step:825/1000 train_loss:5.7086 train_time:203053ms step_avg:249.15ms torch.cuda.memory_allocated()=369551872
14:54:12.253: step:826/1000 train_loss:5.5880 train_time:203311ms step_avg:249.16ms torch.cuda.memory_allocated()=369551872
14:54:12.512: step:827/1000 train_loss:5.4862 train_time:203570ms step_avg:249.17ms torch.cuda.memory_allocated()=369551872
14:54:12.780: step:828/1000 train_loss:5.4647 train_time:203838ms step_avg:249.19ms torch.cuda.memory_allocated()=369551872
14:54:13.058: step:829/1000 train_loss:5.3092 train_time:204116ms step_avg:249.23ms torch.cuda.memory_allocated()=369551872
14:54:13.321: step:830/1000 train_loss:5.7281 train_time:204379ms step_avg:249.24ms torch.cuda.memory_allocated()=369551872
14:54:13.583: step:831/1000 train_loss:5.8303 train_time:204641ms step_avg:249.26ms torch.cuda.memory_allocated()=369551872
14:54:13.862: step:832/1000 train_loss:6.3195 train_time:204920ms step_avg:249.29ms torch.cuda.memory_allocated()=369551872
14:54:14.143: step:833/1000 train_loss:6.0827 train_time:205201ms step_avg:249.33ms torch.cuda.memory_allocated()=369551872
14:54:14.404: step:834/1000 train_loss:5.4991 train_time:205462ms step_avg:249.35ms torch.cuda.memory_allocated()=369551872
14:54:14.667: step:835/1000 train_loss:5.5585 train_time:205725ms step_avg:249.36ms torch.cuda.memory_allocated()=369551872
14:54:14.939: step:836/1000 train_loss:5.6010 train_time:205997ms step_avg:249.39ms torch.cuda.memory_allocated()=369551872
14:54:15.205: step:837/1000 train_loss:5.6678 train_time:206263ms step_avg:249.41ms torch.cuda.memory_allocated()=369551872
14:54:15.464: step:838/1000 train_loss:5.4534 train_time:206521ms step_avg:249.42ms torch.cuda.memory_allocated()=369551872
14:54:15.721: step:839/1000 train_loss:5.5605 train_time:206779ms step_avg:249.43ms torch.cuda.memory_allocated()=369551872
14:54:15.982: step:840/1000 train_loss:5.3785 train_time:207040ms step_avg:249.45ms torch.cuda.memory_allocated()=369551872
14:54:16.242: step:841/1000 train_loss:5.6081 train_time:207300ms step_avg:249.46ms torch.cuda.memory_allocated()=369551872
14:54:16.506: step:842/1000 train_loss:5.6411 train_time:207564ms step_avg:249.48ms torch.cuda.memory_allocated()=369551872
14:54:16.770: step:843/1000 train_loss:5.4126 train_time:207828ms step_avg:249.49ms torch.cuda.memory_allocated()=369551872
14:54:17.029: step:844/1000 train_loss:5.6107 train_time:208087ms step_avg:249.51ms torch.cuda.memory_allocated()=369551872
14:54:17.287: step:845/1000 train_loss:5.6892 train_time:208345ms step_avg:249.51ms torch.cuda.memory_allocated()=369551872
14:54:17.543: step:846/1000 train_loss:5.7409 train_time:208601ms step_avg:249.52ms torch.cuda.memory_allocated()=369551872
14:54:17.803: step:847/1000 train_loss:5.7512 train_time:208861ms step_avg:249.53ms torch.cuda.memory_allocated()=369551872
14:54:18.058: step:848/1000 train_loss:5.7320 train_time:209116ms step_avg:249.54ms torch.cuda.memory_allocated()=369551872
14:54:18.309: step:849/1000 train_loss:5.5329 train_time:209367ms step_avg:249.54ms torch.cuda.memory_allocated()=369551872
14:54:18.567: step:850/1000 train_loss:5.7169 train_time:209624ms step_avg:249.55ms torch.cuda.memory_allocated()=369551872
14:54:18.835: step:851/1000 train_loss:5.4156 train_time:209893ms step_avg:249.58ms torch.cuda.memory_allocated()=369551872
14:54:19.093: step:852/1000 train_loss:5.4914 train_time:210151ms step_avg:249.59ms torch.cuda.memory_allocated()=369551872
14:54:19.353: step:853/1000 train_loss:5.2114 train_time:210410ms step_avg:249.60ms torch.cuda.memory_allocated()=369551872
14:54:19.611: step:854/1000 train_loss:5.8026 train_time:210668ms step_avg:249.61ms torch.cuda.memory_allocated()=369551872
14:54:19.860: step:855/1000 train_loss:5.4257 train_time:210918ms step_avg:249.61ms torch.cuda.memory_allocated()=369551872
14:54:20.115: step:856/1000 train_loss:5.5034 train_time:211173ms step_avg:249.61ms torch.cuda.memory_allocated()=369551872
14:54:20.369: step:857/1000 train_loss:5.6944 train_time:211427ms step_avg:249.62ms torch.cuda.memory_allocated()=369551872
14:54:20.620: step:858/1000 train_loss:5.4498 train_time:211678ms step_avg:249.62ms torch.cuda.memory_allocated()=369551872
14:54:20.876: step:859/1000 train_loss:5.4558 train_time:211934ms step_avg:249.63ms torch.cuda.memory_allocated()=369551872
14:54:21.133: step:860/1000 train_loss:5.8874 train_time:212191ms step_avg:249.64ms torch.cuda.memory_allocated()=369551872
14:54:21.388: step:861/1000 train_loss:5.6607 train_time:212446ms step_avg:249.64ms torch.cuda.memory_allocated()=369551872
14:54:21.643: step:862/1000 train_loss:5.3020 train_time:212701ms step_avg:249.65ms torch.cuda.memory_allocated()=369551872
14:54:21.907: step:863/1000 train_loss:5.4379 train_time:212965ms step_avg:249.67ms torch.cuda.memory_allocated()=369551872
14:54:22.164: step:864/1000 train_loss:5.4507 train_time:213222ms step_avg:249.67ms torch.cuda.memory_allocated()=369551872
14:54:22.426: step:865/1000 train_loss:5.2983 train_time:213484ms step_avg:249.69ms torch.cuda.memory_allocated()=369551872
14:54:22.697: step:866/1000 train_loss:4.9120 train_time:213755ms step_avg:249.71ms torch.cuda.memory_allocated()=369551872
14:54:22.949: step:867/1000 train_loss:5.4922 train_time:214007ms step_avg:249.72ms torch.cuda.memory_allocated()=369551872
14:54:23.204: step:868/1000 train_loss:5.2722 train_time:214262ms step_avg:249.72ms torch.cuda.memory_allocated()=369551872
14:54:23.463: step:869/1000 train_loss:5.4496 train_time:214521ms step_avg:249.73ms torch.cuda.memory_allocated()=369551872
14:54:23.716: step:870/1000 train_loss:5.5116 train_time:214773ms step_avg:249.74ms torch.cuda.memory_allocated()=369551872
14:54:23.974: step:871/1000 train_loss:5.4518 train_time:215032ms step_avg:249.75ms torch.cuda.memory_allocated()=369551872
14:54:24.240: step:872/1000 train_loss:5.3624 train_time:215298ms step_avg:249.77ms torch.cuda.memory_allocated()=369551872
14:54:24.499: step:873/1000 train_loss:5.3044 train_time:215557ms step_avg:249.78ms torch.cuda.memory_allocated()=369551872
14:54:24.764: step:874/1000 train_loss:5.3442 train_time:215821ms step_avg:249.79ms torch.cuda.memory_allocated()=369551872
14:54:25.028: step:875/1000 train_loss:5.1592 train_time:216086ms step_avg:249.81ms torch.cuda.memory_allocated()=369551872
14:54:27.794: step:875/1000 val_loss:5.5786 train_time:216086ms step_avg:249.81ms
14:54:28.049: step:876/1000 train_loss:5.4519 train_time:216341ms step_avg:249.82ms torch.cuda.memory_allocated()=369551872
14:54:28.305: step:877/1000 train_loss:5.4179 train_time:216597ms step_avg:249.82ms torch.cuda.memory_allocated()=369551872
14:54:28.584: step:878/1000 train_loss:4.9570 train_time:216876ms step_avg:249.86ms torch.cuda.memory_allocated()=369551872
14:54:28.845: step:879/1000 train_loss:5.3154 train_time:217137ms step_avg:249.87ms torch.cuda.memory_allocated()=369551872
14:54:29.109: step:880/1000 train_loss:5.3821 train_time:217400ms step_avg:249.89ms torch.cuda.memory_allocated()=369551872
14:54:29.367: step:881/1000 train_loss:5.3910 train_time:217659ms step_avg:249.89ms torch.cuda.memory_allocated()=369551872
14:54:29.633: step:882/1000 train_loss:5.6126 train_time:217925ms step_avg:249.91ms torch.cuda.memory_allocated()=369551872
14:54:29.886: step:883/1000 train_loss:5.3998 train_time:218178ms step_avg:249.92ms torch.cuda.memory_allocated()=369551872
14:54:30.143: step:884/1000 train_loss:5.3732 train_time:218434ms step_avg:249.92ms torch.cuda.memory_allocated()=369551872
14:54:30.403: step:885/1000 train_loss:5.3860 train_time:218695ms step_avg:249.94ms torch.cuda.memory_allocated()=369551872
14:54:30.657: step:886/1000 train_loss:5.3317 train_time:218949ms step_avg:249.94ms torch.cuda.memory_allocated()=369551872
14:54:30.916: step:887/1000 train_loss:5.6910 train_time:219208ms step_avg:249.95ms torch.cuda.memory_allocated()=369551872
14:54:31.175: step:888/1000 train_loss:5.5532 train_time:219466ms step_avg:249.96ms torch.cuda.memory_allocated()=369551872
14:54:31.436: step:889/1000 train_loss:5.5066 train_time:219728ms step_avg:249.98ms torch.cuda.memory_allocated()=369551872
14:54:31.702: step:890/1000 train_loss:5.4172 train_time:219993ms step_avg:249.99ms torch.cuda.memory_allocated()=369551872
14:54:31.966: step:891/1000 train_loss:5.3287 train_time:220258ms step_avg:250.01ms torch.cuda.memory_allocated()=369551872
14:54:32.223: step:892/1000 train_loss:5.3865 train_time:220515ms step_avg:250.02ms torch.cuda.memory_allocated()=369551872
14:54:32.505: step:893/1000 train_loss:5.4383 train_time:220796ms step_avg:250.05ms torch.cuda.memory_allocated()=369551872
14:54:32.759: step:894/1000 train_loss:5.4752 train_time:221051ms step_avg:250.06ms torch.cuda.memory_allocated()=369551872
14:54:33.018: step:895/1000 train_loss:5.4468 train_time:221310ms step_avg:250.07ms torch.cuda.memory_allocated()=369551872
14:54:33.280: step:896/1000 train_loss:5.1994 train_time:221572ms step_avg:250.08ms torch.cuda.memory_allocated()=369551872
14:54:33.543: step:897/1000 train_loss:5.4709 train_time:221834ms step_avg:250.09ms torch.cuda.memory_allocated()=369551872
14:54:33.796: step:898/1000 train_loss:5.5061 train_time:222088ms step_avg:250.10ms torch.cuda.memory_allocated()=369551872
14:54:34.048: step:899/1000 train_loss:5.6259 train_time:222340ms step_avg:250.10ms torch.cuda.memory_allocated()=369551872
14:54:34.314: step:900/1000 train_loss:5.4570 train_time:222606ms step_avg:250.12ms torch.cuda.memory_allocated()=369551872
14:54:34.585: step:901/1000 train_loss:5.5205 train_time:222876ms step_avg:250.14ms torch.cuda.memory_allocated()=369551872
14:54:34.847: step:902/1000 train_loss:5.1378 train_time:223139ms step_avg:250.16ms torch.cuda.memory_allocated()=369551872
14:54:35.107: step:903/1000 train_loss:5.4207 train_time:223399ms step_avg:250.17ms torch.cuda.memory_allocated()=369551872
14:54:35.359: step:904/1000 train_loss:5.4441 train_time:223651ms step_avg:250.17ms torch.cuda.memory_allocated()=369551872
14:54:35.614: step:905/1000 train_loss:5.4451 train_time:223906ms step_avg:250.17ms torch.cuda.memory_allocated()=369551872
14:54:35.869: step:906/1000 train_loss:5.2957 train_time:224161ms step_avg:250.18ms torch.cuda.memory_allocated()=369551872
14:54:36.130: step:907/1000 train_loss:5.4376 train_time:224422ms step_avg:250.19ms torch.cuda.memory_allocated()=369551872
14:54:36.395: step:908/1000 train_loss:5.5296 train_time:224687ms step_avg:250.21ms torch.cuda.memory_allocated()=369551872
14:54:36.655: step:909/1000 train_loss:5.0951 train_time:224947ms step_avg:250.22ms torch.cuda.memory_allocated()=369551872
14:54:36.913: step:910/1000 train_loss:5.5591 train_time:225204ms step_avg:250.23ms torch.cuda.memory_allocated()=369551872
14:54:37.171: step:911/1000 train_loss:5.7435 train_time:225462ms step_avg:250.24ms torch.cuda.memory_allocated()=369551872
14:54:37.454: step:912/1000 train_loss:5.5566 train_time:225746ms step_avg:250.27ms torch.cuda.memory_allocated()=369551872
14:54:37.731: step:913/1000 train_loss:5.9417 train_time:226022ms step_avg:250.30ms torch.cuda.memory_allocated()=369551872
14:54:37.986: step:914/1000 train_loss:5.3856 train_time:226277ms step_avg:250.31ms torch.cuda.memory_allocated()=369551872
14:54:38.241: step:915/1000 train_loss:5.2036 train_time:226533ms step_avg:250.31ms torch.cuda.memory_allocated()=369551872
14:54:38.507: step:916/1000 train_loss:5.9947 train_time:226799ms step_avg:250.33ms torch.cuda.memory_allocated()=369551872
14:54:38.770: step:917/1000 train_loss:5.8468 train_time:227062ms step_avg:250.34ms torch.cuda.memory_allocated()=369551872
14:54:39.029: step:918/1000 train_loss:5.5379 train_time:227321ms step_avg:250.35ms torch.cuda.memory_allocated()=369551872
14:54:39.291: step:919/1000 train_loss:5.7194 train_time:227582ms step_avg:250.37ms torch.cuda.memory_allocated()=369551872
14:54:39.555: step:920/1000 train_loss:5.2672 train_time:227847ms step_avg:250.38ms torch.cuda.memory_allocated()=369551872
14:54:39.816: step:921/1000 train_loss:5.4112 train_time:228107ms step_avg:250.39ms torch.cuda.memory_allocated()=369551872
14:54:40.074: step:922/1000 train_loss:5.3161 train_time:228366ms step_avg:250.40ms torch.cuda.memory_allocated()=369551872
14:54:40.336: step:923/1000 train_loss:5.4248 train_time:228628ms step_avg:250.41ms torch.cuda.memory_allocated()=369551872
14:54:40.590: step:924/1000 train_loss:5.6904 train_time:228882ms step_avg:250.42ms torch.cuda.memory_allocated()=369551872
14:54:40.856: step:925/1000 train_loss:5.1751 train_time:229147ms step_avg:250.43ms torch.cuda.memory_allocated()=369551872
14:54:41.119: step:926/1000 train_loss:5.4900 train_time:229410ms step_avg:250.45ms torch.cuda.memory_allocated()=369551872
14:54:41.385: step:927/1000 train_loss:5.4426 train_time:229677ms step_avg:250.47ms torch.cuda.memory_allocated()=369551872
14:54:41.643: step:928/1000 train_loss:5.2617 train_time:229935ms step_avg:250.47ms torch.cuda.memory_allocated()=369551872
14:54:41.900: step:929/1000 train_loss:5.1417 train_time:230192ms step_avg:250.48ms torch.cuda.memory_allocated()=369551872
14:54:42.160: step:930/1000 train_loss:5.7350 train_time:230452ms step_avg:250.49ms torch.cuda.memory_allocated()=369551872
14:54:42.421: step:931/1000 train_loss:5.3376 train_time:230713ms step_avg:250.50ms torch.cuda.memory_allocated()=369551872
14:54:42.693: step:932/1000 train_loss:4.9532 train_time:230985ms step_avg:250.53ms torch.cuda.memory_allocated()=369551872
14:54:42.957: step:933/1000 train_loss:5.1129 train_time:231248ms step_avg:250.54ms torch.cuda.memory_allocated()=369551872
14:54:43.226: step:934/1000 train_loss:5.1483 train_time:231517ms step_avg:250.56ms torch.cuda.memory_allocated()=369551872
14:54:43.488: step:935/1000 train_loss:5.3039 train_time:231779ms step_avg:250.57ms torch.cuda.memory_allocated()=369551872
14:54:43.745: step:936/1000 train_loss:5.2738 train_time:232036ms step_avg:250.58ms torch.cuda.memory_allocated()=369551872
14:54:43.999: step:937/1000 train_loss:5.4124 train_time:232290ms step_avg:250.58ms torch.cuda.memory_allocated()=369551872
14:54:44.264: step:938/1000 train_loss:5.3606 train_time:232556ms step_avg:250.60ms torch.cuda.memory_allocated()=369551872
14:54:44.525: step:939/1000 train_loss:5.6629 train_time:232817ms step_avg:250.61ms torch.cuda.memory_allocated()=369551872
14:54:44.799: step:940/1000 train_loss:5.3395 train_time:233090ms step_avg:250.63ms torch.cuda.memory_allocated()=369551872
14:54:45.064: step:941/1000 train_loss:5.5397 train_time:233356ms step_avg:250.65ms torch.cuda.memory_allocated()=369551872
14:54:45.332: step:942/1000 train_loss:5.2779 train_time:233624ms step_avg:250.67ms torch.cuda.memory_allocated()=369551872
14:54:45.591: step:943/1000 train_loss:5.1128 train_time:233883ms step_avg:250.68ms torch.cuda.memory_allocated()=369551872
14:54:45.859: step:944/1000 train_loss:5.0315 train_time:234150ms step_avg:250.70ms torch.cuda.memory_allocated()=369551872
14:54:46.120: step:945/1000 train_loss:5.6642 train_time:234411ms step_avg:250.71ms torch.cuda.memory_allocated()=369551872
14:54:46.380: step:946/1000 train_loss:5.3559 train_time:234671ms step_avg:250.72ms torch.cuda.memory_allocated()=369551872
14:54:46.631: step:947/1000 train_loss:5.5101 train_time:234923ms step_avg:250.72ms torch.cuda.memory_allocated()=369551872
14:54:46.884: step:948/1000 train_loss:5.3870 train_time:235176ms step_avg:250.72ms torch.cuda.memory_allocated()=369551872
14:54:47.158: step:949/1000 train_loss:5.3230 train_time:235450ms step_avg:250.75ms torch.cuda.memory_allocated()=369551872
14:54:47.449: step:950/1000 train_loss:5.3910 train_time:235741ms step_avg:250.79ms torch.cuda.memory_allocated()=369551872
14:54:47.714: step:951/1000 train_loss:5.3032 train_time:236005ms step_avg:250.80ms torch.cuda.memory_allocated()=369551872
14:54:47.975: step:952/1000 train_loss:5.4702 train_time:236267ms step_avg:250.81ms torch.cuda.memory_allocated()=369551872
14:54:48.242: step:953/1000 train_loss:5.5462 train_time:236533ms step_avg:250.83ms torch.cuda.memory_allocated()=369551872
14:54:48.501: step:954/1000 train_loss:5.2897 train_time:236793ms step_avg:250.84ms torch.cuda.memory_allocated()=369551872
14:54:48.786: step:955/1000 train_loss:5.1308 train_time:237078ms step_avg:250.88ms torch.cuda.memory_allocated()=369551872
14:54:49.046: step:956/1000 train_loss:5.1404 train_time:237337ms step_avg:250.89ms torch.cuda.memory_allocated()=369551872
14:54:49.308: step:957/1000 train_loss:5.4161 train_time:237599ms step_avg:250.90ms torch.cuda.memory_allocated()=369551872
14:54:49.594: step:958/1000 train_loss:5.8984 train_time:237886ms step_avg:250.93ms torch.cuda.memory_allocated()=369551872
14:54:49.857: step:959/1000 train_loss:5.3087 train_time:238149ms step_avg:250.95ms torch.cuda.memory_allocated()=369551872
14:54:50.119: step:960/1000 train_loss:5.3916 train_time:238411ms step_avg:250.96ms torch.cuda.memory_allocated()=369551872
14:54:50.383: step:961/1000 train_loss:5.1165 train_time:238675ms step_avg:250.97ms torch.cuda.memory_allocated()=369551872
14:54:50.649: step:962/1000 train_loss:5.0781 train_time:238941ms step_avg:250.99ms torch.cuda.memory_allocated()=369551872
14:54:50.909: step:963/1000 train_loss:5.4125 train_time:239201ms step_avg:251.00ms torch.cuda.memory_allocated()=369551872
14:54:51.168: step:964/1000 train_loss:5.3654 train_time:239460ms step_avg:251.01ms torch.cuda.memory_allocated()=369551872
14:54:51.434: step:965/1000 train_loss:5.3553 train_time:239725ms step_avg:251.02ms torch.cuda.memory_allocated()=369551872
14:54:51.692: step:966/1000 train_loss:5.3784 train_time:239984ms step_avg:251.03ms torch.cuda.memory_allocated()=369551872
14:54:51.949: step:967/1000 train_loss:5.4472 train_time:240241ms step_avg:251.04ms torch.cuda.memory_allocated()=369551872
14:54:52.205: step:968/1000 train_loss:5.3329 train_time:240497ms step_avg:251.04ms torch.cuda.memory_allocated()=369551872
14:54:52.469: step:969/1000 train_loss:5.1630 train_time:240761ms step_avg:251.05ms torch.cuda.memory_allocated()=369551872
14:54:52.724: step:970/1000 train_loss:5.2030 train_time:241015ms step_avg:251.06ms torch.cuda.memory_allocated()=369551872
14:54:52.984: step:971/1000 train_loss:5.3797 train_time:241275ms step_avg:251.07ms torch.cuda.memory_allocated()=369551872
14:54:53.243: step:972/1000 train_loss:5.2578 train_time:241535ms step_avg:251.08ms torch.cuda.memory_allocated()=369551872
14:54:53.504: step:973/1000 train_loss:5.3605 train_time:241795ms step_avg:251.09ms torch.cuda.memory_allocated()=369551872
14:54:53.759: step:974/1000 train_loss:5.1717 train_time:242051ms step_avg:251.09ms torch.cuda.memory_allocated()=369551872
14:54:54.012: step:975/1000 train_loss:5.3683 train_time:242304ms step_avg:251.09ms torch.cuda.memory_allocated()=369551872
14:54:54.263: step:976/1000 train_loss:5.3997 train_time:242554ms step_avg:251.09ms torch.cuda.memory_allocated()=369551872
14:54:54.532: step:977/1000 train_loss:5.4239 train_time:242824ms step_avg:251.11ms torch.cuda.memory_allocated()=369551872
14:54:54.797: step:978/1000 train_loss:5.7304 train_time:243089ms step_avg:251.12ms torch.cuda.memory_allocated()=369551872
14:54:55.085: step:979/1000 train_loss:6.0206 train_time:243376ms step_avg:251.16ms torch.cuda.memory_allocated()=369551872
14:54:55.380: step:980/1000 train_loss:6.2265 train_time:243671ms step_avg:251.21ms torch.cuda.memory_allocated()=369551872
14:54:55.649: step:981/1000 train_loss:5.5758 train_time:243940ms step_avg:251.23ms torch.cuda.memory_allocated()=369551872
14:54:55.905: step:982/1000 train_loss:5.5845 train_time:244196ms step_avg:251.23ms torch.cuda.memory_allocated()=369551872
14:54:56.170: step:983/1000 train_loss:5.4443 train_time:244462ms step_avg:251.25ms torch.cuda.memory_allocated()=369551872
14:54:56.430: step:984/1000 train_loss:5.3221 train_time:244722ms step_avg:251.25ms torch.cuda.memory_allocated()=369551872
14:54:56.694: step:985/1000 train_loss:5.2694 train_time:244986ms step_avg:251.27ms torch.cuda.memory_allocated()=369551872
14:54:56.947: step:986/1000 train_loss:5.2335 train_time:245239ms step_avg:251.27ms torch.cuda.memory_allocated()=369551872
14:54:57.204: step:987/1000 train_loss:5.1579 train_time:245496ms step_avg:251.27ms torch.cuda.memory_allocated()=369551872
14:54:57.463: step:988/1000 train_loss:5.4977 train_time:245754ms step_avg:251.28ms torch.cuda.memory_allocated()=369551872
14:54:57.731: step:989/1000 train_loss:5.1420 train_time:246022ms step_avg:251.30ms torch.cuda.memory_allocated()=369551872
14:54:57.999: step:990/1000 train_loss:5.3481 train_time:246290ms step_avg:251.32ms torch.cuda.memory_allocated()=369551872
14:54:58.271: step:991/1000 train_loss:5.1302 train_time:246563ms step_avg:251.34ms torch.cuda.memory_allocated()=369551872
14:54:58.530: step:992/1000 train_loss:5.2218 train_time:246822ms step_avg:251.35ms torch.cuda.memory_allocated()=369551872
14:54:58.784: step:993/1000 train_loss:5.3055 train_time:247075ms step_avg:251.35ms torch.cuda.memory_allocated()=369551872
14:54:59.039: step:994/1000 train_loss:5.1656 train_time:247331ms step_avg:251.35ms torch.cuda.memory_allocated()=369551872
14:54:59.312: step:995/1000 train_loss:5.2395 train_time:247603ms step_avg:251.37ms torch.cuda.memory_allocated()=369551872
14:54:59.592: step:996/1000 train_loss:4.5894 train_time:247883ms step_avg:251.40ms torch.cuda.memory_allocated()=369551872
14:54:59.858: step:997/1000 train_loss:5.0009 train_time:248150ms step_avg:251.42ms torch.cuda.memory_allocated()=369551872
14:55:00.122: step:998/1000 train_loss:5.0052 train_time:248414ms step_avg:251.43ms torch.cuda.memory_allocated()=369551872
14:55:00.375: step:999/1000 train_loss:5.2720 train_time:248667ms step_avg:251.43ms torch.cuda.memory_allocated()=369551872
14:55:00.626: step:1000/1000 train_loss:5.1586 train_time:248917ms step_avg:251.43ms torch.cuda.memory_allocated()=369551872
14:55:03.405: step:1000/1000 val_loss:5.4239 train_time:248918ms step_avg:251.43ms
14:55:33.154: Renamed logs/5b0e4da1-caad-442e-8fed-d5c5a7d50b34.txt -> logs/20250202_MoEUT_ReLU2_RMSNormMLP.txt
14:55:33.156: peak memory allocated: 6688 MiB reserved: 10684 MiB
